{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJc7GRtFbrRK"
   },
   "source": [
    "# A Simple Regression Example of Balanced MSE\n",
    "\n",
    "This notebook is developed on [Balanced MSE for Imbalanced Visual Regression](https://github.com/jiawei-ren/BalancedMSE/tree/main/tutorial) and [Deep Imbalanced Regression (DIR) Tutorial](https://github.com/YyzHarry/imbalanced-regression/tree/main/tutorial). We thank the authors for their amazing tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7mBaSzabN6I"
   },
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xWuPbiEYmzvS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if (os.path.exists('./boston.data.train') and os.path.exists('./boston.data.val') and os.path.exists('./boston.data.test')):\n",
    "    print('Data already downloaded.')\n",
    "else:\n",
    "    print('No Data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnWsY4FzYC0Y"
   },
   "source": [
    "# Define the deep learning (neural network) model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jKm3tqsMhDOA"
   },
   "outputs": [],
   "source": [
    "# fcnet.py: Define the deep learning (neural network) model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FCNet(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, dropout=None):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.fc0 = nn.Linear(13, layers[0])            # features\n",
    "        self.fc1 = nn.Linear(layers[0], layers[1])\n",
    "        self.fc2 = nn.Linear(layers[1], layers[2])\n",
    "        self.fc_final = nn.Linear(layers[-1], 1)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.use_dropout = True if dropout else False\n",
    "        if self.use_dropout:\n",
    "            print(f'Using dropout: {dropout}')\n",
    "            self.dropout0 = nn.Dropout(p=dropout)\n",
    "            self.dropout1 = nn.Dropout(p=dropout)\n",
    "            self.dropout2 = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout0 = nn.Identity()\n",
    "            self.dropout1 = nn.Identity()\n",
    "            self.dropout2 = nn.Identity()\n",
    "\n",
    "    def forward(self, x, targets=None, epoch=None):\n",
    "        x = self.dropout0(F.relu(self.fc0(x)))\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout2(F.relu(self.fc2(x)))\n",
    "        x = self.fc_final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def fcnet1(**kwargs):\n",
    "    return FCNet([512, 512, 512], **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWohJXZPYbY3"
   },
   "source": [
    "# Define the loss functions. Here we only need the L1 loss: |f(x) - y|."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7Kzc8PTjhDIH"
   },
   "outputs": [],
   "source": [
    "# loss.py: Define the loss functions (here we only need the L1 loss)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def weighted_mse_loss(inputs, targets, weights=None):\n",
    "    loss = (inputs - targets) ** 2\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_l1_loss(inputs, targets, weights=None):\n",
    "    loss = F.l1_loss(inputs, targets, reduction='none')\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_focal_mse_loss(inputs, targets, weights=None, activate='sigmoid', beta=.2, gamma=1):\n",
    "    loss = (inputs - targets) ** 2\n",
    "    loss *= (torch.tanh(beta * torch.abs(inputs - targets))) ** gamma if activate == 'tanh' else \\\n",
    "        (2 * torch.sigmoid(beta * torch.abs(inputs - targets)) - 1) ** gamma\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_focal_l1_loss(inputs, targets, weights=None, activate='sigmoid', beta=.2, gamma=1):\n",
    "    loss = F.l1_loss(inputs, targets, reduction='none')\n",
    "    loss *= (torch.tanh(beta * torch.abs(inputs - targets))) ** gamma if activate == 'tanh' else \\\n",
    "        (2 * torch.sigmoid(beta * torch.abs(inputs - targets)) - 1) ** gamma\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_huber_loss(inputs, targets, weights=None, beta=1.):\n",
    "    l1_loss = torch.abs(inputs - targets)\n",
    "    cond = l1_loss < beta\n",
    "    loss = torch.where(cond, 0.5 * l1_loss ** 2 / beta, l1_loss - 0.5 * beta)\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Balanced MSE Loss (BMC version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bmse_loss(inputs, targets, noise_sigma=8.):\n",
    "    return bmc_loss(inputs, targets, noise_sigma ** 2)\n",
    "\n",
    "def bmc_loss(pred, target, noise_var):\n",
    "    logits = - 0.5 * (pred - target.T).pow(2) / noise_var\n",
    "    loss = F.cross_entropy(logits, torch.arange(pred.shape[0]))\n",
    "    loss = loss * (2 * noise_var)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aunbes6NY_Pz"
   },
   "source": [
    "# Define some utility functions (not the focus of this course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "f3XXH8Yik4M9"
   },
   "outputs": [],
   "source": [
    "# utils.py: Define some utility functions (not the focus of this course).\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal.windows import triang\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_batch_fmtstr(num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "def query_yes_no(question):\n",
    "    \"\"\" Ask a yes/no question via input() and return their answer. \"\"\"\n",
    "    valid = {\"yes\": True, \"y\": True, \"ye\": True, \"no\": False, \"n\": False}\n",
    "    prompt = \" [Y/n] \"\n",
    "\n",
    "    while True:\n",
    "        print(question + prompt, end=':')\n",
    "        choice = input().lower()\n",
    "        if choice == '':\n",
    "            return valid['y']\n",
    "        elif choice in valid:\n",
    "            return valid[choice]\n",
    "        else:\n",
    "            print(\"Please respond with 'yes' or 'no' (or 'y' or 'n').\\n\")\n",
    "\n",
    "def prepare_folders(args):\n",
    "    folders_util = [args.store_root, os.path.join(args.store_root, args.store_name)]\n",
    "    if os.path.exists(folders_util[-1]) and not args.resume and not args.evaluate:\n",
    "        if query_yes_no('overwrite previous folder: {} ?'.format(folders_util[-1])):\n",
    "            shutil.rmtree(folders_util[-1])\n",
    "            print(folders_util[-1] + ' removed.')\n",
    "        else:\n",
    "            raise RuntimeError('Output folder {} already exists'.format(folders_util[-1]))\n",
    "    for folder in folders_util:\n",
    "        if not os.path.exists(folder):\n",
    "            print(f\"===> Creating folder: {folder}\")\n",
    "            os.mkdir(folder)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    lr = args.lr\n",
    "    for milestone in args.schedule:\n",
    "        lr *= 0.1 if epoch >= milestone else 1.\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def save_checkpoint(args, state, is_best, prefix=''):\n",
    "    filename = f\"{args.store_root}/{args.store_name}/{prefix}ckpt.pth.tar\"\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        print(\"===> Saving current best checkpoint...\")\n",
    "        shutil.copyfile(filename, filename.replace('pth.tar', 'best.pth.tar'))\n",
    "\n",
    "def calibrate_mean_var(matrix, m1, v1, m2, v2, clip_min=0.1, clip_max=10):\n",
    "    if torch.sum(v1) < 1e-10:\n",
    "        return matrix\n",
    "    if (v1 == 0.).any():\n",
    "        valid = (v1 != 0.)\n",
    "        factor = torch.clamp(v2[valid] / v1[valid], clip_min, clip_max)\n",
    "        matrix[:, valid] = (matrix[:, valid] - m1[valid]) * torch.sqrt(factor) + m2[valid]\n",
    "        return matrix\n",
    "\n",
    "    factor = torch.clamp(v2 / v1, clip_min, clip_max)\n",
    "    return (matrix - m1) * torch.sqrt(factor) + m2\n",
    "\n",
    "def get_lds_kernel_window(kernel, ks, sigma):\n",
    "    assert kernel in ['gaussian', 'triang', 'laplace']\n",
    "    half_ks = (ks - 1) // 2\n",
    "    if kernel == 'gaussian':\n",
    "        base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks\n",
    "        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / max(gaussian_filter1d(base_kernel, sigma=sigma))\n",
    "    elif kernel == 'triang':\n",
    "        kernel_window = triang(ks)\n",
    "    else:\n",
    "        laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)\n",
    "        kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / max(map(laplace, np.arange(-half_ks, half_ks + 1)))\n",
    "\n",
    "    return kernel_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpbjtnotZUkk"
   },
   "source": [
    "# Define the data iterator (data loader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gqK8H1UghC_v"
   },
   "outputs": [],
   "source": [
    "# datasets.py: Define the data iterator (data loader).\n",
    "from scipy.ndimage import convolve1d\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "# from utils import get_lds_kernel_window\n",
    "\n",
    "class BostonHousing(data.Dataset):\n",
    "    def __init__(self, data_dir, split='train', reweight='none',\n",
    "                 lds=False, lds_kernel='gaussian', lds_ks=5, lds_sigma=2):\n",
    "        self.split = split\n",
    "        #self.data = np.loadtxt(data_dir, dtype='float32',encoding='utf-16')\n",
    "        self.data = np.loadtxt(data_dir, dtype='float32')\n",
    "        self.weights = self._prepare_weights(reweight=reweight, lds=lds, lds_kernel=lds_kernel, lds_ks=lds_ks, lds_sigma=lds_sigma)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = index % self.data.shape[0]\n",
    "        feature = self.data[index, :-1]\n",
    "        label = np.expand_dims(np.asarray(self.data[index, -1]), axis=0)\n",
    "        weight = np.asarray([self.weights[index]]).astype('float32') if self.weights is not None else np.asarray([np.float32(1.)])\n",
    "        return feature, label, weight\n",
    "\n",
    "    def _prepare_weights(self, reweight, max_target=51, lds=False, lds_kernel='gaussian', lds_ks=5, lds_sigma=2):\n",
    "        assert reweight in {'none', 'inverse', 'sqrt_inv'}\n",
    "        assert reweight != 'none' if lds else True, \\\n",
    "            \"Set reweight to \\'sqrt_inv\\' (default) or \\'inverse\\' when using LDS\"\n",
    "\n",
    "        value_dict = {x: 0 for x in range(max_target)}\n",
    "        labels = self.data[:, -1].tolist()\n",
    "        # mbr\n",
    "        for label in labels:\n",
    "            value_dict[min(max_target - 1, int(label))] += 1\n",
    "        if reweight == 'sqrt_inv':\n",
    "            value_dict = {k: np.sqrt(v) for k, v in value_dict.items()}\n",
    "        elif reweight == 'inverse':\n",
    "            value_dict = {k: np.clip(v, 0, 1000) for k, v in value_dict.items()}     # clip weights for inverse re-weight!!!!!!\n",
    "        num_per_label = [value_dict[min(max_target - 1, int(label))] for label in labels]\n",
    "        if not len(num_per_label) or reweight == 'none':\n",
    "            return None\n",
    "        print(f\"Using re-weighting: [{reweight.upper()}]\")\n",
    "\n",
    "        if lds:\n",
    "            lds_kernel_window = get_lds_kernel_window(lds_kernel, lds_ks, lds_sigma)\n",
    "            print(f'Using LDS: [{lds_kernel.upper()}] ({lds_ks}/{lds_sigma})')\n",
    "            smoothed_value = convolve1d(\n",
    "                np.asarray([v for _, v in value_dict.items()]), weights=lds_kernel_window, mode='constant')\n",
    "            num_per_label = [smoothed_value[min(max_target - 1, int(label))] for label in labels]\n",
    "\n",
    "        weights = [np.float32(1 / x) for x in num_per_label]\n",
    "        scaling = len(weights) / np.sum(weights)\n",
    "        weights = [scaling * x for x in weights]\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zcuwqp5paPk3"
   },
   "source": [
    "# Set up some default configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "T7mtQacPhBoW"
   },
   "outputs": [],
   "source": [
    "# train.py, Part 1: Set up some default configurations.\n",
    "import time\n",
    "import argparse\n",
    "#import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gmean\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from resnet import resnet50\n",
    "# from fcnet import fcnet1\n",
    "# from loss import *\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\"   # Window 系统原因，在不启用Cuda情况下，CPU线程问题会中断pipeline\n",
    "\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# CPU only\n",
    "parser.add_argument('--cpu_only', action='store_true', default=False, help='whether to use CPU only')\n",
    "# imbalanced related\n",
    "# LDS\n",
    "parser.add_argument('--lds', action='store_true', default=False, help='whether to enable LDS')\n",
    "parser.add_argument('--lds_kernel', type=str, default='gaussian',\n",
    "                    choices=['gaussian', 'triang', 'laplace'], help='LDS kernel type')\n",
    "parser.add_argument('--lds_ks', type=int, default=9, help='LDS kernel size: should be odd number')\n",
    "parser.add_argument('--lds_sigma', type=float, default=1, help='LDS gaussian/laplace kernel sigma')\n",
    "\n",
    "# re-weighting: SQRT_INV / INV\n",
    "parser.add_argument('--reweight', type=str, default='none', choices=['none', 'sqrt_inv', 'inverse'], help='cost-sensitive reweighting scheme')\n",
    "\n",
    "# training/optimization related\n",
    "parser.add_argument('--dataset', type=str, default='bostonhousing', choices=['imdb_wiki', 'agedb'], help='dataset name')\n",
    "parser.add_argument('--data_dir', type=str, default='./boston.data', help='data directory')\n",
    "parser.add_argument('--model', type=str, default='fcnet1', help='model name')\n",
    "parser.add_argument('--store_root', type=str, default='checkpoint', help='root path for storing checkpoints, logs')\n",
    "parser.add_argument('--store_name', type=str, default='', help='experiment store name')\n",
    "parser.add_argument('--gpu', type=int, default=None)\n",
    "parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'], help='optimizer type')\n",
    "parser.add_argument('--loss', type=str, default='l1', choices=['mse', 'l1', 'focal_l1', 'focal_mse', 'huber'], help='training loss type')\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help='initial learning rate')\n",
    "parser.add_argument('--epoch', type=int, default=10, help='number of epochs to train')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='optimizer momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4, help='optimizer weight decay')\n",
    "parser.add_argument('--schedule', type=int, nargs='*', default=[60, 80], help='lr schedule (when to drop lr by 10x)')\n",
    "#parser.add_argument('--batch_size', type=int, default=256, help='batch size')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--print_freq', type=int, default=10, help='logging frequency')\n",
    "parser.add_argument('--img_size', type=int, default=224, help='image size used in training')\n",
    "parser.add_argument('--workers', type=int, default=32, help='number of workers used in data loading')\n",
    "# checkpoints\n",
    "parser.add_argument('--resume', type=str, default='', help='checkpoint file path to resume training')\n",
    "parser.add_argument('--evaluate', action='store_true', help='evaluate only flag')\n",
    "\n",
    "# Balanced MSE\n",
    "parser.add_argument('--bmse', action='store_true', help='use Balanced MSE')\n",
    "parser.set_defaults(augment=True)\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nWvzFoYJnwuF"
   },
   "outputs": [],
   "source": [
    "args.cpu_only = True # Use CPU to train/test models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtG61vP8ROkP"
   },
   "source": [
    "# DNN None sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tSrzhog1gxyY",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overwrite previous folder: checkpoint\\bostonhousing_fcnet1_adam_l1_0.001_64 ? [Y/n] :"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\\bostonhousing_fcnet1_adam_l1_0.001_64 removed.\n",
      "===> Creating folder: checkpoint\\bostonhousing_fcnet1_adam_l1_0.001_64\n",
      "Args: Namespace(augment=True, batch_size=64, best_loss=100000.0, bmse=False, cpu_only=True, data_dir='./boston.data', dataset='bostonhousing', epoch=10, evaluate=False, gpu=None, img_size=224, lds=False, lds_kernel='gaussian', lds_ks=9, lds_sigma=1, loss='l1', lr=0.001, model='fcnet1', momentum=0.9, optimizer='adam', print_freq=10, resume='', reweight='none', schedule=[60, 80], start_epoch=0, store_name='bostonhousing_fcnet1_adam_l1_0.001_64', store_root='checkpoint', weight_decay=0.0001, workers=32)\n",
      "Store name: bostonhousing_fcnet1_adam_l1_0.001_64\n",
      "=====> Preparing data...\n",
      "Training data size: 353\n",
      "Validation data size: 77\n",
      "Test data size: 76\n",
      "=====> Building model...\n",
      "Epoch: [0][0/6]\tTime   0.17 (  0.17)\tData 0.0600 (0.0600)\tLoss (L1) 18.296 (18.296)\n",
      "Val: [0/2]\tTime  0.005 ( 0.005)\tLoss (MSE) 361.615 (361.615)\tLoss (L1) 15.603 (15.603)\n",
      " * Overall: MSE 356.780\tL1 15.391\tG-Mean 10.497\n",
      " * Many: MSE 126.883\tL1 9.017\tG-Mean 6.833\n",
      " * Median: MSE 269.487\tL1 14.483\tG-Mean 10.521\n",
      " * Low: MSE 813.862\tL1 26.016\tG-Mean 19.625\n",
      "Best L1 Loss: 15.391\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #0: Train loss [23.5279]; Val loss: MSE [356.7801], L1 [15.3911], G-Mean [10.4968]\n",
      "Epoch: [1][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 11.038 (11.038)\n",
      "Val: [0/2]\tTime  0.008 ( 0.008)\tLoss (MSE) 179.688 (179.688)\tLoss (L1) 10.659 (10.659)\n",
      " * Overall: MSE 174.504\tL1 10.334\tG-Mean 6.863\n",
      " * Many: MSE 45.979\tL1 4.252\tG-Mean 2.896\n",
      " * Median: MSE 109.426\tL1 9.969\tG-Mean 9.242\n",
      " * Low: MSE 457.977\tL1 20.374\tG-Mean 18.714\n",
      "Best L1 Loss: 10.334\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #1: Train loss [7.7090]; Val loss: MSE [174.5039], L1 [10.3337], G-Mean [6.8628]\n",
      "Epoch: [2][0/6]\tTime   0.01 (  0.01)\tData 0.0040 (0.0040)\tLoss (L1) 5.392 (5.392)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 120.675 (120.675)\tLoss (L1) 8.484 (8.484)\n",
      " * Overall: MSE 116.503\tL1 8.215\tG-Mean 4.736\n",
      " * Many: MSE 39.924\tL1 4.147\tG-Mean 2.492\n",
      " * Median: MSE 58.852\tL1 6.851\tG-Mean 5.714\n",
      " * Low: MSE 310.530\tL1 16.637\tG-Mean 14.988\n",
      "Best L1 Loss: 8.215\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #2: Train loss [5.7080]; Val loss: MSE [116.5029], L1 [8.2150], G-Mean [4.7360]\n",
      "Epoch: [3][0/6]\tTime   0.01 (  0.01)\tData 0.0020 (0.0020)\tLoss (L1) 5.096 (5.096)\n",
      "Val: [0/2]\tTime  0.005 ( 0.005)\tLoss (MSE) 104.313 (104.313)\tLoss (L1) 7.880 (7.880)\n",
      " * Overall: MSE 101.252\tL1 7.621\tG-Mean 4.869\n",
      " * Many: MSE 35.771\tL1 3.978\tG-Mean 2.644\n",
      " * Median: MSE 49.113\tL1 6.192\tG-Mean 4.899\n",
      " * Low: MSE 269.977\tL1 15.405\tG-Mean 14.112\n",
      "Best L1 Loss: 7.621\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #3: Train loss [4.9186]; Val loss: MSE [101.2516], L1 [7.6212], G-Mean [4.8686]\n",
      "Epoch: [4][0/6]\tTime   0.01 (  0.01)\tData 0.0020 (0.0020)\tLoss (L1) 6.279 (6.279)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 132.769 (132.769)\tLoss (L1) 8.775 (8.775)\n",
      " * Overall: MSE 130.025\tL1 8.477\tG-Mean 4.908\n",
      " * Many: MSE 35.268\tL1 3.145\tG-Mean 1.652\n",
      " * Median: MSE 73.608\tL1 8.029\tG-Mean 7.332\n",
      " * Low: MSE 345.534\tL1 17.339\tG-Mean 15.850\n",
      "Best L1 Loss: 7.621\n",
      "Epoch #4: Train loss [4.8381]; Val loss: MSE [130.0255], L1 [8.4773], G-Mean [4.9079]\n",
      "Epoch: [5][0/6]\tTime   0.01 (  0.01)\tData 0.0030 (0.0030)\tLoss (L1) 4.586 (4.586)\n",
      "Val: [0/2]\tTime  0.006 ( 0.006)\tLoss (MSE) 117.474 (117.474)\tLoss (L1) 8.161 (8.161)\n",
      " * Overall: MSE 114.023\tL1 7.861\tG-Mean 4.578\n",
      " * Many: MSE 34.276\tL1 3.315\tG-Mean 1.926\n",
      " * Median: MSE 57.814\tL1 6.895\tG-Mean 5.721\n",
      " * Low: MSE 306.597\tL1 16.267\tG-Mean 14.709\n",
      "Best L1 Loss: 7.621\n",
      "Epoch #5: Train loss [4.8386]; Val loss: MSE [114.0234], L1 [7.8610], G-Mean [4.5778]\n",
      "Epoch: [6][0/6]\tTime   0.01 (  0.01)\tData 0.0030 (0.0030)\tLoss (L1) 4.322 (4.322)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 115.581 (115.581)\tLoss (L1) 8.219 (8.219)\n",
      " * Overall: MSE 110.050\tL1 7.920\tG-Mean 4.693\n",
      " * Many: MSE 35.702\tL1 3.834\tG-Mean 2.410\n",
      " * Median: MSE 59.788\tL1 6.814\tG-Mean 4.894\n",
      " * Low: MSE 292.348\tL1 16.164\tG-Mean 15.110\n",
      "Best L1 Loss: 7.621\n",
      "Epoch #6: Train loss [4.7319]; Val loss: MSE [110.0497], L1 [7.9198], G-Mean [4.6929]\n",
      "Epoch: [7][0/6]\tTime   0.01 (  0.01)\tData 0.0030 (0.0030)\tLoss (L1) 4.458 (4.458)\n",
      "Val: [0/2]\tTime  0.006 ( 0.006)\tLoss (MSE) 117.141 (117.141)\tLoss (L1) 8.135 (8.135)\n",
      " * Overall: MSE 111.157\tL1 7.765\tG-Mean 4.121\n",
      " * Many: MSE 34.433\tL1 3.424\tG-Mean 1.813\n",
      " * Median: MSE 55.302\tL1 6.610\tG-Mean 4.535\n",
      " * Low: MSE 301.645\tL1 16.297\tG-Mean 14.985\n",
      "Best L1 Loss: 7.621\n",
      "Epoch #7: Train loss [4.4874]; Val loss: MSE [111.1573], L1 [7.7648], G-Mean [4.1207]\n",
      "Epoch: [8][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 5.681 (5.681)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 119.115 (119.115)\tLoss (L1) 8.150 (8.150)\n",
      " * Overall: MSE 112.923\tL1 7.742\tG-Mean 4.176\n",
      " * Many: MSE 34.043\tL1 3.204\tG-Mean 1.647\n",
      " * Median: MSE 55.259\tL1 6.669\tG-Mean 4.912\n",
      " * Low: MSE 307.643\tL1 16.384\tG-Mean 14.982\n",
      "Best L1 Loss: 7.621\n",
      "Epoch #8: Train loss [4.5297]; Val loss: MSE [112.9229], L1 [7.7416], G-Mean [4.1757]\n",
      "Epoch: [9][0/6]\tTime   0.02 (  0.02)\tData 0.0040 (0.0040)\tLoss (L1) 4.310 (4.310)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 138.407 (138.407)\tLoss (L1) 8.868 (8.868)\n",
      " * Overall: MSE 131.622\tL1 8.423\tG-Mean 4.618\n",
      " * Many: MSE 36.140\tL1 3.007\tG-Mean 1.238\n",
      " * Median: MSE 69.570\tL1 7.752\tG-Mean 6.890\n",
      " * Low: MSE 355.889\tL1 17.615\tG-Mean 16.107\n",
      "Best L1 Loss: 7.621\n",
      "Epoch #9: Train loss [4.4262]; Val loss: MSE [131.6222], L1 [8.4231], G-Mean [4.6179]\n",
      "========================================================================================================================\n",
      "Test best model on testset...\n",
      "Loaded best model, epoch 4, best val loss 7.6212\n",
      "Test: [0/2]\tTime  0.005 ( 0.005)\tLoss (MSE) 79.313 (79.313)\tLoss (L1) 7.013 (7.013)\n",
      " * Overall: MSE 73.076\tL1 6.801\tG-Mean 4.480\n",
      " * Many: MSE 17.000\tL1 3.479\tG-Mean 2.332\n",
      " * Median: MSE 47.473\tL1 6.174\tG-Mean 5.244\n",
      " * Low: MSE 205.697\tL1 13.588\tG-Mean 12.598\n",
      "Test loss: MSE [73.0757], L1 [6.8014], G-Mean [4.4804]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Option 1: To train the basic model, use the default setting, don't need to do anything\n",
    "args.reweight = 'none'\n",
    "args.lds = False\n",
    "\n",
    "# train.py Part 2: Train/Evaluate the model.\n",
    "args.store_name = ''\n",
    "args.start_epoch, args.best_loss = 0, 1e5\n",
    "\n",
    "if len(args.store_name):\n",
    "    args.store_name = f'_{args.store_name}'\n",
    "if not args.lds and args.reweight != 'none':\n",
    "    args.store_name += f'_{args.reweight}'\n",
    "if args.lds:\n",
    "    args.store_name += f'_lds_{args.lds_kernel[:3]}_{args.lds_ks}'\n",
    "    if args.lds_kernel in ['gaussian', 'laplace']:\n",
    "        args.store_name += f'_{args.lds_sigma}'\n",
    "if args.bmse:\n",
    "  args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_bmse_{args.lr}_{args.batch_size}\"\n",
    "else:\n",
    "  args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_{args.loss}_{args.lr}_{args.batch_size}\"\n",
    "\n",
    "prepare_folders(args)\n",
    "\n",
    "print(f\"Args: {args}\")\n",
    "print(f\"Store name: {args.store_name}\")\n",
    "\n",
    "def main():\n",
    "    if args.gpu is not None:\n",
    "        print(f\"Use GPU: {args.gpu} for training\")\n",
    "\n",
    "    # Data\n",
    "    print('=====> Preparing data...')\n",
    "    #train_labels = np.loadtxt(args.data_dir+'.train',encoding='utf-16')[:, -1]\n",
    "    train_labels = np.loadtxt(args.data_dir+'.train')[:, -1]\n",
    "    train_dataset = BostonHousing(data_dir=args.data_dir+'.train', split='train',\n",
    "                          reweight=args.reweight, lds=args.lds, lds_kernel=args.lds_kernel, lds_ks=args.lds_ks, lds_sigma=args.lds_sigma)\n",
    "    val_dataset = BostonHousing(data_dir=args.data_dir+'.val', split='val')\n",
    "    test_dataset = BostonHousing(data_dir=args.data_dir+'.test', split='test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                              num_workers=0, pin_memory=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                            num_workers=0, pin_memory=True, drop_last=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                             num_workers=0, pin_memory=True, drop_last=False)\n",
    "    print(f\"Training data size: {len(train_dataset)}\")\n",
    "    print(f\"Validation data size: {len(val_dataset)}\")\n",
    "    print(f\"Test data size: {len(test_dataset)}\")\n",
    "\n",
    "    # Random Seed\n",
    "    np.random.seed(999)\n",
    "    # random.seed(999)\n",
    "    torch.manual_seed(999)\n",
    "\n",
    "    # Model\n",
    "    print('=====> Building model...')\n",
    "    model = fcnet1()\n",
    "    if not args.cpu_only:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # evaluate only\n",
    "    if args.evaluate:\n",
    "        assert args.resume, 'Specify a trained model using [args.resume]'\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        print(f\"===> Checkpoint '{args.resume}' loaded (epoch [{checkpoint['epoch']}]), testing...\")\n",
    "        validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "        return\n",
    "\n",
    "    # Loss and optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr) if args.optimizer == 'adam' else \\\n",
    "        torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(f\"===> Loading checkpoint '{args.resume}'\")\n",
    "            checkpoint = torch.load(args.resume) if args.gpu is None else \\\n",
    "                torch.load(args.resume, map_location=torch.device(f'cuda:{str(args.gpu)}'))\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            args.best_loss = checkpoint['best_loss']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(f\"===> Loaded checkpoint '{args.resume}' (Epoch [{checkpoint['epoch']}])\")\n",
    "        else:\n",
    "            print(f\"===> No checkpoint found at '{args.resume}'\")\n",
    "\n",
    "    if not args.cpu_only:\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epoch):\n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "        train_loss = train(train_loader, model, optimizer, epoch)\n",
    "        val_loss_mse, val_loss_l1, val_loss_gmean = validate(val_loader, model, train_labels=train_labels)\n",
    "\n",
    "        loss_metric = val_loss_mse if args.loss == 'mse' else val_loss_l1\n",
    "        is_best = loss_metric < args.best_loss\n",
    "        args.best_loss = min(loss_metric, args.best_loss)\n",
    "        print(f\"Best {'L1' if 'l1' in args.loss else 'MSE'} Loss: {args.best_loss:.3f}\")\n",
    "        save_checkpoint(args, {\n",
    "            'epoch': epoch + 1,\n",
    "            'model': args.model,\n",
    "            'best_loss': args.best_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print(f\"Epoch #{epoch}: Train loss [{train_loss:.4f}]; \"\n",
    "              f\"Val loss: MSE [{val_loss_mse:.4f}], L1 [{val_loss_l1:.4f}], G-Mean [{val_loss_gmean:.4f}]\")\n",
    "\n",
    "    # test with best checkpoint\n",
    "    print(\"=\" * 120)\n",
    "    print(\"Test best model on testset...\")\n",
    "    checkpoint = torch.load(f\"{args.store_root}/{args.store_name}/ckpt.best.pth.tar\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(f\"Loaded best model, epoch {checkpoint['epoch']}, best val loss {checkpoint['best_loss']:.4f}\")\n",
    "\n",
    "    test_loss_mse, test_loss_l1, test_loss_gmean = validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "    print(f\"Test loss: MSE [{test_loss_mse:.4f}], L1 [{test_loss_l1:.4f}], G-Mean [{test_loss_gmean:.4f}]\\nDone\")\n",
    "\n",
    "def train(train_loader, model, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.2f')\n",
    "    data_time = AverageMeter('Data', ':6.4f')\n",
    "    losses = AverageMeter(f'Loss ({args.loss.upper()})', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch)\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for idx, (inputs, targets, weights) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        if not args.cpu_only:\n",
    "            inputs, targets, weights = \\\n",
    "                inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True), weights.cuda(non_blocking=True)\n",
    "        outputs = model(inputs, targets, epoch)\n",
    "        if args.bmse:\n",
    "          loss = globals()[f\"bmse_loss\"](outputs, targets)\n",
    "        else:\n",
    "          loss = globals()[f\"weighted_{args.loss}_loss\"](outputs, targets, weights)\n",
    "        assert not (np.isnan(loss.item()) or loss.item() > 1e6), f\"Loss explosion: {loss.item()}\"\n",
    "\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if idx % args.print_freq == 0:\n",
    "            progress.display(idx)\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "def validate(val_loader, model, train_labels=None, prefix='Val'):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses_mse = AverageMeter('Loss (MSE)', ':.3f')\n",
    "    losses_l1 = AverageMeter('Loss (L1)', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses_mse, losses_l1],\n",
    "        prefix=f'{prefix}: '\n",
    "    )\n",
    "\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    criterion_gmean = nn.L1Loss(reduction='none')\n",
    "\n",
    "    model.eval()\n",
    "    losses_all = []\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (inputs, targets, _) in enumerate(val_loader):\n",
    "            if not args.cpu_only:\n",
    "                inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds.extend(outputs.data.cpu().numpy())\n",
    "            labels.extend(targets.data.cpu().numpy())\n",
    "\n",
    "            loss_mse = criterion_mse(outputs, targets)\n",
    "            loss_l1 = criterion_l1(outputs, targets)\n",
    "            loss_all = criterion_gmean(outputs, targets)\n",
    "            losses_all.extend(loss_all.cpu().numpy())\n",
    "\n",
    "            losses_mse.update(loss_mse.item(), inputs.size(0))\n",
    "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if idx % args.print_freq == 0:\n",
    "                progress.display(idx)\n",
    "\n",
    "\n",
    "        shot_dict = shot_metrics(np.hstack(preds), np.hstack(labels), train_labels)\n",
    "        loss_gmean = gmean(np.hstack(losses_all), axis=None).astype(float)\n",
    "        print(f\" * Overall: MSE {losses_mse.avg:.3f}\\tL1 {losses_l1.avg:.3f}\\tG-Mean {loss_gmean:.3f}\")\n",
    "        print(f\" * Many: MSE {shot_dict['many']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['many']['l1']:.3f}\\tG-Mean {shot_dict['many']['gmean']:.3f}\")\n",
    "        print(f\" * Median: MSE {shot_dict['median']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['median']['l1']:.3f}\\tG-Mean {shot_dict['median']['gmean']:.3f}\")\n",
    "        print(f\" * Low: MSE {shot_dict['low']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['low']['l1']:.3f}\\tG-Mean {shot_dict['low']['gmean']:.3f}\")\n",
    "\n",
    "    return losses_mse.avg, losses_l1.avg, loss_gmean\n",
    "\n",
    "def shot_metrics(preds, labels, train_labels, many_shot_thr=10, low_shot_thr=2):\n",
    "    train_labels = np.array(train_labels).astype(int)\n",
    "\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    elif isinstance(preds, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(f'Type ({type(preds)}) of predictions not supported')\n",
    "\n",
    "    labels = np.array(labels).astype(int)\n",
    "\n",
    "    train_class_count, test_class_count = [], []\n",
    "    mse_per_class, l1_per_class, l1_all_per_class = [], [], []\n",
    "    for l in np.unique(labels):\n",
    "        train_class_count.append(len(train_labels[train_labels == l]))\n",
    "        test_class_count.append(len(labels[labels == l]))\n",
    "        mse_per_class.append(np.sum((preds[labels == l] - labels[labels == l]) ** 2))\n",
    "        l1_per_class.append(np.sum(np.abs(preds[labels == l] - labels[labels == l])))\n",
    "        l1_all_per_class.append(np.abs(preds[labels == l] - labels[labels == l]))\n",
    "\n",
    "    many_shot_mse, median_shot_mse, low_shot_mse = [], [], []\n",
    "    many_shot_l1, median_shot_l1, low_shot_l1 = [], [], []\n",
    "    many_shot_gmean, median_shot_gmean, low_shot_gmean = [], [], []\n",
    "    many_shot_cnt, median_shot_cnt, low_shot_cnt = [], [], []\n",
    "\n",
    "    for i in range(len(train_class_count)):\n",
    "        if train_class_count[i] > many_shot_thr:\n",
    "            many_shot_mse.append(mse_per_class[i])\n",
    "            many_shot_l1.append(l1_per_class[i])\n",
    "            many_shot_gmean += list(l1_all_per_class[i])\n",
    "            many_shot_cnt.append(test_class_count[i])\n",
    "        elif train_class_count[i] < low_shot_thr:\n",
    "            low_shot_mse.append(mse_per_class[i])\n",
    "            low_shot_l1.append(l1_per_class[i])\n",
    "            low_shot_gmean += list(l1_all_per_class[i])\n",
    "            low_shot_cnt.append(test_class_count[i])\n",
    "        else:\n",
    "            median_shot_mse.append(mse_per_class[i])\n",
    "            median_shot_l1.append(l1_per_class[i])\n",
    "            median_shot_gmean += list(l1_all_per_class[i])\n",
    "            median_shot_cnt.append(test_class_count[i])\n",
    "\n",
    "    shot_dict = defaultdict(dict)\n",
    "    shot_dict['many']['mse'] = np.sum(many_shot_mse) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['l1'] = np.sum(many_shot_l1) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['gmean'] = gmean(np.hstack(many_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['median']['mse'] = np.sum(median_shot_mse) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['l1'] = np.sum(median_shot_l1) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['gmean'] = gmean(np.hstack(median_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['low']['mse'] = np.sum(low_shot_mse) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['gmean'] = gmean(np.hstack(low_shot_gmean), axis=None).astype(float)\n",
    "\n",
    "\n",
    "    return shot_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overwrite previous folder: checkpoint\\bostonhousing_fcnet1_inverse_adam_l1_0.001_64 ? [Y/n] :"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\\bostonhousing_fcnet1_inverse_adam_l1_0.001_64 removed.\n",
      "===> Creating folder: checkpoint\\bostonhousing_fcnet1_inverse_adam_l1_0.001_64\n",
      "Args: Namespace(augment=True, batch_size=64, best_loss=100000.0, bmse=False, cpu_only=True, data_dir='./boston.data', dataset='bostonhousing', epoch=10, evaluate=False, gpu=None, img_size=224, lds=False, lds_kernel='gaussian', lds_ks=9, lds_sigma=1, loss='l1', lr=0.001, model='fcnet1', momentum=0.9, optimizer='adam', print_freq=10, resume='', reweight='inverse', schedule=[60, 80], start_epoch=0, store_name='bostonhousing_fcnet1_inverse_adam_l1_0.001_64', store_root='checkpoint', weight_decay=0.0001, workers=32)\n",
      "Store name: bostonhousing_fcnet1_inverse_adam_l1_0.001_64\n",
      "=====> Preparing data...\n",
      "Using re-weighting: [INVERSE]\n",
      "Training data size: 353\n",
      "Validation data size: 77\n",
      "Test data size: 76\n",
      "=====> Building model...\n",
      "Epoch: [0][0/6]\tTime   0.01 (  0.01)\tData 0.0020 (0.0020)\tLoss (L1) 16.343 (16.343)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 259.091 (259.091)\tLoss (L1) 13.085 (13.085)\n",
      " * Overall: MSE 254.222\tL1 12.749\tG-Mean 8.030\n",
      " * Many: MSE 82.201\tL1 6.575\tG-Mean 4.214\n",
      " * Median: MSE 171.981\tL1 11.890\tG-Mean 9.203\n",
      " * Low: MSE 616.966\tL1 23.071\tG-Mean 20.070\n",
      "Best L1 Loss: 12.749\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #0: Train loss [21.8572]; Val loss: MSE [254.2222], L1 [12.7487], G-Mean [8.0296]\n",
      "Epoch: [1][0/6]\tTime   0.01 (  0.01)\tData 0.0030 (0.0030)\tLoss (L1) 10.619 (10.619)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 108.994 (108.994)\tLoss (L1) 8.121 (8.121)\n",
      " * Overall: MSE 104.744\tL1 7.876\tG-Mean 4.893\n",
      " * Many: MSE 40.917\tL1 4.521\tG-Mean 2.819\n",
      " * Median: MSE 49.739\tL1 6.095\tG-Mean 4.976\n",
      " * Low: MSE 277.003\tL1 15.752\tG-Mean 14.490\n",
      "Best L1 Loss: 7.876\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #1: Train loss [8.8698]; Val loss: MSE [104.7442], L1 [7.8761], G-Mean [4.8927]\n",
      "Epoch: [2][0/6]\tTime   0.01 (  0.01)\tData 0.0020 (0.0020)\tLoss (L1) 9.455 (9.455)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 68.960 (68.960)\tLoss (L1) 6.594 (6.594)\n",
      " * Overall: MSE 68.029\tL1 6.544\tG-Mean 4.098\n",
      " * Many: MSE 56.801\tL1 6.282\tG-Mean 4.492\n",
      " * Median: MSE 24.724\tL1 3.835\tG-Mean 2.152\n",
      " * Low: MSE 140.198\tL1 10.491\tG-Mean 8.675\n",
      "Best L1 Loss: 6.544\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #2: Train loss [7.0616]; Val loss: MSE [68.0294], L1 [6.5443], G-Mean [4.0981]\n",
      "Epoch: [3][0/6]\tTime   0.01 (  0.01)\tData 0.0030 (0.0030)\tLoss (L1) 7.691 (7.691)\n",
      "Val: [0/2]\tTime  0.005 ( 0.005)\tLoss (MSE) 75.437 (75.437)\tLoss (L1) 6.511 (6.511)\n",
      " * Overall: MSE 78.973\tL1 6.788\tG-Mean 4.381\n",
      " * Many: MSE 49.488\tL1 5.546\tG-Mean 3.724\n",
      " * Median: MSE 38.407\tL1 4.946\tG-Mean 3.111\n",
      " * Low: MSE 170.648\tL1 11.013\tG-Mean 8.503\n",
      "Best L1 Loss: 6.544\n",
      "Epoch #3: Train loss [5.9717]; Val loss: MSE [78.9732], L1 [6.7885], G-Mean [4.3812]\n",
      "Epoch: [4][0/6]\tTime   0.01 (  0.01)\tData 0.0030 (0.0030)\tLoss (L1) 7.697 (7.697)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 80.864 (80.864)\tLoss (L1) 6.681 (6.681)\n",
      " * Overall: MSE 83.443\tL1 6.898\tG-Mean 4.360\n",
      " * Many: MSE 43.276\tL1 4.851\tG-Mean 2.661\n",
      " * Median: MSE 41.120\tL1 5.353\tG-Mean 3.960\n",
      " * Low: MSE 194.420\tL1 12.034\tG-Mean 9.829\n",
      "Best L1 Loss: 6.544\n",
      "Epoch #4: Train loss [5.7910]; Val loss: MSE [83.4430], L1 [6.8979], G-Mean [4.3598]\n",
      "Epoch: [5][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 5.344 (5.344)\n",
      "Val: [0/2]\tTime  0.006 ( 0.006)\tLoss (MSE) 67.507 (67.507)\tLoss (L1) 6.356 (6.356)\n",
      " * Overall: MSE 69.536\tL1 6.544\tG-Mean 4.488\n",
      " * Many: MSE 57.162\tL1 6.355\tG-Mean 4.899\n",
      " * Median: MSE 29.284\tL1 4.337\tG-Mean 2.864\n",
      " * Low: MSE 136.279\tL1 9.570\tG-Mean 6.340\n",
      "Best L1 Loss: 6.544\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #5: Train loss [5.5620]; Val loss: MSE [69.5362], L1 [6.5438], G-Mean [4.4876]\n",
      "Epoch: [6][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 5.160 (5.160)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 85.991 (85.991)\tLoss (L1) 7.090 (7.090)\n",
      " * Overall: MSE 85.029\tL1 7.032\tG-Mean 4.777\n",
      " * Many: MSE 35.465\tL1 4.216\tG-Mean 2.856\n",
      " * Median: MSE 41.526\tL1 5.645\tG-Mean 4.439\n",
      " * Low: MSE 216.620\tL1 13.415\tG-Mean 12.009\n",
      "Best L1 Loss: 6.544\n",
      "Epoch #6: Train loss [5.6008]; Val loss: MSE [85.0295], L1 [7.0317], G-Mean [4.7765]\n",
      "Epoch: [7][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 5.304 (5.304)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 116.988 (116.988)\tLoss (L1) 7.994 (7.994)\n",
      " * Overall: MSE 118.501\tL1 8.191\tG-Mean 4.652\n",
      " * Many: MSE 74.194\tL1 6.570\tG-Mean 3.400\n",
      " * Median: MSE 50.125\tL1 5.268\tG-Mean 3.002\n",
      " * Low: MSE 258.791\tL1 13.779\tG-Mean 9.766\n",
      "Best L1 Loss: 6.544\n",
      "Epoch #7: Train loss [5.9199]; Val loss: MSE [118.5014], L1 [8.1914], G-Mean [4.6523]\n",
      "Epoch: [8][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 7.843 (7.843)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 68.885 (68.885)\tLoss (L1) 7.024 (7.024)\n",
      " * Overall: MSE 68.585\tL1 6.845\tG-Mean 4.869\n",
      " * Many: MSE 82.091\tL1 7.550\tG-Mean 4.876\n",
      " * Median: MSE 40.475\tL1 5.390\tG-Mean 4.225\n",
      " * Low: MSE 87.550\tL1 7.915\tG-Mean 5.203\n",
      "Best L1 Loss: 6.544\n",
      "Epoch #8: Train loss [6.0746]; Val loss: MSE [68.5852], L1 [6.8447], G-Mean [4.8691]\n",
      "Epoch: [9][0/6]\tTime   0.02 (  0.02)\tData 0.0040 (0.0040)\tLoss (L1) 7.550 (7.550)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 146.240 (146.240)\tLoss (L1) 9.157 (9.157)\n",
      " * Overall: MSE 145.924\tL1 9.100\tG-Mean 5.777\n",
      " * Many: MSE 54.876\tL1 5.096\tG-Mean 3.464\n",
      " * Median: MSE 79.107\tL1 7.534\tG-Mean 5.608\n",
      " * Low: MSE 360.864\tL1 16.808\tG-Mean 13.389\n",
      "Best L1 Loss: 6.544\n",
      "Epoch #9: Train loss [5.5871]; Val loss: MSE [145.9239], L1 [9.0997], G-Mean [5.7770]\n",
      "========================================================================================================================\n",
      "Test best model on testset...\n",
      "Loaded best model, epoch 6, best val loss 6.5438\n",
      "Test: [0/2]\tTime  0.005 ( 0.005)\tLoss (MSE) 53.790 (53.790)\tLoss (L1) 5.631 (5.631)\n",
      " * Overall: MSE 52.932\tL1 5.540\tG-Mean 3.318\n",
      " * Many: MSE 55.222\tL1 6.242\tG-Mean 4.298\n",
      " * Median: MSE 23.219\tL1 3.595\tG-Mean 2.279\n",
      " * Low: MSE 107.535\tL1 8.534\tG-Mean 5.178\n",
      "Test loss: MSE [52.9320], L1 [5.5401], G-Mean [3.3183]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Option 2: To train the inverse weighting model:\n",
    "args.reweight = 'inverse'\n",
    "\n",
    "# train.py Part 2: Train/Evaluate the model.\n",
    "args.store_name = ''\n",
    "args.start_epoch, args.best_loss = 0, 1e5\n",
    "\n",
    "if len(args.store_name):\n",
    "    args.store_name = f'_{args.store_name}'\n",
    "if not args.lds and args.reweight != 'none':\n",
    "    args.store_name += f'_{args.reweight}'\n",
    "if args.lds:\n",
    "    args.store_name += f'_lds_{args.lds_kernel[:3]}_{args.lds_ks}'\n",
    "    if args.lds_kernel in ['gaussian', 'laplace']:\n",
    "        args.store_name += f'_{args.lds_sigma}'\n",
    "if args.bmse:\n",
    "  args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_bmse_{args.lr}_{args.batch_size}\"\n",
    "else:\n",
    "  args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_{args.loss}_{args.lr}_{args.batch_size}\"\n",
    "\n",
    "prepare_folders(args)\n",
    "\n",
    "print(f\"Args: {args}\")\n",
    "print(f\"Store name: {args.store_name}\")\n",
    "\n",
    "def main():\n",
    "    if args.gpu is not None:\n",
    "        print(f\"Use GPU: {args.gpu} for training\")\n",
    "\n",
    "    # Data\n",
    "    print('=====> Preparing data...')\n",
    "    #train_labels = np.loadtxt(args.data_dir+'.train',encoding='utf-16')[:, -1]\n",
    "    train_labels = np.loadtxt(args.data_dir+'.train')[:, -1]\n",
    "    train_dataset = BostonHousing(data_dir=args.data_dir+'.train', split='train',\n",
    "                          reweight=args.reweight, lds=args.lds, lds_kernel=args.lds_kernel, lds_ks=args.lds_ks, lds_sigma=args.lds_sigma)\n",
    "    val_dataset = BostonHousing(data_dir=args.data_dir+'.val', split='val')\n",
    "    test_dataset = BostonHousing(data_dir=args.data_dir+'.test', split='test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                              num_workers=0, pin_memory=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                            num_workers=0, pin_memory=True, drop_last=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                             num_workers=0, pin_memory=True, drop_last=False)\n",
    "    print(f\"Training data size: {len(train_dataset)}\")\n",
    "    print(f\"Validation data size: {len(val_dataset)}\")\n",
    "    print(f\"Test data size: {len(test_dataset)}\")\n",
    "\n",
    "    # Random Seed\n",
    "    np.random.seed(999)\n",
    "    # random.seed(999)\n",
    "    torch.manual_seed(999)\n",
    "\n",
    "    # Model\n",
    "    print('=====> Building model...')\n",
    "    model = fcnet1()\n",
    "    if not args.cpu_only:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # evaluate only\n",
    "    if args.evaluate:\n",
    "        assert args.resume, 'Specify a trained model using [args.resume]'\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        print(f\"===> Checkpoint '{args.resume}' loaded (epoch [{checkpoint['epoch']}]), testing...\")\n",
    "        validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "        return\n",
    "\n",
    "    # Loss and optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr) if args.optimizer == 'adam' else \\\n",
    "        torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(f\"===> Loading checkpoint '{args.resume}'\")\n",
    "            checkpoint = torch.load(args.resume) if args.gpu is None else \\\n",
    "                torch.load(args.resume, map_location=torch.device(f'cuda:{str(args.gpu)}'))\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            args.best_loss = checkpoint['best_loss']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(f\"===> Loaded checkpoint '{args.resume}' (Epoch [{checkpoint['epoch']}])\")\n",
    "        else:\n",
    "            print(f\"===> No checkpoint found at '{args.resume}'\")\n",
    "\n",
    "    if not args.cpu_only:\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epoch):\n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "        train_loss = train(train_loader, model, optimizer, epoch)\n",
    "        val_loss_mse, val_loss_l1, val_loss_gmean = validate(val_loader, model, train_labels=train_labels)\n",
    "\n",
    "        loss_metric = val_loss_mse if args.loss == 'mse' else val_loss_l1\n",
    "        is_best = loss_metric < args.best_loss\n",
    "        args.best_loss = min(loss_metric, args.best_loss)\n",
    "        print(f\"Best {'L1' if 'l1' in args.loss else 'MSE'} Loss: {args.best_loss:.3f}\")\n",
    "        save_checkpoint(args, {\n",
    "            'epoch': epoch + 1,\n",
    "            'model': args.model,\n",
    "            'best_loss': args.best_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print(f\"Epoch #{epoch}: Train loss [{train_loss:.4f}]; \"\n",
    "              f\"Val loss: MSE [{val_loss_mse:.4f}], L1 [{val_loss_l1:.4f}], G-Mean [{val_loss_gmean:.4f}]\")\n",
    "\n",
    "    # test with best checkpoint\n",
    "    print(\"=\" * 120)\n",
    "    print(\"Test best model on testset...\")\n",
    "    checkpoint = torch.load(f\"{args.store_root}/{args.store_name}/ckpt.best.pth.tar\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(f\"Loaded best model, epoch {checkpoint['epoch']}, best val loss {checkpoint['best_loss']:.4f}\")\n",
    "\n",
    "    test_loss_mse, test_loss_l1, test_loss_gmean = validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "    print(f\"Test loss: MSE [{test_loss_mse:.4f}], L1 [{test_loss_l1:.4f}], G-Mean [{test_loss_gmean:.4f}]\\nDone\")\n",
    "\n",
    "def train(train_loader, model, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.2f')\n",
    "    data_time = AverageMeter('Data', ':6.4f')\n",
    "    losses = AverageMeter(f'Loss ({args.loss.upper()})', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch)\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for idx, (inputs, targets, weights) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        if not args.cpu_only:\n",
    "            inputs, targets, weights = \\\n",
    "                inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True), weights.cuda(non_blocking=True)\n",
    "        outputs = model(inputs, targets, epoch)\n",
    "        if args.bmse:\n",
    "          loss = globals()[f\"bmse_loss\"](outputs, targets)\n",
    "        else:\n",
    "          loss = globals()[f\"weighted_{args.loss}_loss\"](outputs, targets, weights)\n",
    "        assert not (np.isnan(loss.item()) or loss.item() > 1e6), f\"Loss explosion: {loss.item()}\"\n",
    "\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if idx % args.print_freq == 0:\n",
    "            progress.display(idx)\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "def validate(val_loader, model, train_labels=None, prefix='Val'):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses_mse = AverageMeter('Loss (MSE)', ':.3f')\n",
    "    losses_l1 = AverageMeter('Loss (L1)', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses_mse, losses_l1],\n",
    "        prefix=f'{prefix}: '\n",
    "    )\n",
    "\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    criterion_gmean = nn.L1Loss(reduction='none')\n",
    "\n",
    "    model.eval()\n",
    "    losses_all = []\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (inputs, targets, _) in enumerate(val_loader):\n",
    "            if not args.cpu_only:\n",
    "                inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds.extend(outputs.data.cpu().numpy())\n",
    "            labels.extend(targets.data.cpu().numpy())\n",
    "\n",
    "            loss_mse = criterion_mse(outputs, targets)\n",
    "            loss_l1 = criterion_l1(outputs, targets)\n",
    "            loss_all = criterion_gmean(outputs, targets)\n",
    "            losses_all.extend(loss_all.cpu().numpy())\n",
    "\n",
    "            losses_mse.update(loss_mse.item(), inputs.size(0))\n",
    "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if idx % args.print_freq == 0:\n",
    "                progress.display(idx)\n",
    "\n",
    "\n",
    "        shot_dict = shot_metrics(np.hstack(preds), np.hstack(labels), train_labels)\n",
    "        loss_gmean = gmean(np.hstack(losses_all), axis=None).astype(float)\n",
    "        print(f\" * Overall: MSE {losses_mse.avg:.3f}\\tL1 {losses_l1.avg:.3f}\\tG-Mean {loss_gmean:.3f}\")\n",
    "        print(f\" * Many: MSE {shot_dict['many']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['many']['l1']:.3f}\\tG-Mean {shot_dict['many']['gmean']:.3f}\")\n",
    "        print(f\" * Median: MSE {shot_dict['median']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['median']['l1']:.3f}\\tG-Mean {shot_dict['median']['gmean']:.3f}\")\n",
    "        print(f\" * Low: MSE {shot_dict['low']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['low']['l1']:.3f}\\tG-Mean {shot_dict['low']['gmean']:.3f}\")\n",
    "\n",
    "    return losses_mse.avg, losses_l1.avg, loss_gmean\n",
    "\n",
    "def shot_metrics(preds, labels, train_labels, many_shot_thr=10, low_shot_thr=2):\n",
    "    train_labels = np.array(train_labels).astype(int)\n",
    "\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    elif isinstance(preds, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(f'Type ({type(preds)}) of predictions not supported')\n",
    "\n",
    "    labels = np.array(labels).astype(int)\n",
    "\n",
    "    train_class_count, test_class_count = [], []\n",
    "    mse_per_class, l1_per_class, l1_all_per_class = [], [], []\n",
    "    for l in np.unique(labels):\n",
    "        train_class_count.append(len(train_labels[train_labels == l]))\n",
    "        test_class_count.append(len(labels[labels == l]))\n",
    "        mse_per_class.append(np.sum((preds[labels == l] - labels[labels == l]) ** 2))\n",
    "        l1_per_class.append(np.sum(np.abs(preds[labels == l] - labels[labels == l])))\n",
    "        l1_all_per_class.append(np.abs(preds[labels == l] - labels[labels == l]))\n",
    "\n",
    "    many_shot_mse, median_shot_mse, low_shot_mse = [], [], []\n",
    "    many_shot_l1, median_shot_l1, low_shot_l1 = [], [], []\n",
    "    many_shot_gmean, median_shot_gmean, low_shot_gmean = [], [], []\n",
    "    many_shot_cnt, median_shot_cnt, low_shot_cnt = [], [], []\n",
    "\n",
    "    for i in range(len(train_class_count)):\n",
    "        if train_class_count[i] > many_shot_thr:\n",
    "            many_shot_mse.append(mse_per_class[i])\n",
    "            many_shot_l1.append(l1_per_class[i])\n",
    "            many_shot_gmean += list(l1_all_per_class[i])\n",
    "            many_shot_cnt.append(test_class_count[i])\n",
    "        elif train_class_count[i] < low_shot_thr:\n",
    "            low_shot_mse.append(mse_per_class[i])\n",
    "            low_shot_l1.append(l1_per_class[i])\n",
    "            low_shot_gmean += list(l1_all_per_class[i])\n",
    "            low_shot_cnt.append(test_class_count[i])\n",
    "        else:\n",
    "            median_shot_mse.append(mse_per_class[i])\n",
    "            median_shot_l1.append(l1_per_class[i])\n",
    "            median_shot_gmean += list(l1_all_per_class[i])\n",
    "            median_shot_cnt.append(test_class_count[i])\n",
    "\n",
    "    shot_dict = defaultdict(dict)\n",
    "    shot_dict['many']['mse'] = np.sum(many_shot_mse) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['l1'] = np.sum(many_shot_l1) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['gmean'] = gmean(np.hstack(many_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['median']['mse'] = np.sum(median_shot_mse) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['l1'] = np.sum(median_shot_l1) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['gmean'] = gmean(np.hstack(median_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['low']['mse'] = np.sum(low_shot_mse) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['gmean'] = gmean(np.hstack(low_shot_gmean), axis=None).astype(float)\n",
    "\n",
    "\n",
    "    return shot_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sqrt-Inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overwrite previous folder: checkpoint\\bostonhousing_fcnet1_sqrt_inv_adam_l1_0.001_64 ? [Y/n] :"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\\bostonhousing_fcnet1_sqrt_inv_adam_l1_0.001_64 removed.\n",
      "===> Creating folder: checkpoint\\bostonhousing_fcnet1_sqrt_inv_adam_l1_0.001_64\n",
      "Args: Namespace(augment=True, batch_size=64, best_loss=100000.0, bmse=False, cpu_only=True, data_dir='./boston.data', dataset='bostonhousing', epoch=10, evaluate=False, gpu=None, img_size=224, lds=False, lds_kernel='gaussian', lds_ks=9, lds_sigma=1, loss='l1', lr=0.001, model='fcnet1', momentum=0.9, optimizer='adam', print_freq=10, resume='', reweight='sqrt_inv', schedule=[60, 80], start_epoch=0, store_name='bostonhousing_fcnet1_sqrt_inv_adam_l1_0.001_64', store_root='checkpoint', weight_decay=0.0001, workers=32)\n",
      "Store name: bostonhousing_fcnet1_sqrt_inv_adam_l1_0.001_64\n",
      "=====> Preparing data...\n",
      "Using re-weighting: [SQRT_INV]\n",
      "Training data size: 353\n",
      "Validation data size: 77\n",
      "Test data size: 76\n",
      "=====> Building model...\n",
      "Epoch: [0][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 17.469 (17.469)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 288.540 (288.540)\tLoss (L1) 13.763 (13.763)\n",
      " * Overall: MSE 283.725\tL1 13.458\tG-Mean 8.825\n",
      " * Many: MSE 85.599\tL1 6.626\tG-Mean 4.419\n",
      " * Median: MSE 204.530\tL1 13.088\tG-Mean 9.835\n",
      " * Low: MSE 685.748\tL1 24.298\tG-Mean 20.422\n",
      "Best L1 Loss: 13.458\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #0: Train loss [22.7432]; Val loss: MSE [283.7248], L1 [13.4580], G-Mean [8.8247]\n",
      "Epoch: [1][0/6]\tTime   0.02 (  0.02)\tData 0.0050 (0.0050)\tLoss (L1) 9.805 (9.805)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 156.036 (156.036)\tLoss (L1) 9.785 (9.785)\n",
      " * Overall: MSE 151.843\tL1 9.386\tG-Mean 5.733\n",
      " * Many: MSE 42.932\tL1 3.934\tG-Mean 2.413\n",
      " * Median: MSE 84.846\tL1 8.602\tG-Mean 7.500\n",
      " * Low: MSE 403.667\tL1 18.833\tG-Mean 15.505\n",
      "Best L1 Loss: 9.386\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #1: Train loss [7.7599]; Val loss: MSE [151.8427], L1 [9.3856], G-Mean [5.7334]\n",
      "Epoch: [2][0/6]\tTime   0.02 (  0.02)\tData 0.0020 (0.0020)\tLoss (L1) 6.995 (6.995)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 90.033 (90.033)\tLoss (L1) 7.406 (7.406)\n",
      " * Overall: MSE 87.075\tL1 7.175\tG-Mean 4.400\n",
      " * Many: MSE 41.825\tL1 4.928\tG-Mean 3.464\n",
      " * Median: MSE 37.722\tL1 4.953\tG-Mean 3.306\n",
      " * Low: MSE 221.960\tL1 13.826\tG-Mean 12.317\n",
      "Best L1 Loss: 7.175\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #2: Train loss [6.4789]; Val loss: MSE [87.0752], L1 [7.1749], G-Mean [4.3998]\n",
      "Epoch: [3][0/6]\tTime   0.02 (  0.02)\tData 0.0020 (0.0020)\tLoss (L1) 6.780 (6.780)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 87.394 (87.394)\tLoss (L1) 7.125 (7.125)\n",
      " * Overall: MSE 86.261\tL1 7.018\tG-Mean 4.704\n",
      " * Many: MSE 37.927\tL1 4.400\tG-Mean 2.928\n",
      " * Median: MSE 37.218\tL1 5.205\tG-Mean 4.074\n",
      " * Low: MSE 221.263\tL1 13.535\tG-Mean 11.888\n",
      "Best L1 Loss: 7.018\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #3: Train loss [5.8257]; Val loss: MSE [86.2611], L1 [7.0182], G-Mean [4.7044]\n",
      "Epoch: [4][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 7.364 (7.364)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 114.735 (114.735)\tLoss (L1) 7.951 (7.951)\n",
      " * Overall: MSE 113.227\tL1 7.834\tG-Mean 4.574\n",
      " * Many: MSE 35.659\tL1 3.431\tG-Mean 1.490\n",
      " * Median: MSE 60.988\tL1 6.954\tG-Mean 5.323\n",
      " * Low: MSE 295.291\tL1 15.675\tG-Mean 13.980\n",
      "Best L1 Loss: 7.018\n",
      "Epoch #4: Train loss [5.3791]; Val loss: MSE [113.2271], L1 [7.8337], G-Mean [4.5740]\n",
      "Epoch: [5][0/6]\tTime   0.01 (  0.01)\tData 0.0030 (0.0030)\tLoss (L1) 5.276 (5.276)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 94.585 (94.585)\tLoss (L1) 7.139 (7.139)\n",
      " * Overall: MSE 92.529\tL1 7.066\tG-Mean 3.965\n",
      " * Many: MSE 38.316\tL1 4.146\tG-Mean 2.404\n",
      " * Median: MSE 39.731\tL1 5.401\tG-Mean 4.096\n",
      " * Low: MSE 239.340\tL1 13.851\tG-Mean 11.797\n",
      "Best L1 Loss: 7.018\n",
      "Epoch #5: Train loss [5.3383]; Val loss: MSE [92.5293], L1 [7.0658], G-Mean [3.9650]\n",
      "Epoch: [6][0/6]\tTime   0.01 (  0.01)\tData 0.0020 (0.0020)\tLoss (L1) 4.841 (4.841)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 94.023 (94.023)\tLoss (L1) 7.294 (7.294)\n",
      " * Overall: MSE 90.429\tL1 7.092\tG-Mean 4.506\n",
      " * Many: MSE 35.248\tL1 3.956\tG-Mean 2.557\n",
      " * Median: MSE 42.362\tL1 5.688\tG-Mean 4.439\n",
      " * Low: MSE 237.124\tL1 14.059\tG-Mean 12.531\n",
      "Best L1 Loss: 7.018\n",
      "Epoch #6: Train loss [5.3108]; Val loss: MSE [90.4286], L1 [7.0922], G-Mean [4.5056]\n",
      "Epoch: [7][0/6]\tTime   0.01 (  0.01)\tData 0.0020 (0.0020)\tLoss (L1) 5.018 (5.018)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 105.271 (105.271)\tLoss (L1) 7.578 (7.578)\n",
      " * Overall: MSE 100.763\tL1 7.334\tG-Mean 4.320\n",
      " * Many: MSE 34.056\tL1 3.403\tG-Mean 1.828\n",
      " * Median: MSE 47.827\tL1 6.215\tG-Mean 5.129\n",
      " * Low: MSE 269.780\tL1 15.039\tG-Mean 13.514\n",
      "Best L1 Loss: 7.018\n",
      "Epoch #7: Train loss [5.2371]; Val loss: MSE [100.7626], L1 [7.3343], G-Mean [4.3196]\n",
      "Epoch: [8][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 5.971 (5.971)\n",
      "Val: [0/2]\tTime  0.005 ( 0.005)\tLoss (MSE) 102.711 (102.711)\tLoss (L1) 7.325 (7.325)\n",
      " * Overall: MSE 98.599\tL1 7.180\tG-Mean 4.117\n",
      " * Many: MSE 38.254\tL1 3.810\tG-Mean 1.927\n",
      " * Median: MSE 41.430\tL1 5.568\tG-Mean 4.236\n",
      " * Low: MSE 259.824\tL1 14.449\tG-Mean 12.371\n",
      "Best L1 Loss: 7.018\n",
      "Epoch #8: Train loss [5.2486]; Val loss: MSE [98.5993], L1 [7.1796], G-Mean [4.1174]\n",
      "Epoch: [9][0/6]\tTime   0.02 (  0.02)\tData 0.0020 (0.0020)\tLoss (L1) 5.035 (5.035)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 153.661 (153.661)\tLoss (L1) 9.411 (9.411)\n",
      " * Overall: MSE 147.889\tL1 9.099\tG-Mean 5.717\n",
      " * Many: MSE 48.119\tL1 4.258\tG-Mean 2.458\n",
      " * Median: MSE 76.873\tL1 7.806\tG-Mean 6.371\n",
      " * Low: MSE 384.634\tL1 17.818\tG-Mean 15.112\n",
      "Best L1 Loss: 7.018\n",
      "Epoch #9: Train loss [5.0553]; Val loss: MSE [147.8892], L1 [9.0993], G-Mean [5.7166]\n",
      "========================================================================================================================\n",
      "Test best model on testset...\n",
      "Loaded best model, epoch 4, best val loss 7.0182\n",
      "Test: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 65.246 (65.246)\tLoss (L1) 6.225 (6.225)\n",
      " * Overall: MSE 60.561\tL1 6.058\tG-Mean 4.181\n",
      " * Many: MSE 21.516\tL1 3.975\tG-Mean 3.246\n",
      " * Median: MSE 30.872\tL1 4.712\tG-Mean 3.279\n",
      " * Low: MSE 174.302\tL1 12.160\tG-Mean 10.824\n",
      "Test loss: MSE [60.5615], L1 [6.0579], G-Mean [4.1815]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Option 3: To train the sqrt_inverse weighting model:\n",
    "args.reweight = 'sqrt_inv'\n",
    "\n",
    "# train.py Part 2: Train/Evaluate the model.\n",
    "args.store_name = ''\n",
    "args.start_epoch, args.best_loss = 0, 1e5\n",
    "\n",
    "if len(args.store_name):\n",
    "    args.store_name = f'_{args.store_name}'\n",
    "if not args.lds and args.reweight != 'none':\n",
    "    args.store_name += f'_{args.reweight}'\n",
    "if args.lds:\n",
    "    args.store_name += f'_lds_{args.lds_kernel[:3]}_{args.lds_ks}'\n",
    "    if args.lds_kernel in ['gaussian', 'laplace']:\n",
    "        args.store_name += f'_{args.lds_sigma}'\n",
    "if args.bmse:\n",
    "  args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_bmse_{args.lr}_{args.batch_size}\"\n",
    "else:\n",
    "  args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_{args.loss}_{args.lr}_{args.batch_size}\"\n",
    "\n",
    "prepare_folders(args)\n",
    "\n",
    "print(f\"Args: {args}\")\n",
    "print(f\"Store name: {args.store_name}\")\n",
    "\n",
    "def main():\n",
    "    if args.gpu is not None:\n",
    "        print(f\"Use GPU: {args.gpu} for training\")\n",
    "\n",
    "    # Data\n",
    "    print('=====> Preparing data...')\n",
    "    #train_labels = np.loadtxt(args.data_dir+'.train',encoding='utf-16')[:, -1]\n",
    "    train_labels = np.loadtxt(args.data_dir+'.train')[:, -1]\n",
    "    train_dataset = BostonHousing(data_dir=args.data_dir+'.train', split='train',\n",
    "                          reweight=args.reweight, lds=args.lds, lds_kernel=args.lds_kernel, lds_ks=args.lds_ks, lds_sigma=args.lds_sigma)\n",
    "    val_dataset = BostonHousing(data_dir=args.data_dir+'.val', split='val')\n",
    "    test_dataset = BostonHousing(data_dir=args.data_dir+'.test', split='test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                              num_workers=0, pin_memory=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                            num_workers=0, pin_memory=True, drop_last=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                             num_workers=0, pin_memory=True, drop_last=False)\n",
    "    print(f\"Training data size: {len(train_dataset)}\")\n",
    "    print(f\"Validation data size: {len(val_dataset)}\")\n",
    "    print(f\"Test data size: {len(test_dataset)}\")\n",
    "\n",
    "    # Random Seed\n",
    "    np.random.seed(999)\n",
    "    # random.seed(999)\n",
    "    torch.manual_seed(999)\n",
    "\n",
    "    # Model\n",
    "    print('=====> Building model...')\n",
    "    model = fcnet1()\n",
    "    if not args.cpu_only:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # evaluate only\n",
    "    if args.evaluate:\n",
    "        assert args.resume, 'Specify a trained model using [args.resume]'\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        print(f\"===> Checkpoint '{args.resume}' loaded (epoch [{checkpoint['epoch']}]), testing...\")\n",
    "        validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "        return\n",
    "\n",
    "    # Loss and optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr) if args.optimizer == 'adam' else \\\n",
    "        torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(f\"===> Loading checkpoint '{args.resume}'\")\n",
    "            checkpoint = torch.load(args.resume) if args.gpu is None else \\\n",
    "                torch.load(args.resume, map_location=torch.device(f'cuda:{str(args.gpu)}'))\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            args.best_loss = checkpoint['best_loss']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(f\"===> Loaded checkpoint '{args.resume}' (Epoch [{checkpoint['epoch']}])\")\n",
    "        else:\n",
    "            print(f\"===> No checkpoint found at '{args.resume}'\")\n",
    "\n",
    "    if not args.cpu_only:\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epoch):\n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "        train_loss = train(train_loader, model, optimizer, epoch)\n",
    "        val_loss_mse, val_loss_l1, val_loss_gmean = validate(val_loader, model, train_labels=train_labels)\n",
    "\n",
    "        loss_metric = val_loss_mse if args.loss == 'mse' else val_loss_l1\n",
    "        is_best = loss_metric < args.best_loss\n",
    "        args.best_loss = min(loss_metric, args.best_loss)\n",
    "        print(f\"Best {'L1' if 'l1' in args.loss else 'MSE'} Loss: {args.best_loss:.3f}\")\n",
    "        save_checkpoint(args, {\n",
    "            'epoch': epoch + 1,\n",
    "            'model': args.model,\n",
    "            'best_loss': args.best_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print(f\"Epoch #{epoch}: Train loss [{train_loss:.4f}]; \"\n",
    "              f\"Val loss: MSE [{val_loss_mse:.4f}], L1 [{val_loss_l1:.4f}], G-Mean [{val_loss_gmean:.4f}]\")\n",
    "\n",
    "    # test with best checkpoint\n",
    "    print(\"=\" * 120)\n",
    "    print(\"Test best model on testset...\")\n",
    "    checkpoint = torch.load(f\"{args.store_root}/{args.store_name}/ckpt.best.pth.tar\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(f\"Loaded best model, epoch {checkpoint['epoch']}, best val loss {checkpoint['best_loss']:.4f}\")\n",
    "\n",
    "    test_loss_mse, test_loss_l1, test_loss_gmean = validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "    print(f\"Test loss: MSE [{test_loss_mse:.4f}], L1 [{test_loss_l1:.4f}], G-Mean [{test_loss_gmean:.4f}]\\nDone\")\n",
    "\n",
    "def train(train_loader, model, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.2f')\n",
    "    data_time = AverageMeter('Data', ':6.4f')\n",
    "    losses = AverageMeter(f'Loss ({args.loss.upper()})', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch)\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for idx, (inputs, targets, weights) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        if not args.cpu_only:\n",
    "            inputs, targets, weights = \\\n",
    "                inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True), weights.cuda(non_blocking=True)\n",
    "        outputs = model(inputs, targets, epoch)\n",
    "        if args.bmse:\n",
    "          loss = globals()[f\"bmse_loss\"](outputs, targets)\n",
    "        else:\n",
    "          loss = globals()[f\"weighted_{args.loss}_loss\"](outputs, targets, weights)\n",
    "        assert not (np.isnan(loss.item()) or loss.item() > 1e6), f\"Loss explosion: {loss.item()}\"\n",
    "\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if idx % args.print_freq == 0:\n",
    "            progress.display(idx)\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "def validate(val_loader, model, train_labels=None, prefix='Val'):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses_mse = AverageMeter('Loss (MSE)', ':.3f')\n",
    "    losses_l1 = AverageMeter('Loss (L1)', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses_mse, losses_l1],\n",
    "        prefix=f'{prefix}: '\n",
    "    )\n",
    "\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    criterion_gmean = nn.L1Loss(reduction='none')\n",
    "\n",
    "    model.eval()\n",
    "    losses_all = []\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (inputs, targets, _) in enumerate(val_loader):\n",
    "            if not args.cpu_only:\n",
    "                inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds.extend(outputs.data.cpu().numpy())\n",
    "            labels.extend(targets.data.cpu().numpy())\n",
    "\n",
    "            loss_mse = criterion_mse(outputs, targets)\n",
    "            loss_l1 = criterion_l1(outputs, targets)\n",
    "            loss_all = criterion_gmean(outputs, targets)\n",
    "            losses_all.extend(loss_all.cpu().numpy())\n",
    "\n",
    "            losses_mse.update(loss_mse.item(), inputs.size(0))\n",
    "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if idx % args.print_freq == 0:\n",
    "                progress.display(idx)\n",
    "\n",
    "\n",
    "        shot_dict = shot_metrics(np.hstack(preds), np.hstack(labels), train_labels)\n",
    "        loss_gmean = gmean(np.hstack(losses_all), axis=None).astype(float)\n",
    "        print(f\" * Overall: MSE {losses_mse.avg:.3f}\\tL1 {losses_l1.avg:.3f}\\tG-Mean {loss_gmean:.3f}\")\n",
    "        print(f\" * Many: MSE {shot_dict['many']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['many']['l1']:.3f}\\tG-Mean {shot_dict['many']['gmean']:.3f}\")\n",
    "        print(f\" * Median: MSE {shot_dict['median']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['median']['l1']:.3f}\\tG-Mean {shot_dict['median']['gmean']:.3f}\")\n",
    "        print(f\" * Low: MSE {shot_dict['low']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['low']['l1']:.3f}\\tG-Mean {shot_dict['low']['gmean']:.3f}\")\n",
    "\n",
    "    return losses_mse.avg, losses_l1.avg, loss_gmean\n",
    "\n",
    "def shot_metrics(preds, labels, train_labels, many_shot_thr=10, low_shot_thr=2):\n",
    "    train_labels = np.array(train_labels).astype(int)\n",
    "\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    elif isinstance(preds, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(f'Type ({type(preds)}) of predictions not supported')\n",
    "\n",
    "    labels = np.array(labels).astype(int)\n",
    "\n",
    "    train_class_count, test_class_count = [], []\n",
    "    mse_per_class, l1_per_class, l1_all_per_class = [], [], []\n",
    "    for l in np.unique(labels):\n",
    "        train_class_count.append(len(train_labels[train_labels == l]))\n",
    "        test_class_count.append(len(labels[labels == l]))\n",
    "        mse_per_class.append(np.sum((preds[labels == l] - labels[labels == l]) ** 2))\n",
    "        l1_per_class.append(np.sum(np.abs(preds[labels == l] - labels[labels == l])))\n",
    "        l1_all_per_class.append(np.abs(preds[labels == l] - labels[labels == l]))\n",
    "\n",
    "    many_shot_mse, median_shot_mse, low_shot_mse = [], [], []\n",
    "    many_shot_l1, median_shot_l1, low_shot_l1 = [], [], []\n",
    "    many_shot_gmean, median_shot_gmean, low_shot_gmean = [], [], []\n",
    "    many_shot_cnt, median_shot_cnt, low_shot_cnt = [], [], []\n",
    "\n",
    "    for i in range(len(train_class_count)):\n",
    "        if train_class_count[i] > many_shot_thr:\n",
    "            many_shot_mse.append(mse_per_class[i])\n",
    "            many_shot_l1.append(l1_per_class[i])\n",
    "            many_shot_gmean += list(l1_all_per_class[i])\n",
    "            many_shot_cnt.append(test_class_count[i])\n",
    "        elif train_class_count[i] < low_shot_thr:\n",
    "            low_shot_mse.append(mse_per_class[i])\n",
    "            low_shot_l1.append(l1_per_class[i])\n",
    "            low_shot_gmean += list(l1_all_per_class[i])\n",
    "            low_shot_cnt.append(test_class_count[i])\n",
    "        else:\n",
    "            median_shot_mse.append(mse_per_class[i])\n",
    "            median_shot_l1.append(l1_per_class[i])\n",
    "            median_shot_gmean += list(l1_all_per_class[i])\n",
    "            median_shot_cnt.append(test_class_count[i])\n",
    "\n",
    "    shot_dict = defaultdict(dict)\n",
    "    shot_dict['many']['mse'] = np.sum(many_shot_mse) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['l1'] = np.sum(many_shot_l1) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['gmean'] = gmean(np.hstack(many_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['median']['mse'] = np.sum(median_shot_mse) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['l1'] = np.sum(median_shot_l1) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['gmean'] = gmean(np.hstack(median_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['low']['mse'] = np.sum(low_shot_mse) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['gmean'] = gmean(np.hstack(low_shot_gmean), axis=None).astype(float)\n",
    "\n",
    "\n",
    "    return shot_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSD-INV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overwrite previous folder: checkpoint\\bostonhousing_fcnet1_lds_gau_5_2_adam_l1_0.001_64 ? [Y/n] :"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\\bostonhousing_fcnet1_lds_gau_5_2_adam_l1_0.001_64 removed.\n",
      "===> Creating folder: checkpoint\\bostonhousing_fcnet1_lds_gau_5_2_adam_l1_0.001_64\n",
      "Args: Namespace(augment=True, batch_size=64, best_loss=100000.0, bmse=False, cpu_only=True, data_dir='./boston.data', dataset='bostonhousing', epoch=10, evaluate=False, gpu=None, img_size=224, lds=True, lds_kernel='gaussian', lds_ks=5, lds_sigma=2, loss='l1', lr=0.001, model='fcnet1', momentum=0.9, optimizer='adam', print_freq=10, resume='', reweight='sqrt_inv', schedule=[60, 80], start_epoch=0, store_name='bostonhousing_fcnet1_lds_gau_5_2_adam_l1_0.001_64', store_root='checkpoint', weight_decay=0.0001, workers=32)\n",
      "Store name: bostonhousing_fcnet1_lds_gau_5_2_adam_l1_0.001_64\n",
      "=====> Preparing data...\n",
      "Using re-weighting: [SQRT_INV]\n",
      "Using LDS: [GAUSSIAN] (5/2)\n",
      "Training data size: 353\n",
      "Validation data size: 77\n",
      "Test data size: 76\n",
      "=====> Building model...\n",
      "Epoch: [0][0/6]\tTime   0.02 (  0.02)\tData 0.0010 (0.0010)\tLoss (L1) 21.311 (21.311)\n",
      "Val: [0/2]\tTime  0.005 ( 0.005)\tLoss (MSE) 291.202 (291.202)\tLoss (L1) 13.904 (13.904)\n",
      " * Overall: MSE 285.351\tL1 13.539\tG-Mean 9.253\n",
      " * Many: MSE 83.092\tL1 6.328\tG-Mean 4.213\n",
      " * Median: MSE 207.602\tL1 13.482\tG-Mean 11.972\n",
      " * Low: MSE 694.236\tL1 24.671\tG-Mean 21.789\n",
      "Best L1 Loss: 13.539\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #0: Train loss [23.4593]; Val loss: MSE [285.3506], L1 [13.5387], G-Mean [9.2528]\n",
      "Epoch: [1][0/6]\tTime   0.02 (  0.02)\tData 0.0040 (0.0040)\tLoss (L1) 14.105 (14.105)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 124.174 (124.174)\tLoss (L1) 8.740 (8.740)\n",
      " * Overall: MSE 119.240\tL1 8.593\tG-Mean 5.816\n",
      " * Many: MSE 53.026\tL1 5.561\tG-Mean 3.668\n",
      " * Median: MSE 67.181\tL1 6.608\tG-Mean 5.056\n",
      " * Low: MSE 295.793\tL1 16.333\tG-Mean 15.074\n",
      "Best L1 Loss: 8.593\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #1: Train loss [10.5308]; Val loss: MSE [119.2400], L1 [8.5935], G-Mean [5.8159]\n",
      "Epoch: [2][0/6]\tTime   0.02 (  0.02)\tData 0.0040 (0.0040)\tLoss (L1) 8.512 (8.512)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 93.068 (93.068)\tLoss (L1) 7.468 (7.468)\n",
      " * Overall: MSE 90.604\tL1 7.205\tG-Mean 4.535\n",
      " * Many: MSE 42.247\tL1 4.802\tG-Mean 3.274\n",
      " * Median: MSE 36.664\tL1 4.948\tG-Mean 3.315\n",
      " * Low: MSE 232.629\tL1 14.002\tG-Mean 11.695\n",
      "Best L1 Loss: 7.205\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #2: Train loss [9.1918]; Val loss: MSE [90.6041], L1 [7.2052], G-Mean [4.5346]\n",
      "Epoch: [3][0/6]\tTime   0.02 (  0.02)\tData 0.0040 (0.0040)\tLoss (L1) 8.611 (8.611)\n",
      "Val: [0/2]\tTime  0.002 ( 0.002)\tLoss (MSE) 84.084 (84.084)\tLoss (L1) 7.102 (7.102)\n",
      " * Overall: MSE 82.928\tL1 7.079\tG-Mean 4.580\n",
      " * Many: MSE 42.422\tL1 4.984\tG-Mean 3.174\n",
      " * Median: MSE 46.198\tL1 5.556\tG-Mean 3.463\n",
      " * Low: MSE 196.527\tL1 12.652\tG-Mean 10.884\n",
      "Best L1 Loss: 7.079\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #3: Train loss [8.5614]; Val loss: MSE [82.9284], L1 [7.0788], G-Mean [4.5803]\n",
      "Epoch: [4][0/6]\tTime   0.02 (  0.02)\tData 0.0040 (0.0040)\tLoss (L1) 12.192 (12.192)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 104.778 (104.778)\tLoss (L1) 7.484 (7.484)\n",
      " * Overall: MSE 105.368\tL1 7.521\tG-Mean 4.394\n",
      " * Many: MSE 39.267\tL1 3.997\tG-Mean 2.116\n",
      " * Median: MSE 56.252\tL1 6.368\tG-Mean 4.607\n",
      " * Low: MSE 264.256\tL1 14.273\tG-Mean 11.834\n",
      "Best L1 Loss: 7.079\n",
      "Epoch #4: Train loss [8.0211]; Val loss: MSE [105.3681], L1 [7.5214], G-Mean [4.3945]\n",
      "Epoch: [5][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 6.936 (6.936)\n",
      "Val: [0/2]\tTime  0.005 ( 0.005)\tLoss (MSE) 74.099 (74.099)\tLoss (L1) 6.601 (6.601)\n",
      " * Overall: MSE 75.351\tL1 6.619\tG-Mean 4.331\n",
      " * Many: MSE 47.670\tL1 5.344\tG-Mean 3.754\n",
      " * Median: MSE 39.859\tL1 5.266\tG-Mean 3.904\n",
      " * Low: MSE 163.482\tL1 10.510\tG-Mean 5.995\n",
      "Best L1 Loss: 6.619\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #5: Train loss [7.9697]; Val loss: MSE [75.3508], L1 [6.6191], G-Mean [4.3306]\n",
      "Epoch: [6][0/6]\tTime   0.02 (  0.02)\tData 0.0040 (0.0040)\tLoss (L1) 7.348 (7.348)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 89.956 (89.956)\tLoss (L1) 7.257 (7.257)\n",
      " * Overall: MSE 87.894\tL1 7.257\tG-Mean 4.760\n",
      " * Many: MSE 44.105\tL1 5.105\tG-Mean 3.161\n",
      " * Median: MSE 51.065\tL1 5.736\tG-Mean 4.020\n",
      " * Low: MSE 207.751\tL1 12.959\tG-Mean 11.117\n",
      "Best L1 Loss: 6.619\n",
      "Epoch #6: Train loss [7.9279]; Val loss: MSE [87.8942], L1 [7.2573], G-Mean [4.7600]\n",
      "Epoch: [7][0/6]\tTime   0.02 (  0.02)\tData 0.0040 (0.0040)\tLoss (L1) 6.653 (6.653)\n",
      "Val: [0/2]\tTime  0.006 ( 0.006)\tLoss (MSE) 89.402 (89.402)\tLoss (L1) 6.843 (6.843)\n",
      " * Overall: MSE 89.146\tL1 6.852\tG-Mean 3.982\n",
      " * Many: MSE 40.139\tL1 4.286\tG-Mean 2.694\n",
      " * Median: MSE 42.446\tL1 5.455\tG-Mean 3.907\n",
      " * Low: MSE 220.666\tL1 12.727\tG-Mean 9.795\n",
      "Best L1 Loss: 6.619\n",
      "Epoch #7: Train loss [7.9132]; Val loss: MSE [89.1462], L1 [6.8520], G-Mean [3.9819]\n",
      "Epoch: [8][0/6]\tTime   0.02 (  0.02)\tData 0.0020 (0.0020)\tLoss (L1) 12.095 (12.095)\n",
      "Val: [0/2]\tTime  0.007 ( 0.007)\tLoss (MSE) 80.136 (80.136)\tLoss (L1) 6.562 (6.562)\n",
      " * Overall: MSE 81.240\tL1 6.600\tG-Mean 3.812\n",
      " * Many: MSE 43.982\tL1 4.819\tG-Mean 3.244\n",
      " * Median: MSE 41.478\tL1 5.285\tG-Mean 3.687\n",
      " * Low: MSE 187.844\tL1 11.244\tG-Mean 7.242\n",
      "Best L1 Loss: 6.600\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #8: Train loss [7.9594]; Val loss: MSE [81.2402], L1 [6.5998], G-Mean [3.8119]\n",
      "Epoch: [9][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 7.950 (7.950)\n",
      "Val: [0/2]\tTime  0.005 ( 0.005)\tLoss (MSE) 129.561 (129.561)\tLoss (L1) 8.472 (8.472)\n",
      " * Overall: MSE 127.459\tL1 8.367\tG-Mean 5.114\n",
      " * Many: MSE 49.441\tL1 4.607\tG-Mean 2.975\n",
      " * Median: MSE 66.977\tL1 7.037\tG-Mean 5.459\n",
      " * Low: MSE 315.935\tL1 15.449\tG-Mean 10.302\n",
      "Best L1 Loss: 6.600\n",
      "Epoch #9: Train loss [7.6786]; Val loss: MSE [127.4590], L1 [8.3667], G-Mean [5.1143]\n",
      "========================================================================================================================\n",
      "Test best model on testset...\n",
      "Loaded best model, epoch 9, best val loss 6.5998\n",
      "Test: [0/2]\tTime  0.010 ( 0.010)\tLoss (MSE) 62.481 (62.481)\tLoss (L1) 6.128 (6.128)\n",
      " * Overall: MSE 58.755\tL1 6.019\tG-Mean 3.926\n",
      " * Many: MSE 29.411\tL1 4.424\tG-Mean 3.122\n",
      " * Median: MSE 35.703\tL1 5.179\tG-Mean 3.821\n",
      " * Low: MSE 147.452\tL1 10.342\tG-Mean 6.214\n",
      "Test loss: MSE [58.7546], L1 [6.0185], G-Mean [3.9262]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Option 4: To train the Label Distribution Smoothing (LDS) model:\n",
    "args.reweight = 'sqrt_inv'\n",
    "args.lds = True\n",
    "args.lds_kernel = 'gaussian'\n",
    "args.lds_ks = 5 # 5\n",
    "args.lds_sigma = 2 # 2\n",
    "\n",
    "\n",
    "# train.py Part 2: Train/Evaluate the model.\n",
    "args.store_name = ''\n",
    "args.start_epoch, args.best_loss = 0, 1e5\n",
    "\n",
    "if len(args.store_name):\n",
    "    args.store_name = f'_{args.store_name}'\n",
    "if not args.lds and args.reweight != 'none':\n",
    "    args.store_name += f'_{args.reweight}'\n",
    "if args.lds:\n",
    "    args.store_name += f'_lds_{args.lds_kernel[:3]}_{args.lds_ks}'\n",
    "    if args.lds_kernel in ['gaussian', 'laplace']:\n",
    "        args.store_name += f'_{args.lds_sigma}'\n",
    "if args.bmse:\n",
    "  args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_bmse_{args.lr}_{args.batch_size}\"\n",
    "else:\n",
    "  args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_{args.loss}_{args.lr}_{args.batch_size}\"\n",
    "\n",
    "prepare_folders(args)\n",
    "\n",
    "print(f\"Args: {args}\")\n",
    "print(f\"Store name: {args.store_name}\")\n",
    "\n",
    "def main():\n",
    "    if args.gpu is not None:\n",
    "        print(f\"Use GPU: {args.gpu} for training\")\n",
    "\n",
    "    # Data\n",
    "    print('=====> Preparing data...')\n",
    "    #train_labels = np.loadtxt(args.data_dir+'.train',encoding='utf-16')[:, -1]\n",
    "    train_labels = np.loadtxt(args.data_dir+'.train')[:, -1]\n",
    "    train_dataset = BostonHousing(data_dir=args.data_dir+'.train', split='train',\n",
    "                          reweight=args.reweight, lds=args.lds, lds_kernel=args.lds_kernel, lds_ks=args.lds_ks, lds_sigma=args.lds_sigma)\n",
    "    val_dataset = BostonHousing(data_dir=args.data_dir+'.val', split='val')\n",
    "    test_dataset = BostonHousing(data_dir=args.data_dir+'.test', split='test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                              num_workers=0, pin_memory=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                            num_workers=0, pin_memory=True, drop_last=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                             num_workers=0, pin_memory=True, drop_last=False)\n",
    "    print(f\"Training data size: {len(train_dataset)}\")\n",
    "    print(f\"Validation data size: {len(val_dataset)}\")\n",
    "    print(f\"Test data size: {len(test_dataset)}\")\n",
    "\n",
    "    # Random Seed\n",
    "    np.random.seed(999)\n",
    "    # random.seed(999)\n",
    "    torch.manual_seed(999)\n",
    "\n",
    "    # Model\n",
    "    print('=====> Building model...')\n",
    "    model = fcnet1()\n",
    "    if not args.cpu_only:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # evaluate only\n",
    "    if args.evaluate:\n",
    "        assert args.resume, 'Specify a trained model using [args.resume]'\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        print(f\"===> Checkpoint '{args.resume}' loaded (epoch [{checkpoint['epoch']}]), testing...\")\n",
    "        validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "        return\n",
    "\n",
    "    # Loss and optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr) if args.optimizer == 'adam' else \\\n",
    "        torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(f\"===> Loading checkpoint '{args.resume}'\")\n",
    "            checkpoint = torch.load(args.resume) if args.gpu is None else \\\n",
    "                torch.load(args.resume, map_location=torch.device(f'cuda:{str(args.gpu)}'))\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            args.best_loss = checkpoint['best_loss']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(f\"===> Loaded checkpoint '{args.resume}' (Epoch [{checkpoint['epoch']}])\")\n",
    "        else:\n",
    "            print(f\"===> No checkpoint found at '{args.resume}'\")\n",
    "\n",
    "    if not args.cpu_only:\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epoch):\n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "        train_loss = train(train_loader, model, optimizer, epoch)\n",
    "        val_loss_mse, val_loss_l1, val_loss_gmean = validate(val_loader, model, train_labels=train_labels)\n",
    "\n",
    "        loss_metric = val_loss_mse if args.loss == 'mse' else val_loss_l1\n",
    "        is_best = loss_metric < args.best_loss\n",
    "        args.best_loss = min(loss_metric, args.best_loss)\n",
    "        print(f\"Best {'L1' if 'l1' in args.loss else 'MSE'} Loss: {args.best_loss:.3f}\")\n",
    "        save_checkpoint(args, {\n",
    "            'epoch': epoch + 1,\n",
    "            'model': args.model,\n",
    "            'best_loss': args.best_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print(f\"Epoch #{epoch}: Train loss [{train_loss:.4f}]; \"\n",
    "              f\"Val loss: MSE [{val_loss_mse:.4f}], L1 [{val_loss_l1:.4f}], G-Mean [{val_loss_gmean:.4f}]\")\n",
    "\n",
    "    # test with best checkpoint\n",
    "    print(\"=\" * 120)\n",
    "    print(\"Test best model on testset...\")\n",
    "    checkpoint = torch.load(f\"{args.store_root}/{args.store_name}/ckpt.best.pth.tar\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(f\"Loaded best model, epoch {checkpoint['epoch']}, best val loss {checkpoint['best_loss']:.4f}\")\n",
    "\n",
    "    test_loss_mse, test_loss_l1, test_loss_gmean = validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "    print(f\"Test loss: MSE [{test_loss_mse:.4f}], L1 [{test_loss_l1:.4f}], G-Mean [{test_loss_gmean:.4f}]\\nDone\")\n",
    "\n",
    "def train(train_loader, model, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.2f')\n",
    "    data_time = AverageMeter('Data', ':6.4f')\n",
    "    losses = AverageMeter(f'Loss ({args.loss.upper()})', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch)\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for idx, (inputs, targets, weights) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        if not args.cpu_only:\n",
    "            inputs, targets, weights = \\\n",
    "                inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True), weights.cuda(non_blocking=True)\n",
    "        outputs = model(inputs, targets, epoch)\n",
    "        if args.bmse:\n",
    "          loss = globals()[f\"bmse_loss\"](outputs, targets)\n",
    "        else:\n",
    "          loss = globals()[f\"weighted_{args.loss}_loss\"](outputs, targets, weights)\n",
    "        assert not (np.isnan(loss.item()) or loss.item() > 1e6), f\"Loss explosion: {loss.item()}\"\n",
    "\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if idx % args.print_freq == 0:\n",
    "            progress.display(idx)\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "def validate(val_loader, model, train_labels=None, prefix='Val'):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses_mse = AverageMeter('Loss (MSE)', ':.3f')\n",
    "    losses_l1 = AverageMeter('Loss (L1)', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses_mse, losses_l1],\n",
    "        prefix=f'{prefix}: '\n",
    "    )\n",
    "\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    criterion_gmean = nn.L1Loss(reduction='none')\n",
    "\n",
    "    model.eval()\n",
    "    losses_all = []\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (inputs, targets, _) in enumerate(val_loader):\n",
    "            if not args.cpu_only:\n",
    "                inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds.extend(outputs.data.cpu().numpy())\n",
    "            labels.extend(targets.data.cpu().numpy())\n",
    "\n",
    "            loss_mse = criterion_mse(outputs, targets)\n",
    "            loss_l1 = criterion_l1(outputs, targets)\n",
    "            loss_all = criterion_gmean(outputs, targets)\n",
    "            losses_all.extend(loss_all.cpu().numpy())\n",
    "\n",
    "            losses_mse.update(loss_mse.item(), inputs.size(0))\n",
    "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if idx % args.print_freq == 0:\n",
    "                progress.display(idx)\n",
    "\n",
    "\n",
    "        shot_dict = shot_metrics(np.hstack(preds), np.hstack(labels), train_labels)\n",
    "        loss_gmean = gmean(np.hstack(losses_all), axis=None).astype(float)\n",
    "        print(f\" * Overall: MSE {losses_mse.avg:.3f}\\tL1 {losses_l1.avg:.3f}\\tG-Mean {loss_gmean:.3f}\")\n",
    "        print(f\" * Many: MSE {shot_dict['many']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['many']['l1']:.3f}\\tG-Mean {shot_dict['many']['gmean']:.3f}\")\n",
    "        print(f\" * Median: MSE {shot_dict['median']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['median']['l1']:.3f}\\tG-Mean {shot_dict['median']['gmean']:.3f}\")\n",
    "        print(f\" * Low: MSE {shot_dict['low']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['low']['l1']:.3f}\\tG-Mean {shot_dict['low']['gmean']:.3f}\")\n",
    "\n",
    "    return losses_mse.avg, losses_l1.avg, loss_gmean\n",
    "\n",
    "def shot_metrics(preds, labels, train_labels, many_shot_thr=10, low_shot_thr=2):\n",
    "    train_labels = np.array(train_labels).astype(int)\n",
    "\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    elif isinstance(preds, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(f'Type ({type(preds)}) of predictions not supported')\n",
    "\n",
    "    labels = np.array(labels).astype(int)\n",
    "\n",
    "    train_class_count, test_class_count = [], []\n",
    "    mse_per_class, l1_per_class, l1_all_per_class = [], [], []\n",
    "    for l in np.unique(labels):\n",
    "        train_class_count.append(len(train_labels[train_labels == l]))\n",
    "        test_class_count.append(len(labels[labels == l]))\n",
    "        mse_per_class.append(np.sum((preds[labels == l] - labels[labels == l]) ** 2))\n",
    "        l1_per_class.append(np.sum(np.abs(preds[labels == l] - labels[labels == l])))\n",
    "        l1_all_per_class.append(np.abs(preds[labels == l] - labels[labels == l]))\n",
    "\n",
    "    many_shot_mse, median_shot_mse, low_shot_mse = [], [], []\n",
    "    many_shot_l1, median_shot_l1, low_shot_l1 = [], [], []\n",
    "    many_shot_gmean, median_shot_gmean, low_shot_gmean = [], [], []\n",
    "    many_shot_cnt, median_shot_cnt, low_shot_cnt = [], [], []\n",
    "\n",
    "    for i in range(len(train_class_count)):\n",
    "        if train_class_count[i] > many_shot_thr:\n",
    "            many_shot_mse.append(mse_per_class[i])\n",
    "            many_shot_l1.append(l1_per_class[i])\n",
    "            many_shot_gmean += list(l1_all_per_class[i])\n",
    "            many_shot_cnt.append(test_class_count[i])\n",
    "        elif train_class_count[i] < low_shot_thr:\n",
    "            low_shot_mse.append(mse_per_class[i])\n",
    "            low_shot_l1.append(l1_per_class[i])\n",
    "            low_shot_gmean += list(l1_all_per_class[i])\n",
    "            low_shot_cnt.append(test_class_count[i])\n",
    "        else:\n",
    "            median_shot_mse.append(mse_per_class[i])\n",
    "            median_shot_l1.append(l1_per_class[i])\n",
    "            median_shot_gmean += list(l1_all_per_class[i])\n",
    "            median_shot_cnt.append(test_class_count[i])\n",
    "\n",
    "    shot_dict = defaultdict(dict)\n",
    "    shot_dict['many']['mse'] = np.sum(many_shot_mse) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['l1'] = np.sum(many_shot_l1) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['gmean'] = gmean(np.hstack(many_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['median']['mse'] = np.sum(median_shot_mse) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['l1'] = np.sum(median_shot_l1) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['gmean'] = gmean(np.hstack(median_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['low']['mse'] = np.sum(low_shot_mse) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['gmean'] = gmean(np.hstack(low_shot_gmean), axis=None).astype(float)\n",
    "\n",
    "\n",
    "    return shot_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overwrite previous folder: checkpoint\\bostonhousing_fcnet1_adam_bmse_0.001_64 ? [Y/n] :"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\\bostonhousing_fcnet1_adam_bmse_0.001_64 removed.\n",
      "===> Creating folder: checkpoint\\bostonhousing_fcnet1_adam_bmse_0.001_64\n",
      "Args: Namespace(augment=True, batch_size=64, best_loss=100000.0, bmse=True, cpu_only=True, data_dir='./boston.data', dataset='bostonhousing', epoch=10, evaluate=False, gpu=None, img_size=224, lds=False, lds_kernel='gaussian', lds_ks=5, lds_sigma=2, loss='l1', lr=0.001, model='fcnet1', momentum=0.9, optimizer='adam', print_freq=10, resume='', reweight='none', schedule=[60, 80], start_epoch=0, store_name='bostonhousing_fcnet1_adam_bmse_0.001_64', store_root='checkpoint', weight_decay=0.0001, workers=32)\n",
      "Store name: bostonhousing_fcnet1_adam_bmse_0.001_64\n",
      "=====> Preparing data...\n",
      "Training data size: 353\n",
      "Validation data size: 77\n",
      "Test data size: 76\n",
      "=====> Building model...\n",
      "Epoch: [0][0/6]\tTime   0.04 (  0.04)\tData 0.0020 (0.0020)\tLoss (L1) 699.757 (699.757)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 657.311 (657.311)\tLoss (L1) 23.838 (23.838)\n",
      " * Overall: MSE 645.608\tL1 23.610\tG-Mean 21.636\n",
      " * Many: MSE 418.980\tL1 19.140\tG-Mean 18.031\n",
      " * Median: MSE 529.648\tL1 22.248\tG-Mean 21.138\n",
      " * Low: MSE 1103.750\tL1 31.270\tG-Mean 27.827\n",
      "Best L1 Loss: 23.610\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #0: Train loss [964.8521]; Val loss: MSE [645.6080], L1 [23.6099], G-Mean [21.6362]\n",
      "Epoch: [1][0/6]\tTime   0.02 (  0.02)\tData 0.0040 (0.0040)\tLoss (L1) 706.318 (706.318)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 263.008 (263.008)\tLoss (L1) 12.882 (12.882)\n",
      " * Overall: MSE 257.165\tL1 12.835\tG-Mean 8.054\n",
      " * Many: MSE 247.310\tL1 14.258\tG-Mean 12.379\n",
      " * Median: MSE 247.388\tL1 10.323\tG-Mean 3.770\n",
      " * Low: MSE 315.068\tL1 14.729\tG-Mean 11.700\n",
      "Best L1 Loss: 12.835\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #1: Train loss [595.5794]; Val loss: MSE [257.1654], L1 [12.8353], G-Mean [8.0539]\n",
      "Epoch: [2][0/6]\tTime   0.01 (  0.01)\tData 0.0020 (0.0020)\tLoss (L1) 630.619 (630.619)\n",
      "Val: [0/2]\tTime  0.005 ( 0.005)\tLoss (MSE) 137.455 (137.455)\tLoss (L1) 9.302 (9.302)\n",
      " * Overall: MSE 132.082\tL1 8.792\tG-Mean 5.069\n",
      " * Many: MSE 53.820\tL1 4.502\tG-Mean 2.186\n",
      " * Median: MSE 62.372\tL1 7.352\tG-Mean 6.566\n",
      " * Low: MSE 336.446\tL1 17.309\tG-Mean 16.117\n",
      "Best L1 Loss: 8.792\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #2: Train loss [565.3956]; Val loss: MSE [132.0823], L1 [8.7923], G-Mean [5.0685]\n",
      "Epoch: [3][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 549.101 (549.101)\n",
      "Val: [0/2]\tTime  0.005 ( 0.005)\tLoss (MSE) 108.404 (108.404)\tLoss (L1) 8.548 (8.548)\n",
      " * Overall: MSE 100.622\tL1 8.126\tG-Mean 4.999\n",
      " * Many: MSE 129.535\tL1 9.529\tG-Mean 6.051\n",
      " * Median: MSE 50.513\tL1 5.555\tG-Mean 3.378\n",
      " * Low: MSE 115.004\tL1 9.241\tG-Mean 5.821\n",
      "Best L1 Loss: 8.126\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #3: Train loss [538.4851]; Val loss: MSE [100.6223], L1 [8.1259], G-Mean [4.9994]\n",
      "Epoch: [4][0/6]\tTime   0.01 (  0.01)\tData 0.0030 (0.0030)\tLoss (L1) 554.287 (554.287)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 102.832 (102.832)\tLoss (L1) 8.159 (8.159)\n",
      " * Overall: MSE 97.858\tL1 8.034\tG-Mean 5.706\n",
      " * Many: MSE 109.422\tL1 8.800\tG-Mean 6.860\n",
      " * Median: MSE 43.950\tL1 5.137\tG-Mean 3.183\n",
      " * Low: MSE 138.471\tL1 10.167\tG-Mean 7.686\n",
      "Best L1 Loss: 8.034\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #4: Train loss [528.8528]; Val loss: MSE [97.8582], L1 [8.0342], G-Mean [5.7062]\n",
      "Epoch: [5][0/6]\tTime   0.01 (  0.01)\tData 0.0030 (0.0030)\tLoss (L1) 520.390 (520.390)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 88.040 (88.040)\tLoss (L1) 7.613 (7.613)\n",
      " * Overall: MSE 86.275\tL1 7.582\tG-Mean 5.033\n",
      " * Many: MSE 92.669\tL1 8.197\tG-Mean 6.206\n",
      " * Median: MSE 39.595\tL1 5.154\tG-Mean 3.459\n",
      " * Low: MSE 128.257\tL1 9.483\tG-Mean 5.807\n",
      "Best L1 Loss: 7.582\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #5: Train loss [528.8297]; Val loss: MSE [86.2749], L1 [7.5817], G-Mean [5.0332]\n",
      "Epoch: [6][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 501.305 (501.305)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 84.436 (84.436)\tLoss (L1) 7.859 (7.859)\n",
      " * Overall: MSE 83.601\tL1 7.596\tG-Mean 5.410\n",
      " * Many: MSE 91.655\tL1 8.215\tG-Mean 6.126\n",
      " * Median: MSE 51.933\tL1 5.901\tG-Mean 4.179\n",
      " * Low: MSE 113.063\tL1 9.004\tG-Mean 6.661\n",
      "Best L1 Loss: 7.582\n",
      "Epoch #6: Train loss [522.3706]; Val loss: MSE [83.6011], L1 [7.5961], G-Mean [5.4104]\n",
      "Epoch: [7][0/6]\tTime   0.02 (  0.02)\tData 0.0020 (0.0020)\tLoss (L1) 521.228 (521.228)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 91.486 (91.486)\tLoss (L1) 7.675 (7.675)\n",
      " * Overall: MSE 88.427\tL1 7.436\tG-Mean 4.649\n",
      " * Many: MSE 78.743\tL1 6.982\tG-Mean 3.821\n",
      " * Median: MSE 42.700\tL1 5.406\tG-Mean 3.424\n",
      " * Low: MSE 156.890\tL1 10.694\tG-Mean 6.841\n",
      "Best L1 Loss: 7.436\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #7: Train loss [521.6849]; Val loss: MSE [88.4271], L1 [7.4359], G-Mean [4.6495]\n",
      "Epoch: [8][0/6]\tTime   0.01 (  0.01)\tData 0.0020 (0.0020)\tLoss (L1) 542.201 (542.201)\n",
      "Val: [0/2]\tTime  0.006 ( 0.006)\tLoss (MSE) 100.153 (100.153)\tLoss (L1) 8.140 (8.140)\n",
      " * Overall: MSE 96.563\tL1 7.806\tG-Mean 4.968\n",
      " * Many: MSE 98.307\tL1 7.897\tG-Mean 5.497\n",
      " * Median: MSE 56.166\tL1 6.017\tG-Mean 3.741\n",
      " * Low: MSE 141.035\tL1 10.019\tG-Mean 7.020\n",
      "Best L1 Loss: 7.436\n",
      "Epoch #8: Train loss [521.7997]; Val loss: MSE [96.5627], L1 [7.8064], G-Mean [4.9680]\n",
      "Epoch: [9][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 529.812 (529.812)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 103.418 (103.418)\tLoss (L1) 8.082 (8.082)\n",
      " * Overall: MSE 99.267\tL1 7.798\tG-Mean 4.861\n",
      " * Many: MSE 98.488\tL1 7.641\tG-Mean 3.726\n",
      " * Median: MSE 56.017\tL1 5.953\tG-Mean 3.033\n",
      " * Low: MSE 148.861\tL1 10.257\tG-Mean 7.404\n",
      "Best L1 Loss: 7.436\n",
      "Epoch #9: Train loss [518.7461]; Val loss: MSE [99.2669], L1 [7.7983], G-Mean [4.8607]\n",
      "========================================================================================================================\n",
      "Test best model on testset...\n",
      "Loaded best model, epoch 8, best val loss 7.4359\n",
      "Test: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 66.524 (66.524)\tLoss (L1) 6.891 (6.891)\n",
      " * Overall: MSE 66.431\tL1 6.895\tG-Mean 5.109\n",
      " * Many: MSE 53.070\tL1 6.228\tG-Mean 4.107\n",
      " * Median: MSE 49.268\tL1 6.107\tG-Mean 4.620\n",
      " * Low: MSE 121.026\tL1 9.674\tG-Mean 7.545\n",
      "Test loss: MSE [66.4314], L1 [6.8951], G-Mean [5.1091]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Option 5: To train the Balanced MSE model:\n",
    "args.reweight = 'none'\n",
    "args.lds = False\n",
    "args.bmse = True\n",
    "\n",
    "# train.py Part 2: Train/Evaluate the model.\n",
    "args.store_name = ''\n",
    "args.start_epoch, args.best_loss = 0, 1e5\n",
    "\n",
    "if len(args.store_name):\n",
    "    args.store_name = f'_{args.store_name}'\n",
    "if not args.lds and args.reweight != 'none':\n",
    "    args.store_name += f'_{args.reweight}'\n",
    "if args.lds:\n",
    "    args.store_name += f'_lds_{args.lds_kernel[:3]}_{args.lds_ks}'\n",
    "    if args.lds_kernel in ['gaussian', 'laplace']:\n",
    "        args.store_name += f'_{args.lds_sigma}'\n",
    "if args.bmse:\n",
    "  args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_bmse_{args.lr}_{args.batch_size}\"\n",
    "else:\n",
    "  args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_{args.loss}_{args.lr}_{args.batch_size}\"\n",
    "\n",
    "prepare_folders(args)\n",
    "\n",
    "print(f\"Args: {args}\")\n",
    "print(f\"Store name: {args.store_name}\")\n",
    "\n",
    "def main():\n",
    "    if args.gpu is not None:\n",
    "        print(f\"Use GPU: {args.gpu} for training\")\n",
    "\n",
    "    # Data\n",
    "    print('=====> Preparing data...')\n",
    "    #train_labels = np.loadtxt(args.data_dir+'.train',encoding='utf-16')[:, -1]\n",
    "    train_labels = np.loadtxt(args.data_dir+'.train')[:, -1]\n",
    "    train_dataset = BostonHousing(data_dir=args.data_dir+'.train', split='train',\n",
    "                          reweight=args.reweight, lds=args.lds, lds_kernel=args.lds_kernel, lds_ks=args.lds_ks, lds_sigma=args.lds_sigma)\n",
    "    val_dataset = BostonHousing(data_dir=args.data_dir+'.val', split='val')\n",
    "    test_dataset = BostonHousing(data_dir=args.data_dir+'.test', split='test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                              num_workers=0, pin_memory=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                            num_workers=0, pin_memory=True, drop_last=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                             num_workers=0, pin_memory=True, drop_last=False)\n",
    "    print(f\"Training data size: {len(train_dataset)}\")\n",
    "    print(f\"Validation data size: {len(val_dataset)}\")\n",
    "    print(f\"Test data size: {len(test_dataset)}\")\n",
    "\n",
    "    # Random Seed\n",
    "    np.random.seed(999)\n",
    "    # random.seed(999)\n",
    "    torch.manual_seed(999)\n",
    "\n",
    "    # Model\n",
    "    print('=====> Building model...')\n",
    "    model = fcnet1()\n",
    "    if not args.cpu_only:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # evaluate only\n",
    "    if args.evaluate:\n",
    "        assert args.resume, 'Specify a trained model using [args.resume]'\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        print(f\"===> Checkpoint '{args.resume}' loaded (epoch [{checkpoint['epoch']}]), testing...\")\n",
    "        validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "        return\n",
    "\n",
    "    # Loss and optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr) if args.optimizer == 'adam' else \\\n",
    "        torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(f\"===> Loading checkpoint '{args.resume}'\")\n",
    "            checkpoint = torch.load(args.resume) if args.gpu is None else \\\n",
    "                torch.load(args.resume, map_location=torch.device(f'cuda:{str(args.gpu)}'))\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            args.best_loss = checkpoint['best_loss']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(f\"===> Loaded checkpoint '{args.resume}' (Epoch [{checkpoint['epoch']}])\")\n",
    "        else:\n",
    "            print(f\"===> No checkpoint found at '{args.resume}'\")\n",
    "\n",
    "    if not args.cpu_only:\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epoch):\n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "        train_loss = train(train_loader, model, optimizer, epoch)\n",
    "        val_loss_mse, val_loss_l1, val_loss_gmean = validate(val_loader, model, train_labels=train_labels)\n",
    "\n",
    "        loss_metric = val_loss_mse if args.loss == 'mse' else val_loss_l1\n",
    "        is_best = loss_metric < args.best_loss\n",
    "        args.best_loss = min(loss_metric, args.best_loss)\n",
    "        print(f\"Best {'L1' if 'l1' in args.loss else 'MSE'} Loss: {args.best_loss:.3f}\")\n",
    "        save_checkpoint(args, {\n",
    "            'epoch': epoch + 1,\n",
    "            'model': args.model,\n",
    "            'best_loss': args.best_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print(f\"Epoch #{epoch}: Train loss [{train_loss:.4f}]; \"\n",
    "              f\"Val loss: MSE [{val_loss_mse:.4f}], L1 [{val_loss_l1:.4f}], G-Mean [{val_loss_gmean:.4f}]\")\n",
    "\n",
    "    # test with best checkpoint\n",
    "    print(\"=\" * 120)\n",
    "    print(\"Test best model on testset...\")\n",
    "    checkpoint = torch.load(f\"{args.store_root}/{args.store_name}/ckpt.best.pth.tar\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(f\"Loaded best model, epoch {checkpoint['epoch']}, best val loss {checkpoint['best_loss']:.4f}\")\n",
    "\n",
    "    test_loss_mse, test_loss_l1, test_loss_gmean = validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "    print(f\"Test loss: MSE [{test_loss_mse:.4f}], L1 [{test_loss_l1:.4f}], G-Mean [{test_loss_gmean:.4f}]\\nDone\")\n",
    "\n",
    "def train(train_loader, model, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.2f')\n",
    "    data_time = AverageMeter('Data', ':6.4f')\n",
    "    losses = AverageMeter(f'Loss ({args.loss.upper()})', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch)\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for idx, (inputs, targets, weights) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        if not args.cpu_only:\n",
    "            inputs, targets, weights = \\\n",
    "                inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True), weights.cuda(non_blocking=True)\n",
    "        outputs = model(inputs, targets, epoch)\n",
    "        if args.bmse:\n",
    "          loss = globals()[f\"bmse_loss\"](outputs, targets)\n",
    "        else:\n",
    "          loss = globals()[f\"weighted_{args.loss}_loss\"](outputs, targets, weights)\n",
    "        assert not (np.isnan(loss.item()) or loss.item() > 1e6), f\"Loss explosion: {loss.item()}\"\n",
    "\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if idx % args.print_freq == 0:\n",
    "            progress.display(idx)\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "def validate(val_loader, model, train_labels=None, prefix='Val'):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses_mse = AverageMeter('Loss (MSE)', ':.3f')\n",
    "    losses_l1 = AverageMeter('Loss (L1)', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses_mse, losses_l1],\n",
    "        prefix=f'{prefix}: '\n",
    "    )\n",
    "\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    criterion_gmean = nn.L1Loss(reduction='none')\n",
    "\n",
    "    model.eval()\n",
    "    losses_all = []\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (inputs, targets, _) in enumerate(val_loader):\n",
    "            if not args.cpu_only:\n",
    "                inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds.extend(outputs.data.cpu().numpy())\n",
    "            labels.extend(targets.data.cpu().numpy())\n",
    "\n",
    "            loss_mse = criterion_mse(outputs, targets)\n",
    "            loss_l1 = criterion_l1(outputs, targets)\n",
    "            loss_all = criterion_gmean(outputs, targets)\n",
    "            losses_all.extend(loss_all.cpu().numpy())\n",
    "\n",
    "            losses_mse.update(loss_mse.item(), inputs.size(0))\n",
    "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if idx % args.print_freq == 0:\n",
    "                progress.display(idx)\n",
    "\n",
    "\n",
    "        shot_dict = shot_metrics(np.hstack(preds), np.hstack(labels), train_labels)\n",
    "        loss_gmean = gmean(np.hstack(losses_all), axis=None).astype(float)\n",
    "        print(f\" * Overall: MSE {losses_mse.avg:.3f}\\tL1 {losses_l1.avg:.3f}\\tG-Mean {loss_gmean:.3f}\")\n",
    "        print(f\" * Many: MSE {shot_dict['many']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['many']['l1']:.3f}\\tG-Mean {shot_dict['many']['gmean']:.3f}\")\n",
    "        print(f\" * Median: MSE {shot_dict['median']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['median']['l1']:.3f}\\tG-Mean {shot_dict['median']['gmean']:.3f}\")\n",
    "        print(f\" * Low: MSE {shot_dict['low']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['low']['l1']:.3f}\\tG-Mean {shot_dict['low']['gmean']:.3f}\")\n",
    "\n",
    "    return losses_mse.avg, losses_l1.avg, loss_gmean\n",
    "\n",
    "def shot_metrics(preds, labels, train_labels, many_shot_thr=10, low_shot_thr=2):\n",
    "    train_labels = np.array(train_labels).astype(int)\n",
    "\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    elif isinstance(preds, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(f'Type ({type(preds)}) of predictions not supported')\n",
    "\n",
    "    labels = np.array(labels).astype(int)\n",
    "\n",
    "    train_class_count, test_class_count = [], []\n",
    "    mse_per_class, l1_per_class, l1_all_per_class = [], [], []\n",
    "    for l in np.unique(labels):\n",
    "        train_class_count.append(len(train_labels[train_labels == l]))\n",
    "        test_class_count.append(len(labels[labels == l]))\n",
    "        mse_per_class.append(np.sum((preds[labels == l] - labels[labels == l]) ** 2))\n",
    "        l1_per_class.append(np.sum(np.abs(preds[labels == l] - labels[labels == l])))\n",
    "        l1_all_per_class.append(np.abs(preds[labels == l] - labels[labels == l]))\n",
    "\n",
    "    many_shot_mse, median_shot_mse, low_shot_mse = [], [], []\n",
    "    many_shot_l1, median_shot_l1, low_shot_l1 = [], [], []\n",
    "    many_shot_gmean, median_shot_gmean, low_shot_gmean = [], [], []\n",
    "    many_shot_cnt, median_shot_cnt, low_shot_cnt = [], [], []\n",
    "\n",
    "    for i in range(len(train_class_count)):\n",
    "        if train_class_count[i] > many_shot_thr:\n",
    "            many_shot_mse.append(mse_per_class[i])\n",
    "            many_shot_l1.append(l1_per_class[i])\n",
    "            many_shot_gmean += list(l1_all_per_class[i])\n",
    "            many_shot_cnt.append(test_class_count[i])\n",
    "        elif train_class_count[i] < low_shot_thr:\n",
    "            low_shot_mse.append(mse_per_class[i])\n",
    "            low_shot_l1.append(l1_per_class[i])\n",
    "            low_shot_gmean += list(l1_all_per_class[i])\n",
    "            low_shot_cnt.append(test_class_count[i])\n",
    "        else:\n",
    "            median_shot_mse.append(mse_per_class[i])\n",
    "            median_shot_l1.append(l1_per_class[i])\n",
    "            median_shot_gmean += list(l1_all_per_class[i])\n",
    "            median_shot_cnt.append(test_class_count[i])\n",
    "\n",
    "    shot_dict = defaultdict(dict)\n",
    "    shot_dict['many']['mse'] = np.sum(many_shot_mse) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['l1'] = np.sum(many_shot_l1) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['gmean'] = gmean(np.hstack(many_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['median']['mse'] = np.sum(median_shot_mse) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['l1'] = np.sum(median_shot_l1) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['gmean'] = gmean(np.hstack(median_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['low']['mse'] = np.sum(low_shot_mse) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['gmean'] = gmean(np.hstack(low_shot_gmean), axis=None).astype(float)\n",
    "\n",
    "\n",
    "    return shot_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "balanced_mse.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
