{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJc7GRtFbrRK"
   },
   "source": [
    "# A Simple Regression Example of Balanced MSE\n",
    "\n",
    "This notebook is developed on [Balanced MSE for Imbalanced Visual Regression](https://github.com/jiawei-ren/BalancedMSE/tree/main/tutorial) and [Deep Imbalanced Regression (DIR) Tutorial](https://github.com/YyzHarry/imbalanced-regression/tree/main/tutorial). We thank the authors for their amazing tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7mBaSzabN6I"
   },
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xWuPbiEYmzvS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if (os.path.exists('./smogn_boston.data.train') and os.path.exists('./smogn_boston.data.val') and os.path.exists('./smogn_boston.data.test')):\n",
    "    print('Data already downloaded.')\n",
    "else:\n",
    "    print('No Data.!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnWsY4FzYC0Y"
   },
   "source": [
    "# Define the deep learning (neural network) model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jKm3tqsMhDOA"
   },
   "outputs": [],
   "source": [
    "# fcnet.py: Define the deep learning (neural network) model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FCNet(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, dropout=None):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.fc0 = nn.Linear(13, layers[0])               # features!!!!\n",
    "        self.fc1 = nn.Linear(layers[0], layers[1])\n",
    "        self.fc2 = nn.Linear(layers[1], layers[2])\n",
    "        self.fc_final = nn.Linear(layers[-1], 1)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.use_dropout = True if dropout else False\n",
    "        if self.use_dropout:\n",
    "            print(f'Using dropout: {dropout}')\n",
    "            self.dropout0 = nn.Dropout(p=dropout)\n",
    "            self.dropout1 = nn.Dropout(p=dropout)\n",
    "            self.dropout2 = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout0 = nn.Identity()\n",
    "            self.dropout1 = nn.Identity()\n",
    "            self.dropout2 = nn.Identity()\n",
    "\n",
    "    def forward(self, x, targets=None, epoch=None):\n",
    "        x = self.dropout0(F.relu(self.fc0(x)))\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout2(F.relu(self.fc2(x)))\n",
    "        x = self.fc_final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def fcnet1(**kwargs):\n",
    "    return FCNet([512, 512, 512], **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWohJXZPYbY3"
   },
   "source": [
    "# Define the loss functions. Here we only need the L1 loss: |f(x) - y|."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7Kzc8PTjhDIH"
   },
   "outputs": [],
   "source": [
    "# loss.py: Define the loss functions (here we only need the L1 loss)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def weighted_mse_loss(inputs, targets, weights=None):\n",
    "    loss = (inputs - targets) ** 2\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_l1_loss(inputs, targets, weights=None):\n",
    "    loss = F.l1_loss(inputs, targets, reduction='none')\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_focal_mse_loss(inputs, targets, weights=None, activate='sigmoid', beta=.2, gamma=1):\n",
    "    loss = (inputs - targets) ** 2\n",
    "    loss *= (torch.tanh(beta * torch.abs(inputs - targets))) ** gamma if activate == 'tanh' else \\\n",
    "        (2 * torch.sigmoid(beta * torch.abs(inputs - targets)) - 1) ** gamma\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_focal_l1_loss(inputs, targets, weights=None, activate='sigmoid', beta=.2, gamma=1):\n",
    "    loss = F.l1_loss(inputs, targets, reduction='none')\n",
    "    loss *= (torch.tanh(beta * torch.abs(inputs - targets))) ** gamma if activate == 'tanh' else \\\n",
    "        (2 * torch.sigmoid(beta * torch.abs(inputs - targets)) - 1) ** gamma\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_huber_loss(inputs, targets, weights=None, beta=1.):\n",
    "    l1_loss = torch.abs(inputs - targets)\n",
    "    cond = l1_loss < beta\n",
    "    loss = torch.where(cond, 0.5 * l1_loss ** 2 / beta, l1_loss - 0.5 * beta)\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aunbes6NY_Pz"
   },
   "source": [
    "# Define some utility functions (not the focus of this course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "f3XXH8Yik4M9"
   },
   "outputs": [],
   "source": [
    "# utils.py: Define some utility functions (not the focus of this course).\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal.windows import triang\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_batch_fmtstr(num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "def query_yes_no(question):\n",
    "    \"\"\" Ask a yes/no question via input() and return their answer. \"\"\"\n",
    "    valid = {\"yes\": True, \"y\": True, \"ye\": True, \"no\": False, \"n\": False}\n",
    "    prompt = \" [Y/n] \"\n",
    "\n",
    "    while True:\n",
    "        print(question + prompt, end=':')\n",
    "        choice = input().lower()\n",
    "        if choice == '':\n",
    "            return valid['y']\n",
    "        elif choice in valid:\n",
    "            return valid[choice]\n",
    "        else:\n",
    "            print(\"Please respond with 'yes' or 'no' (or 'y' or 'n').\\n\")\n",
    "\n",
    "def prepare_folders(args):\n",
    "    folders_util = [args.store_root, os.path.join(args.store_root, args.store_name)]\n",
    "    if os.path.exists(folders_util[-1]) and not args.resume and not args.evaluate:\n",
    "        if query_yes_no('overwrite previous folder: {} ?'.format(folders_util[-1])):\n",
    "            shutil.rmtree(folders_util[-1])\n",
    "            print(folders_util[-1] + ' removed.')\n",
    "        else:\n",
    "            raise RuntimeError('Output folder {} already exists'.format(folders_util[-1]))\n",
    "    for folder in folders_util:\n",
    "        if not os.path.exists(folder):\n",
    "            print(f\"===> Creating folder: {folder}\")\n",
    "            os.mkdir(folder)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    lr = args.lr\n",
    "    for milestone in args.schedule:\n",
    "        lr *= 0.1 if epoch >= milestone else 1.\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def save_checkpoint(args, state, is_best, prefix=''):\n",
    "    filename = f\"{args.store_root}/{args.store_name}/{prefix}ckpt.pth.tar\"\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        print(\"===> Saving current best checkpoint...\")\n",
    "        shutil.copyfile(filename, filename.replace('pth.tar', 'best.pth.tar'))\n",
    "\n",
    "def calibrate_mean_var(matrix, m1, v1, m2, v2, clip_min=0.1, clip_max=10):\n",
    "    if torch.sum(v1) < 1e-10:\n",
    "        return matrix\n",
    "    if (v1 == 0.).any():\n",
    "        valid = (v1 != 0.)\n",
    "        factor = torch.clamp(v2[valid] / v1[valid], clip_min, clip_max)\n",
    "        matrix[:, valid] = (matrix[:, valid] - m1[valid]) * torch.sqrt(factor) + m2[valid]\n",
    "        return matrix\n",
    "\n",
    "    factor = torch.clamp(v2 / v1, clip_min, clip_max)\n",
    "    return (matrix - m1) * torch.sqrt(factor) + m2\n",
    "\n",
    "def get_lds_kernel_window(kernel, ks, sigma):\n",
    "    assert kernel in ['gaussian', 'triang', 'laplace']\n",
    "    half_ks = (ks - 1) // 2\n",
    "    if kernel == 'gaussian':\n",
    "        base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks\n",
    "        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / max(gaussian_filter1d(base_kernel, sigma=sigma))\n",
    "    elif kernel == 'triang':\n",
    "        kernel_window = triang(ks)\n",
    "    else:\n",
    "        laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)\n",
    "        kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / max(map(laplace, np.arange(-half_ks, half_ks + 1)))\n",
    "\n",
    "    return kernel_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpbjtnotZUkk"
   },
   "source": [
    "# Define the data iterator (data loader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gqK8H1UghC_v"
   },
   "outputs": [],
   "source": [
    "# datasets.py: Define the data iterator (data loader).\n",
    "from scipy.ndimage import convolve1d\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "# from utils import get_lds_kernel_window\n",
    "\n",
    "class BostonHousing(data.Dataset):\n",
    "    def __init__(self, data_dir, split='train', reweight='none',\n",
    "                 lds=False, lds_kernel='gaussian', lds_ks=5, lds_sigma=2):\n",
    "        self.split = split\n",
    "        #self.data = np.loadtxt(data_dir, dtype='float32',encoding='utf-16')\n",
    "        self.data = np.loadtxt(data_dir, dtype='float32')\n",
    "        self.weights = self._prepare_weights(reweight=reweight, lds=lds, lds_kernel=lds_kernel, lds_ks=lds_ks, lds_sigma=lds_sigma)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = index % self.data.shape[0]\n",
    "        feature = self.data[index, :-1]\n",
    "        label = np.expand_dims(np.asarray(self.data[index, -1]), axis=0)\n",
    "        weight = np.asarray([self.weights[index]]).astype('float32') if self.weights is not None else np.asarray([np.float32(1.)])\n",
    "        return feature, label, weight\n",
    "\n",
    "    def _prepare_weights(self, reweight, max_target=51, lds=False, lds_kernel='gaussian', lds_ks=5, lds_sigma=2):\n",
    "        assert reweight in {'none', 'inverse', 'sqrt_inv'}\n",
    "        assert reweight != 'none' if lds else True, \\\n",
    "            \"Set reweight to \\'sqrt_inv\\' (default) or \\'inverse\\' when using LDS\"\n",
    "\n",
    "        value_dict = {x: 0 for x in range(max_target)}\n",
    "        labels = self.data[:, -1].tolist()\n",
    "        # mbr\n",
    "        for label in labels:\n",
    "            value_dict[min(max_target - 1, int(label))] += 1\n",
    "        if reweight == 'sqrt_inv':\n",
    "            value_dict = {k: np.sqrt(v) for k, v in value_dict.items()}\n",
    "        elif reweight == 'inverse':\n",
    "            value_dict = {k: np.clip(v, 0, 1000) for k, v in value_dict.items()}     # clip weights for inverse re-weight!!!!!!\n",
    "        num_per_label = [value_dict[min(max_target - 1, int(label))] for label in labels]\n",
    "        if not len(num_per_label) or reweight == 'none':\n",
    "            return None\n",
    "        print(f\"Using re-weighting: [{reweight.upper()}]\")\n",
    "\n",
    "        if lds:\n",
    "            lds_kernel_window = get_lds_kernel_window(lds_kernel, lds_ks, lds_sigma)\n",
    "            print(f'Using LDS: [{lds_kernel.upper()}] ({lds_ks}/{lds_sigma})')\n",
    "            smoothed_value = convolve1d(\n",
    "                np.asarray([v for _, v in value_dict.items()]), weights=lds_kernel_window, mode='constant')\n",
    "            num_per_label = [smoothed_value[min(max_target - 1, int(label))] for label in labels]\n",
    "\n",
    "        weights = [np.float32(1 / x) for x in num_per_label]\n",
    "        scaling = len(weights) / np.sum(weights)\n",
    "        weights = [scaling * x for x in weights]\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zcuwqp5paPk3"
   },
   "source": [
    "# Set up some default configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "T7mtQacPhBoW"
   },
   "outputs": [],
   "source": [
    "# train.py, Part 1: Set up some default configurations.\n",
    "import time\n",
    "import argparse\n",
    "#import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gmean\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from resnet import resnet50\n",
    "# from fcnet import fcnet1\n",
    "# from loss import *\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\"   # Window 系统原因，在不启用Cuda情况下，CPU线程问题会中断pipeline\n",
    "\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# CPU only\n",
    "parser.add_argument('--cpu_only', action='store_true', default=False, help='whether to use CPU only')\n",
    "# imbalanced related\n",
    "# LDS\n",
    "parser.add_argument('--lds', action='store_true', default=False, help='whether to enable LDS')\n",
    "parser.add_argument('--lds_kernel', type=str, default='gaussian',\n",
    "                    choices=['gaussian', 'triang', 'laplace'], help='LDS kernel type')\n",
    "parser.add_argument('--lds_ks', type=int, default=9, help='LDS kernel size: should be odd number')\n",
    "parser.add_argument('--lds_sigma', type=float, default=1, help='LDS gaussian/laplace kernel sigma')\n",
    "\n",
    "# re-weighting: SQRT_INV / INV\n",
    "parser.add_argument('--reweight', type=str, default='none', choices=['none', 'sqrt_inv', 'inverse'], help='cost-sensitive reweighting scheme')\n",
    "\n",
    "# training/optimization related\n",
    "parser.add_argument('--dataset', type=str, default='bostonhousing', choices=['imdb_wiki', 'agedb'], help='dataset name')\n",
    "parser.add_argument('--data_dir', type=str, default='./smogn_boston.data', help='data directory')\n",
    "parser.add_argument('--model', type=str, default='fcnet1', help='model name')\n",
    "parser.add_argument('--store_root', type=str, default='checkpoint', help='root path for storing checkpoints, logs')\n",
    "parser.add_argument('--store_name', type=str, default='', help='experiment store name')\n",
    "parser.add_argument('--gpu', type=int, default=None)\n",
    "parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'], help='optimizer type')\n",
    "parser.add_argument('--loss', type=str, default='l1', choices=['mse', 'l1', 'focal_l1', 'focal_mse', 'huber'], help='training loss type')\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help='initial learning rate')\n",
    "parser.add_argument('--epoch', type=int, default=10, help='number of epochs to train')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='optimizer momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4, help='optimizer weight decay')\n",
    "parser.add_argument('--schedule', type=int, nargs='*', default=[60, 80], help='lr schedule (when to drop lr by 10x)')\n",
    "#parser.add_argument('--batch_size', type=int, default=256, help='batch size')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--print_freq', type=int, default=10, help='logging frequency')\n",
    "parser.add_argument('--img_size', type=int, default=224, help='image size used in training')\n",
    "parser.add_argument('--workers', type=int, default=32, help='number of workers used in data loading')\n",
    "# checkpoints\n",
    "parser.add_argument('--resume', type=str, default='', help='checkpoint file path to resume training')\n",
    "parser.add_argument('--evaluate', action='store_true', help='evaluate only flag')\n",
    "\n",
    "# Balanced MSE\n",
    "parser.add_argument('--bmse', action='store_true', help='use Balanced MSE')\n",
    "parser.set_defaults(augment=True)\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nWvzFoYJnwuF"
   },
   "outputs": [],
   "source": [
    "args.cpu_only = True # Use CPU to train/test models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtG61vP8ROkP"
   },
   "source": [
    "# Train DNN SMOGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tSrzhog1gxyY",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overwrite previous folder: checkpoint\\bostonhousing_fcnet1_adam_l1_0.001_64 ? [Y/n] :"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\\bostonhousing_fcnet1_adam_l1_0.001_64 removed.\n",
      "===> Creating folder: checkpoint\\bostonhousing_fcnet1_adam_l1_0.001_64\n",
      "Args: Namespace(augment=True, batch_size=64, best_loss=100000.0, bmse=False, cpu_only=True, data_dir='./smogn_boston.data', dataset='bostonhousing', epoch=10, evaluate=False, gpu=None, img_size=224, lds=False, lds_kernel='gaussian', lds_ks=9, lds_sigma=1, loss='l1', lr=0.001, model='fcnet1', momentum=0.9, optimizer='adam', print_freq=10, resume='', reweight='none', schedule=[60, 80], start_epoch=0, store_name='bostonhousing_fcnet1_adam_l1_0.001_64', store_root='checkpoint', weight_decay=0.0001, workers=32)\n",
      "Store name: bostonhousing_fcnet1_adam_l1_0.001_64\n",
      "=====> Preparing data...\n",
      "Training data size: 418\n",
      "Validation data size: 77\n",
      "Test data size: 76\n",
      "=====> Building model...\n",
      "Epoch: [0][0/7]\tTime   0.02 (  0.02)\tData 0.0020 (0.0020)\tLoss (L1) 23.051 (23.051)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 235.654 (235.654)\tLoss (L1) 12.367 (12.367)\n",
      " * Overall: MSE 230.715\tL1 11.985\tG-Mean 7.433\n",
      " * Many: MSE 91.631\tL1 7.166\tG-Mean 5.117\n",
      " * Median: MSE 218.411\tL1 11.864\tG-Mean 6.968\n",
      " * Low: MSE 657.141\tL1 25.362\tG-Mean 25.075\n",
      "Best L1 Loss: 11.985\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #0: Train loss [26.3127]; Val loss: MSE [230.7148], L1 [11.9853], G-Mean [7.4328]\n",
      "Epoch: [1][0/7]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 11.749 (11.749)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 142.517 (142.517)\tLoss (L1) 9.265 (9.265)\n",
      " * Overall: MSE 139.672\tL1 8.929\tG-Mean 5.250\n",
      " * Many: MSE 55.728\tL1 4.573\tG-Mean 2.454\n",
      " * Median: MSE 131.243\tL1 9.111\tG-Mean 5.852\n",
      " * Low: MSE 387.866\tL1 19.255\tG-Mean 18.772\n",
      "Best L1 Loss: 8.929\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #1: Train loss [12.9283]; Val loss: MSE [139.6722], L1 [8.9294], G-Mean [5.2504]\n",
      "Epoch: [2][0/7]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 10.726 (10.726)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 68.974 (68.974)\tLoss (L1) 6.769 (6.769)\n",
      " * Overall: MSE 68.671\tL1 6.659\tG-Mean 3.659\n",
      " * Many: MSE 81.688\tL1 7.821\tG-Mean 4.928\n",
      " * Median: MSE 59.348\tL1 5.958\tG-Mean 3.842\n",
      " * Low: MSE 79.037\tL1 7.354\tG-Mean 2.940\n",
      "Best L1 Loss: 6.659\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #2: Train loss [11.0977]; Val loss: MSE [68.6715], L1 [6.6591], G-Mean [3.6594]\n",
      "Epoch: [3][0/7]\tTime   0.01 (  0.01)\tData 0.0020 (0.0020)\tLoss (L1) 9.202 (9.202)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 81.070 (81.070)\tLoss (L1) 6.610 (6.610)\n",
      " * Overall: MSE 85.197\tL1 6.967\tG-Mean 3.948\n",
      " * Many: MSE 59.163\tL1 5.765\tG-Mean 3.489\n",
      " * Median: MSE 78.774\tL1 6.664\tG-Mean 4.298\n",
      " * Low: MSE 161.537\tL1 11.239\tG-Mean 9.094\n",
      "Best L1 Loss: 6.659\n",
      "Epoch #3: Train loss [10.0950]; Val loss: MSE [85.1969], L1 [6.9668], G-Mean [3.9478]\n",
      "Epoch: [4][0/7]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 12.652 (12.652)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 75.778 (75.778)\tLoss (L1) 6.400 (6.400)\n",
      " * Overall: MSE 81.765\tL1 6.858\tG-Mean 4.102\n",
      " * Many: MSE 67.412\tL1 6.301\tG-Mean 3.995\n",
      " * Median: MSE 75.011\tL1 6.563\tG-Mean 4.442\n",
      " * Low: MSE 129.519\tL1 9.269\tG-Mean 2.996\n",
      "Best L1 Loss: 6.659\n",
      "Epoch #4: Train loss [9.2486]; Val loss: MSE [81.7646], L1 [6.8584], G-Mean [4.1023]\n",
      "Epoch: [5][0/7]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 7.890 (7.890)\n",
      "Val: [0/2]\tTime  0.006 ( 0.006)\tLoss (MSE) 68.234 (68.234)\tLoss (L1) 6.417 (6.417)\n",
      " * Overall: MSE 73.303\tL1 6.688\tG-Mean 4.606\n",
      " * Many: MSE 69.001\tL1 6.848\tG-Mean 4.899\n",
      " * Median: MSE 66.594\tL1 6.178\tG-Mean 3.919\n",
      " * Low: MSE 105.086\tL1 8.450\tG-Mean 6.132\n",
      "Best L1 Loss: 6.659\n",
      "Epoch #5: Train loss [9.1614]; Val loss: MSE [73.3027], L1 [6.6876], G-Mean [4.6059]\n",
      "Epoch: [6][0/7]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 10.945 (10.945)\n",
      "Val: [0/2]\tTime  0.007 ( 0.007)\tLoss (MSE) 93.309 (93.309)\tLoss (L1) 6.917 (6.917)\n",
      " * Overall: MSE 95.691\tL1 7.155\tG-Mean 4.422\n",
      " * Many: MSE 53.495\tL1 5.132\tG-Mean 2.697\n",
      " * Median: MSE 88.499\tL1 6.827\tG-Mean 4.003\n",
      " * Low: MSE 221.293\tL1 13.333\tG-Mean 11.389\n",
      "Best L1 Loss: 6.659\n",
      "Epoch #6: Train loss [9.3718]; Val loss: MSE [95.6909], L1 [7.1545], G-Mean [4.4218]\n",
      "Epoch: [7][0/7]\tTime   0.03 (  0.03)\tData 0.0070 (0.0070)\tLoss (L1) 8.911 (8.911)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 80.879 (80.879)\tLoss (L1) 6.462 (6.462)\n",
      " * Overall: MSE 84.665\tL1 6.794\tG-Mean 4.264\n",
      " * Many: MSE 60.383\tL1 5.664\tG-Mean 3.472\n",
      " * Median: MSE 76.397\tL1 6.453\tG-Mean 4.285\n",
      " * Low: MSE 165.224\tL1 10.959\tG-Mean 8.218\n",
      "Best L1 Loss: 6.659\n",
      "Epoch #7: Train loss [9.3073]; Val loss: MSE [84.6652], L1 [6.7935], G-Mean [4.2642]\n",
      "Epoch: [8][0/7]\tTime   0.02 (  0.02)\tData 0.0020 (0.0020)\tLoss (L1) 7.290 (7.290)\n",
      "Val: [0/2]\tTime  0.007 ( 0.007)\tLoss (MSE) 73.674 (73.674)\tLoss (L1) 6.199 (6.199)\n",
      " * Overall: MSE 78.771\tL1 6.525\tG-Mean 4.098\n",
      " * Many: MSE 59.694\tL1 5.906\tG-Mean 3.853\n",
      " * Median: MSE 71.739\tL1 6.151\tG-Mean 4.213\n",
      " * Low: MSE 146.334\tL1 9.663\tG-Mean 4.294\n",
      "Best L1 Loss: 6.525\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #8: Train loss [9.0846]; Val loss: MSE [78.7711], L1 [6.5252], G-Mean [4.0976]\n",
      "Epoch: [9][0/7]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 7.602 (7.602)\n",
      "Val: [0/2]\tTime  0.005 ( 0.005)\tLoss (MSE) 68.088 (68.088)\tLoss (L1) 6.243 (6.243)\n",
      " * Overall: MSE 74.892\tL1 6.511\tG-Mean 4.059\n",
      " * Many: MSE 66.509\tL1 6.727\tG-Mean 5.260\n",
      " * Median: MSE 69.888\tL1 5.975\tG-Mean 3.127\n",
      " * Low: MSE 113.474\tL1 8.468\tG-Mean 4.945\n",
      "Best L1 Loss: 6.511\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #9: Train loss [8.8504]; Val loss: MSE [74.8923], L1 [6.5109], G-Mean [4.0593]\n",
      "========================================================================================================================\n",
      "Test best model on testset...\n",
      "Loaded best model, epoch 10, best val loss 6.5109\n",
      "Test: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 59.034 (59.034)\tLoss (L1) 6.133 (6.133)\n",
      " * Overall: MSE 58.272\tL1 5.988\tG-Mean 4.008\n",
      " * Many: MSE 55.298\tL1 6.169\tG-Mean 4.566\n",
      " * Median: MSE 57.572\tL1 5.735\tG-Mean 3.509\n",
      " * Low: MSE 89.844\tL1 8.596\tG-Mean 7.169\n",
      "Test loss: MSE [58.2720], L1 [5.9881], G-Mean [4.0077]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Option 1: To train the basic model, use the default setting, don't need to do anything\n",
    "args.reweight = 'none'\n",
    "args.lds = False\n",
    "\n",
    "# train.py Part 2: Train/Evaluate the model.\n",
    "args.store_name = ''\n",
    "args.start_epoch, args.best_loss = 0, 1e5\n",
    "\n",
    "if len(args.store_name):\n",
    "    args.store_name = f'_{args.store_name}'\n",
    "if not args.lds and args.reweight != 'none':\n",
    "    args.store_name += f'_{args.reweight}'\n",
    "if args.lds:\n",
    "    args.store_name += f'_lds_{args.lds_kernel[:3]}_{args.lds_ks}'\n",
    "    if args.lds_kernel in ['gaussian', 'laplace']:\n",
    "        args.store_name += f'_{args.lds_sigma}'\n",
    "if args.bmse:\n",
    "  args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_bmse_{args.lr}_{args.batch_size}\"\n",
    "else:\n",
    "  args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_{args.loss}_{args.lr}_{args.batch_size}\"\n",
    "\n",
    "prepare_folders(args)\n",
    "\n",
    "print(f\"Args: {args}\")\n",
    "print(f\"Store name: {args.store_name}\")\n",
    "\n",
    "def main():\n",
    "    if args.gpu is not None:\n",
    "        print(f\"Use GPU: {args.gpu} for training\")\n",
    "\n",
    "    # Data\n",
    "    print('=====> Preparing data...')\n",
    "    #train_labels = np.loadtxt(args.data_dir+'.train',encoding='utf-16')[:, -1]\n",
    "    train_labels = np.loadtxt(args.data_dir+'.train')[:, -1]\n",
    "    train_dataset = BostonHousing(data_dir=args.data_dir+'.train', split='train',\n",
    "                          reweight=args.reweight, lds=args.lds, lds_kernel=args.lds_kernel, lds_ks=args.lds_ks, lds_sigma=args.lds_sigma)\n",
    "    val_dataset = BostonHousing(data_dir=args.data_dir+'.val', split='val')\n",
    "    test_dataset = BostonHousing(data_dir=args.data_dir+'.test', split='test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                              num_workers=0, pin_memory=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                            num_workers=0, pin_memory=True, drop_last=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                             num_workers=0, pin_memory=True, drop_last=False)\n",
    "    print(f\"Training data size: {len(train_dataset)}\")\n",
    "    print(f\"Validation data size: {len(val_dataset)}\")\n",
    "    print(f\"Test data size: {len(test_dataset)}\")\n",
    "\n",
    "    # Random Seed\n",
    "    np.random.seed(999)\n",
    "    # random.seed(999)\n",
    "    torch.manual_seed(999)\n",
    "\n",
    "    # Model\n",
    "    print('=====> Building model...')\n",
    "    model = fcnet1()\n",
    "    if not args.cpu_only:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # evaluate only\n",
    "    if args.evaluate:\n",
    "        assert args.resume, 'Specify a trained model using [args.resume]'\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        print(f\"===> Checkpoint '{args.resume}' loaded (epoch [{checkpoint['epoch']}]), testing...\")\n",
    "        validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "        return\n",
    "\n",
    "    # Loss and optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr) if args.optimizer == 'adam' else \\\n",
    "        torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(f\"===> Loading checkpoint '{args.resume}'\")\n",
    "            checkpoint = torch.load(args.resume) if args.gpu is None else \\\n",
    "                torch.load(args.resume, map_location=torch.device(f'cuda:{str(args.gpu)}'))\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            args.best_loss = checkpoint['best_loss']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(f\"===> Loaded checkpoint '{args.resume}' (Epoch [{checkpoint['epoch']}])\")\n",
    "        else:\n",
    "            print(f\"===> No checkpoint found at '{args.resume}'\")\n",
    "\n",
    "    if not args.cpu_only:\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epoch):\n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "        train_loss = train(train_loader, model, optimizer, epoch)\n",
    "        val_loss_mse, val_loss_l1, val_loss_gmean = validate(val_loader, model, train_labels=train_labels)\n",
    "\n",
    "        loss_metric = val_loss_mse if args.loss == 'mse' else val_loss_l1\n",
    "        is_best = loss_metric < args.best_loss\n",
    "        args.best_loss = min(loss_metric, args.best_loss)\n",
    "        print(f\"Best {'L1' if 'l1' in args.loss else 'MSE'} Loss: {args.best_loss:.3f}\")\n",
    "        save_checkpoint(args, {\n",
    "            'epoch': epoch + 1,\n",
    "            'model': args.model,\n",
    "            'best_loss': args.best_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print(f\"Epoch #{epoch}: Train loss [{train_loss:.4f}]; \"\n",
    "              f\"Val loss: MSE [{val_loss_mse:.4f}], L1 [{val_loss_l1:.4f}], G-Mean [{val_loss_gmean:.4f}]\")\n",
    "\n",
    "    # test with best checkpoint\n",
    "    print(\"=\" * 120)\n",
    "    print(\"Test best model on testset...\")\n",
    "    checkpoint = torch.load(f\"{args.store_root}/{args.store_name}/ckpt.best.pth.tar\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(f\"Loaded best model, epoch {checkpoint['epoch']}, best val loss {checkpoint['best_loss']:.4f}\")\n",
    "\n",
    "    test_loss_mse, test_loss_l1, test_loss_gmean = validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "    print(f\"Test loss: MSE [{test_loss_mse:.4f}], L1 [{test_loss_l1:.4f}], G-Mean [{test_loss_gmean:.4f}]\\nDone\")\n",
    "\n",
    "def train(train_loader, model, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.2f')\n",
    "    data_time = AverageMeter('Data', ':6.4f')\n",
    "    losses = AverageMeter(f'Loss ({args.loss.upper()})', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch)\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for idx, (inputs, targets, weights) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        if not args.cpu_only:\n",
    "            inputs, targets, weights = \\\n",
    "                inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True), weights.cuda(non_blocking=True)\n",
    "        outputs = model(inputs, targets, epoch)\n",
    "        if args.bmse:\n",
    "          loss = globals()[f\"bmse_loss\"](outputs, targets)\n",
    "        else:\n",
    "          loss = globals()[f\"weighted_{args.loss}_loss\"](outputs, targets, weights)\n",
    "        assert not (np.isnan(loss.item()) or loss.item() > 1e6), f\"Loss explosion: {loss.item()}\"\n",
    "\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if idx % args.print_freq == 0:\n",
    "            progress.display(idx)\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "def validate(val_loader, model, train_labels=None, prefix='Val'):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses_mse = AverageMeter('Loss (MSE)', ':.3f')\n",
    "    losses_l1 = AverageMeter('Loss (L1)', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses_mse, losses_l1],\n",
    "        prefix=f'{prefix}: '\n",
    "    )\n",
    "\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    criterion_gmean = nn.L1Loss(reduction='none')\n",
    "\n",
    "    model.eval()\n",
    "    losses_all = []\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (inputs, targets, _) in enumerate(val_loader):\n",
    "            if not args.cpu_only:\n",
    "                inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds.extend(outputs.data.cpu().numpy())\n",
    "            labels.extend(targets.data.cpu().numpy())\n",
    "\n",
    "            loss_mse = criterion_mse(outputs, targets)\n",
    "            loss_l1 = criterion_l1(outputs, targets)\n",
    "            loss_all = criterion_gmean(outputs, targets)\n",
    "            losses_all.extend(loss_all.cpu().numpy())\n",
    "\n",
    "            losses_mse.update(loss_mse.item(), inputs.size(0))\n",
    "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if idx % args.print_freq == 0:\n",
    "                progress.display(idx)\n",
    "\n",
    "\n",
    "        shot_dict = shot_metrics(np.hstack(preds), np.hstack(labels), train_labels)\n",
    "        loss_gmean = gmean(np.hstack(losses_all), axis=None).astype(float)\n",
    "        print(f\" * Overall: MSE {losses_mse.avg:.3f}\\tL1 {losses_l1.avg:.3f}\\tG-Mean {loss_gmean:.3f}\")\n",
    "        print(f\" * Many: MSE {shot_dict['many']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['many']['l1']:.3f}\\tG-Mean {shot_dict['many']['gmean']:.3f}\")\n",
    "        print(f\" * Median: MSE {shot_dict['median']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['median']['l1']:.3f}\\tG-Mean {shot_dict['median']['gmean']:.3f}\")\n",
    "        print(f\" * Low: MSE {shot_dict['low']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['low']['l1']:.3f}\\tG-Mean {shot_dict['low']['gmean']:.3f}\")\n",
    "\n",
    "    return losses_mse.avg, losses_l1.avg, loss_gmean\n",
    "\n",
    "def shot_metrics(preds, labels, train_labels, many_shot_thr=10, low_shot_thr=2):  #  from data distribute  !!!!!!!!!!!!!!!!\n",
    "    train_labels = np.array(train_labels).astype(int)\n",
    "\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    elif isinstance(preds, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(f'Type ({type(preds)}) of predictions not supported')\n",
    "\n",
    "    labels = np.array(labels).astype(int)\n",
    "\n",
    "    train_class_count, test_class_count = [], []\n",
    "    mse_per_class, l1_per_class, l1_all_per_class = [], [], []\n",
    "    for l in np.unique(labels):\n",
    "        train_class_count.append(len(train_labels[train_labels == l]))\n",
    "        test_class_count.append(len(labels[labels == l]))\n",
    "        mse_per_class.append(np.sum((preds[labels == l] - labels[labels == l]) ** 2))\n",
    "        l1_per_class.append(np.sum(np.abs(preds[labels == l] - labels[labels == l])))\n",
    "        l1_all_per_class.append(np.abs(preds[labels == l] - labels[labels == l]))\n",
    "\n",
    "    many_shot_mse, median_shot_mse, low_shot_mse = [], [], []\n",
    "    many_shot_l1, median_shot_l1, low_shot_l1 = [], [], []\n",
    "    many_shot_gmean, median_shot_gmean, low_shot_gmean = [], [], []\n",
    "    many_shot_cnt, median_shot_cnt, low_shot_cnt = [], [], []\n",
    "\n",
    "    for i in range(len(train_class_count)):\n",
    "        if train_class_count[i] > many_shot_thr:\n",
    "            many_shot_mse.append(mse_per_class[i])\n",
    "            many_shot_l1.append(l1_per_class[i])\n",
    "            many_shot_gmean += list(l1_all_per_class[i])\n",
    "            many_shot_cnt.append(test_class_count[i])\n",
    "        elif train_class_count[i] < low_shot_thr:\n",
    "            low_shot_mse.append(mse_per_class[i])\n",
    "            low_shot_l1.append(l1_per_class[i])\n",
    "            low_shot_gmean += list(l1_all_per_class[i])\n",
    "            low_shot_cnt.append(test_class_count[i])\n",
    "        else:\n",
    "            median_shot_mse.append(mse_per_class[i])\n",
    "            median_shot_l1.append(l1_per_class[i])\n",
    "            median_shot_gmean += list(l1_all_per_class[i])\n",
    "            median_shot_cnt.append(test_class_count[i])\n",
    "\n",
    "    shot_dict = defaultdict(dict)\n",
    "    shot_dict['many']['mse'] = np.sum(many_shot_mse) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['l1'] = np.sum(many_shot_l1) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['gmean'] = gmean(np.hstack(many_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['median']['mse'] = np.sum(median_shot_mse) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['l1'] = np.sum(median_shot_l1) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['gmean'] = gmean(np.hstack(median_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['low']['mse'] = np.sum(low_shot_mse) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['gmean'] = gmean(np.hstack(low_shot_gmean), axis=None).astype(float)\n",
    "\n",
    "\n",
    "    return shot_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "balanced_mse.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
