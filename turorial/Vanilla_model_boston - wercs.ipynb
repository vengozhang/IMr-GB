{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJc7GRtFbrRK"
   },
   "source": [
    "# A Simple Regression Example of Balanced MSE\n",
    "\n",
    "This notebook is developed on [Balanced MSE for Imbalanced Visual Regression](https://github.com/jiawei-ren/BalancedMSE/tree/main/tutorial) and [Deep Imbalanced Regression (DIR) Tutorial](https://github.com/YyzHarry/imbalanced-regression/tree/main/tutorial). We thank the authors for their amazing tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7mBaSzabN6I"
   },
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xWuPbiEYmzvS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if (os.path.exists('./wercs_boston.data.train') and os.path.exists('./wercs_boston.data.val') and os.path.exists('./wercs_boston.data.test')):\n",
    "    print('Data already downloaded.')\n",
    "else:\n",
    "    print('No Data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnWsY4FzYC0Y"
   },
   "source": [
    "# Define the deep learning (neural network) model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jKm3tqsMhDOA"
   },
   "outputs": [],
   "source": [
    "# fcnet.py: Define the deep learning (neural network) model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FCNet(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, dropout=None):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.fc0 = nn.Linear(13, layers[0])            # features\n",
    "        self.fc1 = nn.Linear(layers[0], layers[1])\n",
    "        self.fc2 = nn.Linear(layers[1], layers[2])\n",
    "        self.fc_final = nn.Linear(layers[-1], 1)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.use_dropout = True if dropout else False\n",
    "        if self.use_dropout:\n",
    "            print(f'Using dropout: {dropout}')\n",
    "            self.dropout0 = nn.Dropout(p=dropout)\n",
    "            self.dropout1 = nn.Dropout(p=dropout)\n",
    "            self.dropout2 = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout0 = nn.Identity()\n",
    "            self.dropout1 = nn.Identity()\n",
    "            self.dropout2 = nn.Identity()\n",
    "\n",
    "    def forward(self, x, targets=None, epoch=None):\n",
    "        x = self.dropout0(F.relu(self.fc0(x)))\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout2(F.relu(self.fc2(x)))\n",
    "        x = self.fc_final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def fcnet1(**kwargs):\n",
    "    return FCNet([512, 512, 512], **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWohJXZPYbY3"
   },
   "source": [
    "# Define the loss functions. Here we only need the L1 loss: |f(x) - y|."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7Kzc8PTjhDIH"
   },
   "outputs": [],
   "source": [
    "# loss.py: Define the loss functions (here we only need the L1 loss)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def weighted_mse_loss(inputs, targets, weights=None):\n",
    "    loss = (inputs - targets) ** 2\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_l1_loss(inputs, targets, weights=None):\n",
    "    loss = F.l1_loss(inputs, targets, reduction='none')\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_focal_mse_loss(inputs, targets, weights=None, activate='sigmoid', beta=.2, gamma=1):\n",
    "    loss = (inputs - targets) ** 2\n",
    "    loss *= (torch.tanh(beta * torch.abs(inputs - targets))) ** gamma if activate == 'tanh' else \\\n",
    "        (2 * torch.sigmoid(beta * torch.abs(inputs - targets)) - 1) ** gamma\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_focal_l1_loss(inputs, targets, weights=None, activate='sigmoid', beta=.2, gamma=1):\n",
    "    loss = F.l1_loss(inputs, targets, reduction='none')\n",
    "    loss *= (torch.tanh(beta * torch.abs(inputs - targets))) ** gamma if activate == 'tanh' else \\\n",
    "        (2 * torch.sigmoid(beta * torch.abs(inputs - targets)) - 1) ** gamma\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_huber_loss(inputs, targets, weights=None, beta=1.):\n",
    "    l1_loss = torch.abs(inputs - targets)\n",
    "    cond = l1_loss < beta\n",
    "    loss = torch.where(cond, 0.5 * l1_loss ** 2 / beta, l1_loss - 0.5 * beta)\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aunbes6NY_Pz"
   },
   "source": [
    "# Define some utility functions (not the focus of this course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "f3XXH8Yik4M9"
   },
   "outputs": [],
   "source": [
    "# utils.py: Define some utility functions (not the focus of this course).\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal.windows import triang\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_batch_fmtstr(num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "def query_yes_no(question):\n",
    "    \"\"\" Ask a yes/no question via input() and return their answer. \"\"\"\n",
    "    valid = {\"yes\": True, \"y\": True, \"ye\": True, \"no\": False, \"n\": False}\n",
    "    prompt = \" [Y/n] \"\n",
    "\n",
    "    while True:\n",
    "        print(question + prompt, end=':')\n",
    "        choice = input().lower()\n",
    "        if choice == '':\n",
    "            return valid['y']\n",
    "        elif choice in valid:\n",
    "            return valid[choice]\n",
    "        else:\n",
    "            print(\"Please respond with 'yes' or 'no' (or 'y' or 'n').\\n\")\n",
    "\n",
    "def prepare_folders(args):\n",
    "    folders_util = [args.store_root, os.path.join(args.store_root, args.store_name)]\n",
    "    if os.path.exists(folders_util[-1]) and not args.resume and not args.evaluate:\n",
    "        if query_yes_no('overwrite previous folder: {} ?'.format(folders_util[-1])):\n",
    "            shutil.rmtree(folders_util[-1])\n",
    "            print(folders_util[-1] + ' removed.')\n",
    "        else:\n",
    "            raise RuntimeError('Output folder {} already exists'.format(folders_util[-1]))\n",
    "    for folder in folders_util:\n",
    "        if not os.path.exists(folder):\n",
    "            print(f\"===> Creating folder: {folder}\")\n",
    "            os.mkdir(folder)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    lr = args.lr\n",
    "    for milestone in args.schedule:\n",
    "        lr *= 0.1 if epoch >= milestone else 1.\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def save_checkpoint(args, state, is_best, prefix=''):\n",
    "    filename = f\"{args.store_root}/{args.store_name}/{prefix}ckpt.pth.tar\"\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        print(\"===> Saving current best checkpoint...\")\n",
    "        shutil.copyfile(filename, filename.replace('pth.tar', 'best.pth.tar'))\n",
    "\n",
    "def calibrate_mean_var(matrix, m1, v1, m2, v2, clip_min=0.1, clip_max=10):\n",
    "    if torch.sum(v1) < 1e-10:\n",
    "        return matrix\n",
    "    if (v1 == 0.).any():\n",
    "        valid = (v1 != 0.)\n",
    "        factor = torch.clamp(v2[valid] / v1[valid], clip_min, clip_max)\n",
    "        matrix[:, valid] = (matrix[:, valid] - m1[valid]) * torch.sqrt(factor) + m2[valid]\n",
    "        return matrix\n",
    "\n",
    "    factor = torch.clamp(v2 / v1, clip_min, clip_max)\n",
    "    return (matrix - m1) * torch.sqrt(factor) + m2\n",
    "\n",
    "def get_lds_kernel_window(kernel, ks, sigma):\n",
    "    assert kernel in ['gaussian', 'triang', 'laplace']\n",
    "    half_ks = (ks - 1) // 2\n",
    "    if kernel == 'gaussian':\n",
    "        base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks\n",
    "        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / max(gaussian_filter1d(base_kernel, sigma=sigma))\n",
    "    elif kernel == 'triang':\n",
    "        kernel_window = triang(ks)\n",
    "    else:\n",
    "        laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)\n",
    "        kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / max(map(laplace, np.arange(-half_ks, half_ks + 1)))\n",
    "\n",
    "    return kernel_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpbjtnotZUkk"
   },
   "source": [
    "# Define the data iterator (data loader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gqK8H1UghC_v"
   },
   "outputs": [],
   "source": [
    "# datasets.py: Define the data iterator (data loader).\n",
    "from scipy.ndimage import convolve1d\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "# from utils import get_lds_kernel_window\n",
    "\n",
    "class BostonHousing(data.Dataset):\n",
    "    def __init__(self, data_dir, split='train', reweight='none',\n",
    "                 lds=False, lds_kernel='gaussian', lds_ks=5, lds_sigma=2):\n",
    "        self.split = split\n",
    "        #self.data = np.loadtxt(data_dir, dtype='float32',encoding='utf-16')\n",
    "        self.data = np.loadtxt(data_dir, dtype='float32')\n",
    "        self.weights = self._prepare_weights(reweight=reweight, lds=lds, lds_kernel=lds_kernel, lds_ks=lds_ks, lds_sigma=lds_sigma)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = index % self.data.shape[0]\n",
    "        feature = self.data[index, :-1]\n",
    "        label = np.expand_dims(np.asarray(self.data[index, -1]), axis=0)\n",
    "        weight = np.asarray([self.weights[index]]).astype('float32') if self.weights is not None else np.asarray([np.float32(1.)])\n",
    "        return feature, label, weight\n",
    "\n",
    "    def _prepare_weights(self, reweight, max_target=51, lds=False, lds_kernel='gaussian', lds_ks=5, lds_sigma=2):\n",
    "        assert reweight in {'none', 'inverse', 'sqrt_inv'}\n",
    "        assert reweight != 'none' if lds else True, \\\n",
    "            \"Set reweight to \\'sqrt_inv\\' (default) or \\'inverse\\' when using LDS\"\n",
    "\n",
    "        value_dict = {x: 0 for x in range(max_target)}\n",
    "        labels = self.data[:, -1].tolist()\n",
    "        # mbr\n",
    "        for label in labels:\n",
    "            value_dict[min(max_target - 1, int(label))] += 1\n",
    "        if reweight == 'sqrt_inv':\n",
    "            value_dict = {k: np.sqrt(v) for k, v in value_dict.items()}\n",
    "        elif reweight == 'inverse':\n",
    "            value_dict = {k: np.clip(v, 0, 1000) for k, v in value_dict.items()}     # clip weights for inverse re-weight!!!!!!\n",
    "        num_per_label = [value_dict[min(max_target - 1, int(label))] for label in labels]\n",
    "        if not len(num_per_label) or reweight == 'none':\n",
    "            return None\n",
    "        print(f\"Using re-weighting: [{reweight.upper()}]\")\n",
    "\n",
    "        if lds:\n",
    "            lds_kernel_window = get_lds_kernel_window(lds_kernel, lds_ks, lds_sigma)\n",
    "            print(f'Using LDS: [{lds_kernel.upper()}] ({lds_ks}/{lds_sigma})')\n",
    "            smoothed_value = convolve1d(\n",
    "                np.asarray([v for _, v in value_dict.items()]), weights=lds_kernel_window, mode='constant')\n",
    "            num_per_label = [smoothed_value[min(max_target - 1, int(label))] for label in labels]\n",
    "\n",
    "        weights = [np.float32(1 / x) for x in num_per_label]\n",
    "        scaling = len(weights) / np.sum(weights)\n",
    "        weights = [scaling * x for x in weights]\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zcuwqp5paPk3"
   },
   "source": [
    "# Set up some default configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "T7mtQacPhBoW"
   },
   "outputs": [],
   "source": [
    "# train.py, Part 1: Set up some default configurations.\n",
    "import time\n",
    "import argparse\n",
    "#import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gmean\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from resnet import resnet50\n",
    "# from fcnet import fcnet1\n",
    "# from loss import *\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\"   # Window 系统原因，在不启用Cuda情况下，CPU线程问题会中断pipeline\n",
    "\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# CPU only\n",
    "parser.add_argument('--cpu_only', action='store_true', default=False, help='whether to use CPU only')\n",
    "# imbalanced related\n",
    "# LDS\n",
    "parser.add_argument('--lds', action='store_true', default=False, help='whether to enable LDS')\n",
    "parser.add_argument('--lds_kernel', type=str, default='gaussian',\n",
    "                    choices=['gaussian', 'triang', 'laplace'], help='LDS kernel type')\n",
    "parser.add_argument('--lds_ks', type=int, default=9, help='LDS kernel size: should be odd number')\n",
    "parser.add_argument('--lds_sigma', type=float, default=1, help='LDS gaussian/laplace kernel sigma')\n",
    "\n",
    "# re-weighting: SQRT_INV / INV\n",
    "parser.add_argument('--reweight', type=str, default='none', choices=['none', 'sqrt_inv', 'inverse'], help='cost-sensitive reweighting scheme')\n",
    "\n",
    "# training/optimization related\n",
    "parser.add_argument('--dataset', type=str, default='bostonhousing', choices=['imdb_wiki', 'agedb'], help='dataset name')\n",
    "parser.add_argument('--data_dir', type=str, default='./wercs_boston.data', help='data directory')\n",
    "parser.add_argument('--model', type=str, default='fcnet1', help='model name')\n",
    "parser.add_argument('--store_root', type=str, default='checkpoint', help='root path for storing checkpoints, logs')\n",
    "parser.add_argument('--store_name', type=str, default='', help='experiment store name')\n",
    "parser.add_argument('--gpu', type=int, default=None)\n",
    "parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'], help='optimizer type')\n",
    "parser.add_argument('--loss', type=str, default='l1', choices=['mse', 'l1', 'focal_l1', 'focal_mse', 'huber'], help='training loss type')\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help='initial learning rate')\n",
    "parser.add_argument('--epoch', type=int, default=10, help='number of epochs to train')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='optimizer momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4, help='optimizer weight decay')\n",
    "parser.add_argument('--schedule', type=int, nargs='*', default=[60, 80], help='lr schedule (when to drop lr by 10x)')\n",
    "#parser.add_argument('--batch_size', type=int, default=256, help='batch size')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--print_freq', type=int, default=10, help='logging frequency')\n",
    "parser.add_argument('--img_size', type=int, default=224, help='image size used in training')\n",
    "parser.add_argument('--workers', type=int, default=32, help='number of workers used in data loading')\n",
    "# checkpoints\n",
    "parser.add_argument('--resume', type=str, default='', help='checkpoint file path to resume training')\n",
    "parser.add_argument('--evaluate', action='store_true', help='evaluate only flag')\n",
    "\n",
    "# Balanced MSE\n",
    "parser.add_argument('--bmse', action='store_true', help='use Balanced MSE')\n",
    "parser.set_defaults(augment=True)\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nWvzFoYJnwuF"
   },
   "outputs": [],
   "source": [
    "args.cpu_only = True # Use CPU to train/test models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtG61vP8ROkP"
   },
   "source": [
    "# WERCS DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tSrzhog1gxyY",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overwrite previous folder: checkpoint\\bostonhousing_fcnet1_adam_l1_0.001_64 ? [Y/n] :"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\\bostonhousing_fcnet1_adam_l1_0.001_64 removed.\n",
      "===> Creating folder: checkpoint\\bostonhousing_fcnet1_adam_l1_0.001_64\n",
      "Args: Namespace(augment=True, batch_size=64, best_loss=100000.0, bmse=False, cpu_only=True, data_dir='./wercs_boston.data', dataset='bostonhousing', epoch=10, evaluate=False, gpu=None, img_size=224, lds=False, lds_kernel='gaussian', lds_ks=9, lds_sigma=1, loss='l1', lr=0.001, model='fcnet1', momentum=0.9, optimizer='adam', print_freq=10, resume='', reweight='none', schedule=[60, 80], start_epoch=0, store_name='bostonhousing_fcnet1_adam_l1_0.001_64', store_root='checkpoint', weight_decay=0.0001, workers=32)\n",
      "Store name: bostonhousing_fcnet1_adam_l1_0.001_64\n",
      "=====> Preparing data...\n",
      "Training data size: 352\n",
      "Validation data size: 77\n",
      "Test data size: 76\n",
      "=====> Building model...\n",
      "Epoch: [0][0/6]\tTime   0.02 (  0.02)\tData 0.0020 (0.0020)\tLoss (L1) 19.973 (19.973)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 345.885 (345.885)\tLoss (L1) 15.243 (15.243)\n",
      " * Overall: MSE 340.940\tL1 15.044\tG-Mean 10.477\n",
      " * Many: MSE 113.723\tL1 8.402\tG-Mean 6.047\n",
      " * Median: MSE 302.738\tL1 16.023\tG-Mean 12.397\n",
      " * Low: MSE 792.643\tL1 25.574\tG-Mean 19.796\n",
      "Best L1 Loss: 15.044\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #0: Train loss [24.6633]; Val loss: MSE [340.9405], L1 [15.0444], G-Mean [10.4770]\n",
      "Epoch: [1][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 12.645 (12.645)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 160.986 (160.986)\tLoss (L1) 10.016 (10.016)\n",
      " * Overall: MSE 155.608\tL1 9.729\tG-Mean 5.781\n",
      " * Many: MSE 58.228\tL1 5.006\tG-Mean 2.531\n",
      " * Median: MSE 88.277\tL1 8.923\tG-Mean 8.455\n",
      " * Low: MSE 419.483\tL1 19.665\tG-Mean 18.513\n",
      "Best L1 Loss: 9.729\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #1: Train loss [9.4383]; Val loss: MSE [155.6075], L1 [9.7292], G-Mean [5.7806]\n",
      "Epoch: [2][0/6]\tTime   0.01 (  0.01)\tData 0.0020 (0.0020)\tLoss (L1) 8.109 (8.109)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 95.818 (95.818)\tLoss (L1) 7.623 (7.623)\n",
      " * Overall: MSE 92.109\tL1 7.387\tG-Mean 4.436\n",
      " * Many: MSE 48.944\tL1 5.321\tG-Mean 3.016\n",
      " * Median: MSE 31.315\tL1 4.671\tG-Mean 3.479\n",
      " * Low: MSE 246.985\tL1 14.774\tG-Mean 13.406\n",
      "Best L1 Loss: 7.387\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #2: Train loss [7.3992]; Val loss: MSE [92.1090], L1 [7.3874], G-Mean [4.4357]\n",
      "Epoch: [3][0/6]\tTime   0.01 (  0.01)\tData 0.0030 (0.0030)\tLoss (L1) 6.083 (6.083)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 121.212 (121.212)\tLoss (L1) 8.210 (8.210)\n",
      " * Overall: MSE 119.837\tL1 8.087\tG-Mean 4.886\n",
      " * Many: MSE 37.836\tL1 3.808\tG-Mean 2.215\n",
      " * Median: MSE 69.721\tL1 7.558\tG-Mean 6.463\n",
      " * Low: MSE 323.345\tL1 16.394\tG-Mean 14.224\n",
      "Best L1 Loss: 7.387\n",
      "Epoch #3: Train loss [6.3254]; Val loss: MSE [119.8367], L1 [8.0873], G-Mean [4.8857]\n",
      "Epoch: [4][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 4.927 (4.927)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 103.182 (103.182)\tLoss (L1) 7.446 (7.446)\n",
      " * Overall: MSE 102.914\tL1 7.507\tG-Mean 4.768\n",
      " * Many: MSE 37.513\tL1 4.063\tG-Mean 2.285\n",
      " * Median: MSE 56.746\tL1 6.606\tG-Mean 5.112\n",
      " * Low: MSE 270.974\tL1 14.733\tG-Mean 12.582\n",
      "Best L1 Loss: 7.387\n",
      "Epoch #4: Train loss [5.9287]; Val loss: MSE [102.9136], L1 [7.5072], G-Mean [4.7680]\n",
      "Epoch: [5][0/6]\tTime   0.01 (  0.01)\tData 0.0020 (0.0020)\tLoss (L1) 6.489 (6.489)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 100.671 (100.671)\tLoss (L1) 7.414 (7.414)\n",
      " * Overall: MSE 98.083\tL1 7.298\tG-Mean 4.122\n",
      " * Many: MSE 37.106\tL1 4.021\tG-Mean 1.952\n",
      " * Median: MSE 46.360\tL1 5.957\tG-Mean 4.668\n",
      " * Low: MSE 269.814\tL1 15.092\tG-Mean 13.600\n",
      "Best L1 Loss: 7.298\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #5: Train loss [5.6092]; Val loss: MSE [98.0830], L1 [7.2983], G-Mean [4.1225]\n",
      "Epoch: [6][0/6]\tTime   0.01 (  0.01)\tData 0.0018 (0.0018)\tLoss (L1) 4.969 (4.969)\n",
      "Val: [0/2]\tTime  0.003 ( 0.003)\tLoss (MSE) 96.351 (96.351)\tLoss (L1) 7.113 (7.113)\n",
      " * Overall: MSE 93.564\tL1 7.069\tG-Mean 3.999\n",
      " * Many: MSE 37.434\tL1 4.086\tG-Mean 2.114\n",
      " * Median: MSE 41.624\tL1 5.615\tG-Mean 3.915\n",
      " * Low: MSE 254.228\tL1 14.343\tG-Mean 12.341\n",
      "Best L1 Loss: 7.069\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #6: Train loss [5.5050]; Val loss: MSE [93.5637], L1 [7.0695], G-Mean [3.9989]\n",
      "Epoch: [7][0/6]\tTime   0.02 (  0.02)\tData 0.0020 (0.0020)\tLoss (L1) 6.219 (6.219)\n",
      "Val: [0/2]\tTime  0.005 ( 0.005)\tLoss (MSE) 119.174 (119.174)\tLoss (L1) 7.934 (7.934)\n",
      " * Overall: MSE 114.684\tL1 7.794\tG-Mean 4.363\n",
      " * Many: MSE 37.610\tL1 3.753\tG-Mean 2.159\n",
      " * Median: MSE 59.838\tL1 6.930\tG-Mean 5.639\n",
      " * Low: MSE 315.735\tL1 16.126\tG-Mean 14.103\n",
      "Best L1 Loss: 7.069\n",
      "Epoch #7: Train loss [5.3996]; Val loss: MSE [114.6841], L1 [7.7940], G-Mean [4.3629]\n",
      "Epoch: [8][0/6]\tTime   0.02 (  0.02)\tData 0.0040 (0.0040)\tLoss (L1) 5.444 (5.444)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 104.988 (104.988)\tLoss (L1) 7.366 (7.366)\n",
      " * Overall: MSE 100.240\tL1 7.234\tG-Mean 4.035\n",
      " * Many: MSE 36.918\tL1 3.833\tG-Mean 1.871\n",
      " * Median: MSE 45.223\tL1 5.908\tG-Mean 4.846\n",
      " * Low: MSE 276.406\tL1 14.987\tG-Mean 12.874\n",
      "Best L1 Loss: 7.069\n",
      "Epoch #8: Train loss [5.3469]; Val loss: MSE [100.2405], L1 [7.2338], G-Mean [4.0354]\n",
      "Epoch: [9][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 5.096 (5.096)\n",
      "Val: [0/2]\tTime  0.006 ( 0.006)\tLoss (MSE) 125.475 (125.475)\tLoss (L1) 8.142 (8.142)\n",
      " * Overall: MSE 119.438\tL1 7.924\tG-Mean 4.592\n",
      " * Many: MSE 38.985\tL1 3.806\tG-Mean 2.217\n",
      " * Median: MSE 61.499\tL1 6.966\tG-Mean 5.689\n",
      " * Low: MSE 328.919\tL1 16.351\tG-Mean 13.783\n",
      "Best L1 Loss: 7.069\n",
      "Epoch #9: Train loss [5.3006]; Val loss: MSE [119.4383], L1 [7.9242], G-Mean [4.5920]\n",
      "========================================================================================================================\n",
      "Test best model on testset...\n",
      "Loaded best model, epoch 7, best val loss 7.0695\n",
      "Test: [0/2]\tTime  0.008 ( 0.008)\tLoss (MSE) 70.775 (70.775)\tLoss (L1) 6.203 (6.203)\n",
      " * Overall: MSE 65.923\tL1 6.056\tG-Mean 3.702\n",
      " * Many: MSE 26.330\tL1 4.219\tG-Mean 2.925\n",
      " * Median: MSE 25.761\tL1 4.257\tG-Mean 3.037\n",
      " * Low: MSE 218.578\tL1 13.617\tG-Mean 11.747\n",
      "Test loss: MSE [65.9230], L1 [6.0561], G-Mean [3.7019]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Option 1: To train the basic model, use the default setting, don't need to do anything\n",
    "args.reweight = 'none'\n",
    "args.lds = False\n",
    "\n",
    "# train.py Part 2: Train/Evaluate the model.\n",
    "args.store_name = ''\n",
    "args.start_epoch, args.best_loss = 0, 1e5\n",
    "\n",
    "if len(args.store_name):\n",
    "    args.store_name = f'_{args.store_name}'\n",
    "if not args.lds and args.reweight != 'none':\n",
    "    args.store_name += f'_{args.reweight}'\n",
    "if args.lds:\n",
    "    args.store_name += f'_lds_{args.lds_kernel[:3]}_{args.lds_ks}'\n",
    "    if args.lds_kernel in ['gaussian', 'laplace']:\n",
    "        args.store_name += f'_{args.lds_sigma}'\n",
    "if args.bmse:\n",
    "  args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_bmse_{args.lr}_{args.batch_size}\"\n",
    "else:\n",
    "  args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_{args.loss}_{args.lr}_{args.batch_size}\"\n",
    "\n",
    "prepare_folders(args)\n",
    "\n",
    "print(f\"Args: {args}\")\n",
    "print(f\"Store name: {args.store_name}\")\n",
    "\n",
    "def main():\n",
    "    if args.gpu is not None:\n",
    "        print(f\"Use GPU: {args.gpu} for training\")\n",
    "\n",
    "    # Data\n",
    "    print('=====> Preparing data...')\n",
    "    #train_labels = np.loadtxt(args.data_dir+'.train',encoding='utf-16')[:, -1]\n",
    "    train_labels = np.loadtxt(args.data_dir+'.train')[:, -1]\n",
    "    train_dataset = BostonHousing(data_dir=args.data_dir+'.train', split='train',\n",
    "                          reweight=args.reweight, lds=args.lds, lds_kernel=args.lds_kernel, lds_ks=args.lds_ks, lds_sigma=args.lds_sigma)\n",
    "    val_dataset = BostonHousing(data_dir=args.data_dir+'.val', split='val')\n",
    "    test_dataset = BostonHousing(data_dir=args.data_dir+'.test', split='test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                              num_workers=0, pin_memory=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                            num_workers=0, pin_memory=True, drop_last=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                             num_workers=0, pin_memory=True, drop_last=False)\n",
    "    print(f\"Training data size: {len(train_dataset)}\")\n",
    "    print(f\"Validation data size: {len(val_dataset)}\")\n",
    "    print(f\"Test data size: {len(test_dataset)}\")\n",
    "\n",
    "    # Random Seed\n",
    "    np.random.seed(999)\n",
    "    # random.seed(999)\n",
    "    torch.manual_seed(999)\n",
    "\n",
    "    # Model\n",
    "    print('=====> Building model...')\n",
    "    model = fcnet1()\n",
    "    if not args.cpu_only:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # evaluate only\n",
    "    if args.evaluate:\n",
    "        assert args.resume, 'Specify a trained model using [args.resume]'\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        print(f\"===> Checkpoint '{args.resume}' loaded (epoch [{checkpoint['epoch']}]), testing...\")\n",
    "        validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "        return\n",
    "\n",
    "    # Loss and optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr) if args.optimizer == 'adam' else \\\n",
    "        torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(f\"===> Loading checkpoint '{args.resume}'\")\n",
    "            checkpoint = torch.load(args.resume) if args.gpu is None else \\\n",
    "                torch.load(args.resume, map_location=torch.device(f'cuda:{str(args.gpu)}'))\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            args.best_loss = checkpoint['best_loss']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(f\"===> Loaded checkpoint '{args.resume}' (Epoch [{checkpoint['epoch']}])\")\n",
    "        else:\n",
    "            print(f\"===> No checkpoint found at '{args.resume}'\")\n",
    "\n",
    "    if not args.cpu_only:\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epoch):\n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "        train_loss = train(train_loader, model, optimizer, epoch)\n",
    "        val_loss_mse, val_loss_l1, val_loss_gmean = validate(val_loader, model, train_labels=train_labels)\n",
    "\n",
    "        loss_metric = val_loss_mse if args.loss == 'mse' else val_loss_l1\n",
    "        is_best = loss_metric < args.best_loss\n",
    "        args.best_loss = min(loss_metric, args.best_loss)\n",
    "        print(f\"Best {'L1' if 'l1' in args.loss else 'MSE'} Loss: {args.best_loss:.3f}\")\n",
    "        save_checkpoint(args, {\n",
    "            'epoch': epoch + 1,\n",
    "            'model': args.model,\n",
    "            'best_loss': args.best_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print(f\"Epoch #{epoch}: Train loss [{train_loss:.4f}]; \"\n",
    "              f\"Val loss: MSE [{val_loss_mse:.4f}], L1 [{val_loss_l1:.4f}], G-Mean [{val_loss_gmean:.4f}]\")\n",
    "\n",
    "    # test with best checkpoint\n",
    "    print(\"=\" * 120)\n",
    "    print(\"Test best model on testset...\")\n",
    "    checkpoint = torch.load(f\"{args.store_root}/{args.store_name}/ckpt.best.pth.tar\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(f\"Loaded best model, epoch {checkpoint['epoch']}, best val loss {checkpoint['best_loss']:.4f}\")\n",
    "\n",
    "    test_loss_mse, test_loss_l1, test_loss_gmean = validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "    print(f\"Test loss: MSE [{test_loss_mse:.4f}], L1 [{test_loss_l1:.4f}], G-Mean [{test_loss_gmean:.4f}]\\nDone\")\n",
    "\n",
    "def train(train_loader, model, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.2f')\n",
    "    data_time = AverageMeter('Data', ':6.4f')\n",
    "    losses = AverageMeter(f'Loss ({args.loss.upper()})', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch)\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for idx, (inputs, targets, weights) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        if not args.cpu_only:\n",
    "            inputs, targets, weights = \\\n",
    "                inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True), weights.cuda(non_blocking=True)\n",
    "        outputs = model(inputs, targets, epoch)\n",
    "        if args.bmse:\n",
    "          loss = globals()[f\"bmse_loss\"](outputs, targets)\n",
    "        else:\n",
    "          loss = globals()[f\"weighted_{args.loss}_loss\"](outputs, targets, weights)\n",
    "        assert not (np.isnan(loss.item()) or loss.item() > 1e6), f\"Loss explosion: {loss.item()}\"\n",
    "\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if idx % args.print_freq == 0:\n",
    "            progress.display(idx)\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "def validate(val_loader, model, train_labels=None, prefix='Val'):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses_mse = AverageMeter('Loss (MSE)', ':.3f')\n",
    "    losses_l1 = AverageMeter('Loss (L1)', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses_mse, losses_l1],\n",
    "        prefix=f'{prefix}: '\n",
    "    )\n",
    "\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    criterion_gmean = nn.L1Loss(reduction='none')\n",
    "\n",
    "    model.eval()\n",
    "    losses_all = []\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (inputs, targets, _) in enumerate(val_loader):\n",
    "            if not args.cpu_only:\n",
    "                inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds.extend(outputs.data.cpu().numpy())\n",
    "            labels.extend(targets.data.cpu().numpy())\n",
    "\n",
    "            loss_mse = criterion_mse(outputs, targets)\n",
    "            loss_l1 = criterion_l1(outputs, targets)\n",
    "            loss_all = criterion_gmean(outputs, targets)\n",
    "            losses_all.extend(loss_all.cpu().numpy())\n",
    "\n",
    "            losses_mse.update(loss_mse.item(), inputs.size(0))\n",
    "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if idx % args.print_freq == 0:\n",
    "                progress.display(idx)\n",
    "\n",
    "\n",
    "        shot_dict = shot_metrics(np.hstack(preds), np.hstack(labels), train_labels)\n",
    "        loss_gmean = gmean(np.hstack(losses_all), axis=None).astype(float)\n",
    "        print(f\" * Overall: MSE {losses_mse.avg:.3f}\\tL1 {losses_l1.avg:.3f}\\tG-Mean {loss_gmean:.3f}\")\n",
    "        print(f\" * Many: MSE {shot_dict['many']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['many']['l1']:.3f}\\tG-Mean {shot_dict['many']['gmean']:.3f}\")\n",
    "        print(f\" * Median: MSE {shot_dict['median']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['median']['l1']:.3f}\\tG-Mean {shot_dict['median']['gmean']:.3f}\")\n",
    "        print(f\" * Low: MSE {shot_dict['low']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['low']['l1']:.3f}\\tG-Mean {shot_dict['low']['gmean']:.3f}\")\n",
    "\n",
    "    return losses_mse.avg, losses_l1.avg, loss_gmean\n",
    "\n",
    "def shot_metrics(preds, labels, train_labels, many_shot_thr=10, low_shot_thr=2):  #  from data distribute  !!!!!!!!!!!!!!!!\n",
    "    train_labels = np.array(train_labels).astype(int)\n",
    "\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    elif isinstance(preds, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(f'Type ({type(preds)}) of predictions not supported')\n",
    "\n",
    "    labels = np.array(labels).astype(int)\n",
    "\n",
    "    train_class_count, test_class_count = [], []\n",
    "    mse_per_class, l1_per_class, l1_all_per_class = [], [], []\n",
    "    for l in np.unique(labels):\n",
    "        train_class_count.append(len(train_labels[train_labels == l]))\n",
    "        test_class_count.append(len(labels[labels == l]))\n",
    "        mse_per_class.append(np.sum((preds[labels == l] - labels[labels == l]) ** 2))\n",
    "        l1_per_class.append(np.sum(np.abs(preds[labels == l] - labels[labels == l])))\n",
    "        l1_all_per_class.append(np.abs(preds[labels == l] - labels[labels == l]))\n",
    "\n",
    "    many_shot_mse, median_shot_mse, low_shot_mse = [], [], []\n",
    "    many_shot_l1, median_shot_l1, low_shot_l1 = [], [], []\n",
    "    many_shot_gmean, median_shot_gmean, low_shot_gmean = [], [], []\n",
    "    many_shot_cnt, median_shot_cnt, low_shot_cnt = [], [], []\n",
    "\n",
    "    for i in range(len(train_class_count)):\n",
    "        if train_class_count[i] > many_shot_thr:\n",
    "            many_shot_mse.append(mse_per_class[i])\n",
    "            many_shot_l1.append(l1_per_class[i])\n",
    "            many_shot_gmean += list(l1_all_per_class[i])\n",
    "            many_shot_cnt.append(test_class_count[i])\n",
    "        elif train_class_count[i] < low_shot_thr:\n",
    "            low_shot_mse.append(mse_per_class[i])\n",
    "            low_shot_l1.append(l1_per_class[i])\n",
    "            low_shot_gmean += list(l1_all_per_class[i])\n",
    "            low_shot_cnt.append(test_class_count[i])\n",
    "        else:\n",
    "            median_shot_mse.append(mse_per_class[i])\n",
    "            median_shot_l1.append(l1_per_class[i])\n",
    "            median_shot_gmean += list(l1_all_per_class[i])\n",
    "            median_shot_cnt.append(test_class_count[i])\n",
    "\n",
    "    shot_dict = defaultdict(dict)\n",
    "    shot_dict['many']['mse'] = np.sum(many_shot_mse) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['l1'] = np.sum(many_shot_l1) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['gmean'] = gmean(np.hstack(many_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['median']['mse'] = np.sum(median_shot_mse) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['l1'] = np.sum(median_shot_l1) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['gmean'] = gmean(np.hstack(median_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['low']['mse'] = np.sum(low_shot_mse) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['gmean'] = gmean(np.hstack(low_shot_gmean), axis=None).astype(float)\n",
    "\n",
    "\n",
    "    return shot_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "balanced_mse.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
