{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJc7GRtFbrRK"
   },
   "source": [
    "# A Simple Regression Example of Balanced MSE\n",
    "\n",
    "This notebook is developed on [Balanced MSE for Imbalanced Visual Regression](https://github.com/jiawei-ren/BalancedMSE/tree/main/tutorial) and [Deep Imbalanced Regression (DIR) Tutorial](https://github.com/YyzHarry/imbalanced-regression/tree/main/tutorial). We thank the authors for their amazing tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7mBaSzabN6I"
   },
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xWuPbiEYmzvS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if (os.path.exists('./smoter_boston.data.train') and os.path.exists('./smoter_boston.data.val') and os.path.exists('./smoter_boston.data.test')):\n",
    "    print('Data already downloaded.')\n",
    "else:\n",
    "    print('No Data.!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnWsY4FzYC0Y"
   },
   "source": [
    "# Define the deep learning (neural network) model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jKm3tqsMhDOA"
   },
   "outputs": [],
   "source": [
    "# fcnet.py: Define the deep learning (neural network) model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FCNet(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, dropout=None):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.fc0 = nn.Linear(13, layers[0])            # features!!!\n",
    "        self.fc1 = nn.Linear(layers[0], layers[1])\n",
    "        self.fc2 = nn.Linear(layers[1], layers[2])\n",
    "        self.fc_final = nn.Linear(layers[-1], 1)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.use_dropout = True if dropout else False\n",
    "        if self.use_dropout:\n",
    "            print(f'Using dropout: {dropout}')\n",
    "            self.dropout0 = nn.Dropout(p=dropout)\n",
    "            self.dropout1 = nn.Dropout(p=dropout)\n",
    "            self.dropout2 = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout0 = nn.Identity()\n",
    "            self.dropout1 = nn.Identity()\n",
    "            self.dropout2 = nn.Identity()\n",
    "\n",
    "    def forward(self, x, targets=None, epoch=None):\n",
    "        x = self.dropout0(F.relu(self.fc0(x)))\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout2(F.relu(self.fc2(x)))\n",
    "        x = self.fc_final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def fcnet1(**kwargs):\n",
    "    return FCNet([512, 512, 512], **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWohJXZPYbY3"
   },
   "source": [
    "# Define the loss functions. Here we only need the L1 loss: |f(x) - y|."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7Kzc8PTjhDIH"
   },
   "outputs": [],
   "source": [
    "# loss.py: Define the loss functions (here we only need the L1 loss)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def weighted_mse_loss(inputs, targets, weights=None):\n",
    "    loss = (inputs - targets) ** 2\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_l1_loss(inputs, targets, weights=None):\n",
    "    loss = F.l1_loss(inputs, targets, reduction='none')\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_focal_mse_loss(inputs, targets, weights=None, activate='sigmoid', beta=.2, gamma=1):\n",
    "    loss = (inputs - targets) ** 2\n",
    "    loss *= (torch.tanh(beta * torch.abs(inputs - targets))) ** gamma if activate == 'tanh' else \\\n",
    "        (2 * torch.sigmoid(beta * torch.abs(inputs - targets)) - 1) ** gamma\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_focal_l1_loss(inputs, targets, weights=None, activate='sigmoid', beta=.2, gamma=1):\n",
    "    loss = F.l1_loss(inputs, targets, reduction='none')\n",
    "    loss *= (torch.tanh(beta * torch.abs(inputs - targets))) ** gamma if activate == 'tanh' else \\\n",
    "        (2 * torch.sigmoid(beta * torch.abs(inputs - targets)) - 1) ** gamma\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_huber_loss(inputs, targets, weights=None, beta=1.):\n",
    "    l1_loss = torch.abs(inputs - targets)\n",
    "    cond = l1_loss < beta\n",
    "    loss = torch.where(cond, 0.5 * l1_loss ** 2 / beta, l1_loss - 0.5 * beta)\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aunbes6NY_Pz"
   },
   "source": [
    "# Define some utility functions (not the focus of this course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "f3XXH8Yik4M9"
   },
   "outputs": [],
   "source": [
    "# utils.py: Define some utility functions (not the focus of this course).\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal.windows import triang\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_batch_fmtstr(num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "def query_yes_no(question):\n",
    "    \"\"\" Ask a yes/no question via input() and return their answer. \"\"\"\n",
    "    valid = {\"yes\": True, \"y\": True, \"ye\": True, \"no\": False, \"n\": False}\n",
    "    prompt = \" [Y/n] \"\n",
    "\n",
    "    while True:\n",
    "        print(question + prompt, end=':')\n",
    "        choice = input().lower()\n",
    "        if choice == '':\n",
    "            return valid['y']\n",
    "        elif choice in valid:\n",
    "            return valid[choice]\n",
    "        else:\n",
    "            print(\"Please respond with 'yes' or 'no' (or 'y' or 'n').\\n\")\n",
    "\n",
    "def prepare_folders(args):\n",
    "    folders_util = [args.store_root, os.path.join(args.store_root, args.store_name)]\n",
    "    if os.path.exists(folders_util[-1]) and not args.resume and not args.evaluate:\n",
    "        if query_yes_no('overwrite previous folder: {} ?'.format(folders_util[-1])):\n",
    "            shutil.rmtree(folders_util[-1])\n",
    "            print(folders_util[-1] + ' removed.')\n",
    "        else:\n",
    "            raise RuntimeError('Output folder {} already exists'.format(folders_util[-1]))\n",
    "    for folder in folders_util:\n",
    "        if not os.path.exists(folder):\n",
    "            print(f\"===> Creating folder: {folder}\")\n",
    "            os.mkdir(folder)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    lr = args.lr\n",
    "    for milestone in args.schedule:\n",
    "        lr *= 0.1 if epoch >= milestone else 1.\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def save_checkpoint(args, state, is_best, prefix=''):\n",
    "    filename = f\"{args.store_root}/{args.store_name}/{prefix}ckpt.pth.tar\"\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        print(\"===> Saving current best checkpoint...\")\n",
    "        shutil.copyfile(filename, filename.replace('pth.tar', 'best.pth.tar'))\n",
    "\n",
    "def calibrate_mean_var(matrix, m1, v1, m2, v2, clip_min=0.1, clip_max=10):\n",
    "    if torch.sum(v1) < 1e-10:\n",
    "        return matrix\n",
    "    if (v1 == 0.).any():\n",
    "        valid = (v1 != 0.)\n",
    "        factor = torch.clamp(v2[valid] / v1[valid], clip_min, clip_max)\n",
    "        matrix[:, valid] = (matrix[:, valid] - m1[valid]) * torch.sqrt(factor) + m2[valid]\n",
    "        return matrix\n",
    "\n",
    "    factor = torch.clamp(v2 / v1, clip_min, clip_max)\n",
    "    return (matrix - m1) * torch.sqrt(factor) + m2\n",
    "\n",
    "def get_lds_kernel_window(kernel, ks, sigma):\n",
    "    assert kernel in ['gaussian', 'triang', 'laplace']\n",
    "    half_ks = (ks - 1) // 2\n",
    "    if kernel == 'gaussian':\n",
    "        base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks\n",
    "        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / max(gaussian_filter1d(base_kernel, sigma=sigma))\n",
    "    elif kernel == 'triang':\n",
    "        kernel_window = triang(ks)\n",
    "    else:\n",
    "        laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)\n",
    "        kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / max(map(laplace, np.arange(-half_ks, half_ks + 1)))\n",
    "\n",
    "    return kernel_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpbjtnotZUkk"
   },
   "source": [
    "# Define the data iterator (data loader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gqK8H1UghC_v"
   },
   "outputs": [],
   "source": [
    "# datasets.py: Define the data iterator (data loader).\n",
    "from scipy.ndimage import convolve1d\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "# from utils import get_lds_kernel_window\n",
    "\n",
    "class BostonHousing(data.Dataset):\n",
    "    def __init__(self, data_dir, split='train', reweight='none',\n",
    "                 lds=False, lds_kernel='gaussian', lds_ks=5, lds_sigma=2):\n",
    "        self.split = split\n",
    "        #self.data = np.loadtxt(data_dir, dtype='float32',encoding='utf-16')\n",
    "        self.data = np.loadtxt(data_dir, dtype='float32')\n",
    "        self.weights = self._prepare_weights(reweight=reweight, lds=lds, lds_kernel=lds_kernel, lds_ks=lds_ks, lds_sigma=lds_sigma)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = index % self.data.shape[0]\n",
    "        feature = self.data[index, :-1]\n",
    "        label = np.expand_dims(np.asarray(self.data[index, -1]), axis=0)\n",
    "        weight = np.asarray([self.weights[index]]).astype('float32') if self.weights is not None else np.asarray([np.float32(1.)])\n",
    "        return feature, label, weight\n",
    "\n",
    "    def _prepare_weights(self, reweight, max_target=51, lds=False, lds_kernel='gaussian', lds_ks=5, lds_sigma=2):\n",
    "        assert reweight in {'none', 'inverse', 'sqrt_inv'}\n",
    "        assert reweight != 'none' if lds else True, \\\n",
    "            \"Set reweight to \\'sqrt_inv\\' (default) or \\'inverse\\' when using LDS\"\n",
    "\n",
    "        value_dict = {x: 0 for x in range(max_target)}\n",
    "        labels = self.data[:, -1].tolist()\n",
    "        # mbr\n",
    "        for label in labels:\n",
    "            value_dict[min(max_target - 1, int(label))] += 1\n",
    "        if reweight == 'sqrt_inv':\n",
    "            value_dict = {k: np.sqrt(v) for k, v in value_dict.items()}\n",
    "        elif reweight == 'inverse':\n",
    "            value_dict = {k: np.clip(v, 0, 1000) for k, v in value_dict.items()}     # clip weights for inverse re-weight!!!!!!\n",
    "        num_per_label = [value_dict[min(max_target - 1, int(label))] for label in labels]\n",
    "        if not len(num_per_label) or reweight == 'none':\n",
    "            return None\n",
    "        print(f\"Using re-weighting: [{reweight.upper()}]\")\n",
    "\n",
    "        if lds:\n",
    "            lds_kernel_window = get_lds_kernel_window(lds_kernel, lds_ks, lds_sigma)\n",
    "            print(f'Using LDS: [{lds_kernel.upper()}] ({lds_ks}/{lds_sigma})')\n",
    "            smoothed_value = convolve1d(\n",
    "                np.asarray([v for _, v in value_dict.items()]), weights=lds_kernel_window, mode='constant')\n",
    "            num_per_label = [smoothed_value[min(max_target - 1, int(label))] for label in labels]\n",
    "\n",
    "        weights = [np.float32(1 / x) for x in num_per_label]\n",
    "        scaling = len(weights) / np.sum(weights)\n",
    "        weights = [scaling * x for x in weights]\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zcuwqp5paPk3"
   },
   "source": [
    "# Set up some default configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "T7mtQacPhBoW"
   },
   "outputs": [],
   "source": [
    "# train.py, Part 1: Set up some default configurations.\n",
    "import time\n",
    "import argparse\n",
    "#import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gmean\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from resnet import resnet50\n",
    "# from fcnet import fcnet1\n",
    "# from loss import *\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\"   # Window 系统原因，在不启用Cuda情况下，CPU线程问题会中断pipeline\n",
    "\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# CPU only\n",
    "parser.add_argument('--cpu_only', action='store_true', default=False, help='whether to use CPU only')\n",
    "# imbalanced related\n",
    "# LDS\n",
    "parser.add_argument('--lds', action='store_true', default=False, help='whether to enable LDS')\n",
    "parser.add_argument('--lds_kernel', type=str, default='gaussian',\n",
    "                    choices=['gaussian', 'triang', 'laplace'], help='LDS kernel type')\n",
    "parser.add_argument('--lds_ks', type=int, default=9, help='LDS kernel size: should be odd number')\n",
    "parser.add_argument('--lds_sigma', type=float, default=1, help='LDS gaussian/laplace kernel sigma')\n",
    "\n",
    "# re-weighting: SQRT_INV / INV\n",
    "parser.add_argument('--reweight', type=str, default='none', choices=['none', 'sqrt_inv', 'inverse'], help='cost-sensitive reweighting scheme')\n",
    "\n",
    "# training/optimization related\n",
    "parser.add_argument('--dataset', type=str, default='bostonhousing', choices=['imdb_wiki', 'agedb'], help='dataset name')\n",
    "parser.add_argument('--data_dir', type=str, default='./smoter_boston.data', help='data directory')\n",
    "parser.add_argument('--model', type=str, default='fcnet1', help='model name')\n",
    "parser.add_argument('--store_root', type=str, default='checkpoint', help='root path for storing checkpoints, logs')\n",
    "parser.add_argument('--store_name', type=str, default='', help='experiment store name')\n",
    "parser.add_argument('--gpu', type=int, default=None)\n",
    "parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'], help='optimizer type')\n",
    "parser.add_argument('--loss', type=str, default='l1', choices=['mse', 'l1', 'focal_l1', 'focal_mse', 'huber'], help='training loss type')\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help='initial learning rate')\n",
    "parser.add_argument('--epoch', type=int, default=10, help='number of epochs to train')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='optimizer momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4, help='optimizer weight decay')\n",
    "parser.add_argument('--schedule', type=int, nargs='*', default=[60, 80], help='lr schedule (when to drop lr by 10x)')\n",
    "#parser.add_argument('--batch_size', type=int, default=256, help='batch size')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--print_freq', type=int, default=10, help='logging frequency')\n",
    "parser.add_argument('--img_size', type=int, default=224, help='image size used in training')\n",
    "parser.add_argument('--workers', type=int, default=32, help='number of workers used in data loading')\n",
    "# checkpoints\n",
    "parser.add_argument('--resume', type=str, default='', help='checkpoint file path to resume training')\n",
    "parser.add_argument('--evaluate', action='store_true', help='evaluate only flag')\n",
    "\n",
    "# Balanced MSE\n",
    "parser.add_argument('--bmse', action='store_true', help='use Balanced MSE')\n",
    "parser.set_defaults(augment=True)\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nWvzFoYJnwuF"
   },
   "outputs": [],
   "source": [
    "args.cpu_only = True # Use CPU to train/test models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtG61vP8ROkP"
   },
   "source": [
    "# Train DNN SMOTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tSrzhog1gxyY",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overwrite previous folder: checkpoint\\bostonhousing_fcnet1_adam_l1_0.001_64 ? [Y/n] :"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\\bostonhousing_fcnet1_adam_l1_0.001_64 removed.\n",
      "===> Creating folder: checkpoint\\bostonhousing_fcnet1_adam_l1_0.001_64\n",
      "Args: Namespace(augment=True, batch_size=64, best_loss=100000.0, bmse=False, cpu_only=True, data_dir='./smoter_boston.data', dataset='bostonhousing', epoch=10, evaluate=False, gpu=None, img_size=224, lds=False, lds_kernel='gaussian', lds_ks=9, lds_sigma=1, loss='l1', lr=0.001, model='fcnet1', momentum=0.9, optimizer='adam', print_freq=10, resume='', reweight='none', schedule=[60, 80], start_epoch=0, store_name='bostonhousing_fcnet1_adam_l1_0.001_64', store_root='checkpoint', weight_decay=0.0001, workers=32)\n",
      "Store name: bostonhousing_fcnet1_adam_l1_0.001_64\n",
      "=====> Preparing data...\n",
      "Training data size: 351\n",
      "Validation data size: 77\n",
      "Test data size: 76\n",
      "=====> Building model...\n",
      "Epoch: [0][0/6]\tTime   0.02 (  0.02)\tData 0.0020 (0.0020)\tLoss (L1) 18.586 (18.586)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 298.241 (298.241)\tLoss (L1) 13.986 (13.986)\n",
      " * Overall: MSE 293.942\tL1 13.754\tG-Mean 9.007\n",
      " * Many: MSE 87.658\tL1 8.211\tG-Mean 6.623\n",
      " * Median: MSE 231.605\tL1 12.177\tG-Mean 7.512\n",
      " * Low: MSE 795.034\tL1 26.964\tG-Mean 24.981\n",
      "Best L1 Loss: 13.754\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #0: Train loss [23.5417]; Val loss: MSE [293.9421], L1 [13.7536], G-Mean [9.0075]\n",
      "Epoch: [1][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 10.653 (10.653)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 126.931 (126.931)\tLoss (L1) 8.816 (8.816)\n",
      " * Overall: MSE 123.064\tL1 8.413\tG-Mean 5.085\n",
      " * Many: MSE 29.775\tL1 4.211\tG-Mean 2.381\n",
      " * Median: MSE 82.957\tL1 7.377\tG-Mean 5.120\n",
      " * Low: MSE 368.603\tL1 18.519\tG-Mean 17.790\n",
      "Best L1 Loss: 8.413\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #1: Train loss [8.1708]; Val loss: MSE [123.0642], L1 [8.4126], G-Mean [5.0851]\n",
      "Epoch: [2][0/6]\tTime   0.02 (  0.02)\tData 0.0020 (0.0020)\tLoss (L1) 6.067 (6.067)\n",
      "Val: [0/2]\tTime  0.005 ( 0.005)\tLoss (MSE) 77.624 (77.624)\tLoss (L1) 6.961 (6.961)\n",
      " * Overall: MSE 76.125\tL1 6.823\tG-Mean 4.591\n",
      " * Many: MSE 37.375\tL1 5.259\tG-Mean 4.162\n",
      " * Median: MSE 44.031\tL1 4.749\tG-Mean 2.786\n",
      " * Low: MSE 201.366\tL1 13.246\tG-Mean 12.116\n",
      "Best L1 Loss: 6.823\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #2: Train loss [6.0914]; Val loss: MSE [76.1248], L1 [6.8231], G-Mean [4.5910]\n",
      "Epoch: [3][0/6]\tTime   0.02 (  0.02)\tData 0.0020 (0.0020)\tLoss (L1) 4.971 (4.971)\n",
      "Val: [0/2]\tTime  0.005 ( 0.005)\tLoss (MSE) 75.648 (75.648)\tLoss (L1) 6.705 (6.705)\n",
      " * Overall: MSE 75.924\tL1 6.756\tG-Mean 4.270\n",
      " * Many: MSE 32.166\tL1 4.873\tG-Mean 3.781\n",
      " * Median: MSE 46.938\tL1 5.142\tG-Mean 3.330\n",
      " * Low: MSE 207.152\tL1 13.239\tG-Mean 11.816\n",
      "Best L1 Loss: 6.756\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #3: Train loss [5.1862]; Val loss: MSE [75.9236], L1 [6.7559], G-Mean [4.2698]\n",
      "Epoch: [4][0/6]\tTime   0.02 (  0.02)\tData 0.0030 (0.0030)\tLoss (L1) 5.533 (5.533)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 79.989 (79.989)\tLoss (L1) 6.735 (6.735)\n",
      " * Overall: MSE 81.568\tL1 6.928\tG-Mean 4.693\n",
      " * Many: MSE 37.688\tL1 4.966\tG-Mean 3.387\n",
      " * Median: MSE 47.521\tL1 5.321\tG-Mean 3.918\n",
      " * Low: MSE 213.863\tL1 13.213\tG-Mean 11.517\n",
      "Best L1 Loss: 6.756\n",
      "Epoch #4: Train loss [5.2862]; Val loss: MSE [81.5681], L1 [6.9281], G-Mean [4.6931]\n",
      "Epoch: [5][0/6]\tTime   0.02 (  0.02)\tData 0.0020 (0.0020)\tLoss (L1) 5.828 (5.828)\n",
      "Val: [0/2]\tTime  0.004 ( 0.004)\tLoss (MSE) 85.495 (85.495)\tLoss (L1) 6.937 (6.937)\n",
      " * Overall: MSE 84.866\tL1 6.957\tG-Mean 4.674\n",
      " * Many: MSE 27.001\tL1 4.280\tG-Mean 3.283\n",
      " * Median: MSE 50.400\tL1 5.498\tG-Mean 4.213\n",
      " * Low: MSE 249.244\tL1 14.692\tG-Mean 13.480\n",
      "Best L1 Loss: 6.756\n",
      "Epoch #5: Train loss [5.0073]; Val loss: MSE [84.8664], L1 [6.9575], G-Mean [4.6736]\n",
      "Epoch: [6][0/6]\tTime   0.02 (  0.02)\tData 0.0040 (0.0040)\tLoss (L1) 3.882 (3.882)\n",
      "Val: [0/2]\tTime  0.005 ( 0.005)\tLoss (MSE) 88.298 (88.298)\tLoss (L1) 7.031 (7.031)\n",
      " * Overall: MSE 87.522\tL1 7.075\tG-Mean 4.797\n",
      " * Many: MSE 33.155\tL1 4.587\tG-Mean 3.345\n",
      " * Median: MSE 49.354\tL1 5.418\tG-Mean 4.148\n",
      " * Low: MSE 246.378\tL1 14.496\tG-Mean 13.108\n",
      "Best L1 Loss: 6.756\n",
      "Epoch #6: Train loss [4.9133]; Val loss: MSE [87.5216], L1 [7.0748], G-Mean [4.7965]\n",
      "Epoch: [7][0/6]\tTime   0.02 (  0.02)\tData 0.0020 (0.0020)\tLoss (L1) 5.273 (5.273)\n",
      "Val: [0/2]\tTime  0.008 ( 0.008)\tLoss (MSE) 89.146 (89.146)\tLoss (L1) 7.008 (7.008)\n",
      " * Overall: MSE 87.316\tL1 6.969\tG-Mean 4.495\n",
      " * Many: MSE 25.573\tL1 4.093\tG-Mean 2.870\n",
      " * Median: MSE 50.181\tL1 5.445\tG-Mean 4.173\n",
      " * Low: MSE 262.972\tL1 15.189\tG-Mean 14.077\n",
      "Best L1 Loss: 6.756\n",
      "Epoch #7: Train loss [4.9019]; Val loss: MSE [87.3159], L1 [6.9694], G-Mean [4.4946]\n",
      "Epoch: [8][0/6]\tTime   0.02 (  0.02)\tData 0.0040 (0.0040)\tLoss (L1) 5.522 (5.522)\n",
      "Val: [0/2]\tTime  0.005 ( 0.005)\tLoss (MSE) 72.662 (72.662)\tLoss (L1) 6.535 (6.535)\n",
      " * Overall: MSE 71.030\tL1 6.514\tG-Mean 4.508\n",
      " * Many: MSE 32.013\tL1 4.863\tG-Mean 3.886\n",
      " * Median: MSE 38.143\tL1 4.509\tG-Mean 2.997\n",
      " * Low: MSE 196.964\tL1 12.983\tG-Mean 11.843\n",
      "Best L1 Loss: 6.514\n",
      "===> Saving current best checkpoint...\n",
      "Epoch #8: Train loss [4.8547]; Val loss: MSE [71.0298], L1 [6.5137], G-Mean [4.5082]\n",
      "Epoch: [9][0/6]\tTime   0.03 (  0.03)\tData 0.0050 (0.0050)\tLoss (L1) 4.737 (4.737)\n",
      "Val: [0/2]\tTime  0.012 ( 0.012)\tLoss (MSE) 75.886 (75.886)\tLoss (L1) 6.658 (6.658)\n",
      " * Overall: MSE 73.332\tL1 6.560\tG-Mean 4.442\n",
      " * Many: MSE 28.801\tL1 4.631\tG-Mean 3.691\n",
      " * Median: MSE 39.715\tL1 4.573\tG-Mean 3.069\n",
      " * Low: MSE 212.153\tL1 13.622\tG-Mean 12.628\n",
      "Best L1 Loss: 6.514\n",
      "Epoch #9: Train loss [4.7838]; Val loss: MSE [73.3323], L1 [6.5597], G-Mean [4.4418]\n",
      "========================================================================================================================\n",
      "Test best model on testset...\n",
      "Loaded best model, epoch 9, best val loss 6.5137\n",
      "Test: [0/2]\tTime  0.008 ( 0.008)\tLoss (MSE) 51.570 (51.570)\tLoss (L1) 5.580 (5.580)\n",
      " * Overall: MSE 49.553\tL1 5.502\tG-Mean 3.481\n",
      " * Many: MSE 30.033\tL1 4.425\tG-Mean 2.655\n",
      " * Median: MSE 19.531\tL1 4.004\tG-Mean 3.578\n",
      " * Low: MSE 150.904\tL1 11.475\tG-Mean 10.431\n",
      "Test loss: MSE [49.5529], L1 [5.5017], G-Mean [3.4810]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Option 1: To train the basic model, use the default setting, don't need to do anything\n",
    "args.reweight = 'none'\n",
    "args.lds = False\n",
    "\n",
    "# train.py Part 2: Train/Evaluate the model.\n",
    "args.store_name = ''\n",
    "args.start_epoch, args.best_loss = 0, 1e5\n",
    "\n",
    "if len(args.store_name):\n",
    "    args.store_name = f'_{args.store_name}'\n",
    "if not args.lds and args.reweight != 'none':\n",
    "    args.store_name += f'_{args.reweight}'\n",
    "if args.lds:\n",
    "    args.store_name += f'_lds_{args.lds_kernel[:3]}_{args.lds_ks}'\n",
    "    if args.lds_kernel in ['gaussian', 'laplace']:\n",
    "        args.store_name += f'_{args.lds_sigma}'\n",
    "if args.bmse:\n",
    "  args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_bmse_{args.lr}_{args.batch_size}\"\n",
    "else:\n",
    "  args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_{args.loss}_{args.lr}_{args.batch_size}\"\n",
    "\n",
    "prepare_folders(args)\n",
    "\n",
    "print(f\"Args: {args}\")\n",
    "print(f\"Store name: {args.store_name}\")\n",
    "\n",
    "def main():\n",
    "    if args.gpu is not None:\n",
    "        print(f\"Use GPU: {args.gpu} for training\")\n",
    "\n",
    "    # Data\n",
    "    print('=====> Preparing data...')\n",
    "    #train_labels = np.loadtxt(args.data_dir+'.train',encoding='utf-16')[:, -1]\n",
    "    train_labels = np.loadtxt(args.data_dir+'.train')[:, -1]\n",
    "    train_dataset = BostonHousing(data_dir=args.data_dir+'.train', split='train',\n",
    "                          reweight=args.reweight, lds=args.lds, lds_kernel=args.lds_kernel, lds_ks=args.lds_ks, lds_sigma=args.lds_sigma)\n",
    "    val_dataset = BostonHousing(data_dir=args.data_dir+'.val', split='val')\n",
    "    test_dataset = BostonHousing(data_dir=args.data_dir+'.test', split='test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                              num_workers=0, pin_memory=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                            num_workers=0, pin_memory=True, drop_last=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                             num_workers=0, pin_memory=True, drop_last=False)\n",
    "    print(f\"Training data size: {len(train_dataset)}\")\n",
    "    print(f\"Validation data size: {len(val_dataset)}\")\n",
    "    print(f\"Test data size: {len(test_dataset)}\")\n",
    "\n",
    "    # Random Seed\n",
    "    np.random.seed(999)\n",
    "    # random.seed(999)\n",
    "    torch.manual_seed(999)\n",
    "\n",
    "    # Model\n",
    "    print('=====> Building model...')\n",
    "    model = fcnet1()\n",
    "    if not args.cpu_only:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # evaluate only\n",
    "    if args.evaluate:\n",
    "        assert args.resume, 'Specify a trained model using [args.resume]'\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        print(f\"===> Checkpoint '{args.resume}' loaded (epoch [{checkpoint['epoch']}]), testing...\")\n",
    "        validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "        return\n",
    "\n",
    "    # Loss and optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr) if args.optimizer == 'adam' else \\\n",
    "        torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(f\"===> Loading checkpoint '{args.resume}'\")\n",
    "            checkpoint = torch.load(args.resume) if args.gpu is None else \\\n",
    "                torch.load(args.resume, map_location=torch.device(f'cuda:{str(args.gpu)}'))\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            args.best_loss = checkpoint['best_loss']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(f\"===> Loaded checkpoint '{args.resume}' (Epoch [{checkpoint['epoch']}])\")\n",
    "        else:\n",
    "            print(f\"===> No checkpoint found at '{args.resume}'\")\n",
    "\n",
    "    if not args.cpu_only:\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epoch):\n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "        train_loss = train(train_loader, model, optimizer, epoch)\n",
    "        val_loss_mse, val_loss_l1, val_loss_gmean = validate(val_loader, model, train_labels=train_labels)\n",
    "\n",
    "        loss_metric = val_loss_mse if args.loss == 'mse' else val_loss_l1\n",
    "        is_best = loss_metric < args.best_loss\n",
    "        args.best_loss = min(loss_metric, args.best_loss)\n",
    "        print(f\"Best {'L1' if 'l1' in args.loss else 'MSE'} Loss: {args.best_loss:.3f}\")\n",
    "        save_checkpoint(args, {\n",
    "            'epoch': epoch + 1,\n",
    "            'model': args.model,\n",
    "            'best_loss': args.best_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print(f\"Epoch #{epoch}: Train loss [{train_loss:.4f}]; \"\n",
    "              f\"Val loss: MSE [{val_loss_mse:.4f}], L1 [{val_loss_l1:.4f}], G-Mean [{val_loss_gmean:.4f}]\")\n",
    "\n",
    "    # test with best checkpoint\n",
    "    print(\"=\" * 120)\n",
    "    print(\"Test best model on testset...\")\n",
    "    checkpoint = torch.load(f\"{args.store_root}/{args.store_name}/ckpt.best.pth.tar\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(f\"Loaded best model, epoch {checkpoint['epoch']}, best val loss {checkpoint['best_loss']:.4f}\")\n",
    "\n",
    "    test_loss_mse, test_loss_l1, test_loss_gmean = validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "    print(f\"Test loss: MSE [{test_loss_mse:.4f}], L1 [{test_loss_l1:.4f}], G-Mean [{test_loss_gmean:.4f}]\\nDone\")\n",
    "\n",
    "def train(train_loader, model, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.2f')\n",
    "    data_time = AverageMeter('Data', ':6.4f')\n",
    "    losses = AverageMeter(f'Loss ({args.loss.upper()})', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch)\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for idx, (inputs, targets, weights) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        if not args.cpu_only:\n",
    "            inputs, targets, weights = \\\n",
    "                inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True), weights.cuda(non_blocking=True)\n",
    "        outputs = model(inputs, targets, epoch)\n",
    "        if args.bmse:\n",
    "          loss = globals()[f\"bmse_loss\"](outputs, targets)\n",
    "        else:\n",
    "          loss = globals()[f\"weighted_{args.loss}_loss\"](outputs, targets, weights)\n",
    "        assert not (np.isnan(loss.item()) or loss.item() > 1e6), f\"Loss explosion: {loss.item()}\"\n",
    "\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if idx % args.print_freq == 0:\n",
    "            progress.display(idx)\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "def validate(val_loader, model, train_labels=None, prefix='Val'):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses_mse = AverageMeter('Loss (MSE)', ':.3f')\n",
    "    losses_l1 = AverageMeter('Loss (L1)', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses_mse, losses_l1],\n",
    "        prefix=f'{prefix}: '\n",
    "    )\n",
    "\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    criterion_gmean = nn.L1Loss(reduction='none')\n",
    "\n",
    "    model.eval()\n",
    "    losses_all = []\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (inputs, targets, _) in enumerate(val_loader):\n",
    "            if not args.cpu_only:\n",
    "                inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds.extend(outputs.data.cpu().numpy())\n",
    "            labels.extend(targets.data.cpu().numpy())\n",
    "\n",
    "            loss_mse = criterion_mse(outputs, targets)\n",
    "            loss_l1 = criterion_l1(outputs, targets)\n",
    "            loss_all = criterion_gmean(outputs, targets)\n",
    "            losses_all.extend(loss_all.cpu().numpy())\n",
    "\n",
    "            losses_mse.update(loss_mse.item(), inputs.size(0))\n",
    "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if idx % args.print_freq == 0:\n",
    "                progress.display(idx)\n",
    "\n",
    "\n",
    "        shot_dict = shot_metrics(np.hstack(preds), np.hstack(labels), train_labels)\n",
    "        loss_gmean = gmean(np.hstack(losses_all), axis=None).astype(float)\n",
    "        print(f\" * Overall: MSE {losses_mse.avg:.3f}\\tL1 {losses_l1.avg:.3f}\\tG-Mean {loss_gmean:.3f}\")\n",
    "        print(f\" * Many: MSE {shot_dict['many']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['many']['l1']:.3f}\\tG-Mean {shot_dict['many']['gmean']:.3f}\")\n",
    "        print(f\" * Median: MSE {shot_dict['median']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['median']['l1']:.3f}\\tG-Mean {shot_dict['median']['gmean']:.3f}\")\n",
    "        print(f\" * Low: MSE {shot_dict['low']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['low']['l1']:.3f}\\tG-Mean {shot_dict['low']['gmean']:.3f}\")\n",
    "\n",
    "    return losses_mse.avg, losses_l1.avg, loss_gmean\n",
    "\n",
    "def shot_metrics(preds, labels, train_labels, many_shot_thr=10, low_shot_thr=2):  #  from data distribute  !!!!!!!!!!!!!!!!\n",
    "    train_labels = np.array(train_labels).astype(int)\n",
    "\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    elif isinstance(preds, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(f'Type ({type(preds)}) of predictions not supported')\n",
    "\n",
    "    labels = np.array(labels).astype(int)\n",
    "\n",
    "    train_class_count, test_class_count = [], []\n",
    "    mse_per_class, l1_per_class, l1_all_per_class = [], [], []\n",
    "    for l in np.unique(labels):\n",
    "        train_class_count.append(len(train_labels[train_labels == l]))\n",
    "        test_class_count.append(len(labels[labels == l]))\n",
    "        mse_per_class.append(np.sum((preds[labels == l] - labels[labels == l]) ** 2))\n",
    "        l1_per_class.append(np.sum(np.abs(preds[labels == l] - labels[labels == l])))\n",
    "        l1_all_per_class.append(np.abs(preds[labels == l] - labels[labels == l]))\n",
    "\n",
    "    many_shot_mse, median_shot_mse, low_shot_mse = [], [], []\n",
    "    many_shot_l1, median_shot_l1, low_shot_l1 = [], [], []\n",
    "    many_shot_gmean, median_shot_gmean, low_shot_gmean = [], [], []\n",
    "    many_shot_cnt, median_shot_cnt, low_shot_cnt = [], [], []\n",
    "\n",
    "    for i in range(len(train_class_count)):\n",
    "        if train_class_count[i] > many_shot_thr:\n",
    "            many_shot_mse.append(mse_per_class[i])\n",
    "            many_shot_l1.append(l1_per_class[i])\n",
    "            many_shot_gmean += list(l1_all_per_class[i])\n",
    "            many_shot_cnt.append(test_class_count[i])\n",
    "        elif train_class_count[i] < low_shot_thr:\n",
    "            low_shot_mse.append(mse_per_class[i])\n",
    "            low_shot_l1.append(l1_per_class[i])\n",
    "            low_shot_gmean += list(l1_all_per_class[i])\n",
    "            low_shot_cnt.append(test_class_count[i])\n",
    "        else:\n",
    "            median_shot_mse.append(mse_per_class[i])\n",
    "            median_shot_l1.append(l1_per_class[i])\n",
    "            median_shot_gmean += list(l1_all_per_class[i])\n",
    "            median_shot_cnt.append(test_class_count[i])\n",
    "\n",
    "    shot_dict = defaultdict(dict)\n",
    "    shot_dict['many']['mse'] = np.sum(many_shot_mse) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['l1'] = np.sum(many_shot_l1) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['gmean'] = gmean(np.hstack(many_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['median']['mse'] = np.sum(median_shot_mse) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['l1'] = np.sum(median_shot_l1) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['gmean'] = gmean(np.hstack(median_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['low']['mse'] = np.sum(low_shot_mse) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['gmean'] = gmean(np.hstack(low_shot_gmean), axis=None).astype(float)\n",
    "\n",
    "\n",
    "    return shot_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "balanced_mse.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
